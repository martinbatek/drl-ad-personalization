{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a7b750-f92d-4fbd-bd2b-26de63322724",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "156b5355-fa78-41dc-ad54-b3cc11f50caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "## General\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## In order to run calculations on AWS GPU, need to explicitly specify CUDA lib directory in the environment variables\n",
    "os.environ[\"XLA_FLAGS\"]=\"--xla_gpu_cuda_data_dir=/home/sagemaker-user/.conda/envs/mlds_gpu\"\n",
    "\n",
    "## Data manipulation and preprocessing\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from tensorflow.keras.layers import StringLookup, Normalization\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "\n",
    "## Modelling\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "## Import DeepCTR code\n",
    "## This is done by cloning the github repository instead of installing with pip. This is because of an incompatibility issue\n",
    "## with TF 2.14 that I had to manually fix in the DeepCTR code\n",
    "deepctr_path = '/home/sagemaker-user/drl-ad-personalization/DeepCTR'\n",
    "if deepctr_path not in sys.path:\n",
    "    sys.path.append(deepctr_path)\n",
    "\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr.models.dcn import DCN\n",
    "\n",
    "## We want to be able to query the list of available adverts from athena, so we need a PyAthena connection\n",
    "from pyathena import connect\n",
    "conn = connect(s3_staging_dir='s3://mlds-final-project-bucket/athena_output/',\n",
    "               region_name='eu-west-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a4dc1-c6dd-4daa-bdc4-0f7ad1b60b23",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4331cf3-c32b-4fe3-96e7-a390f55d84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of categorical colums for each dataset\n",
    "categorical_columns = [\n",
    "    'DisplayURL',\n",
    "    'AdID',\n",
    "    'AdvertiserID',\n",
    "    'QueryID',\n",
    "    'KeywordID',\n",
    "    'TitleID',\n",
    "    'DescriptionID',\n",
    "    'UserID'\n",
    "]\n",
    "\n",
    "# Import categorical feature mappings and define stringloohup objects for each dataset\n",
    "stringlookups = {}\n",
    "vocab_lengths = {}\n",
    "for field in categorical_columns:\n",
    "    df = pd.read_csv(f'./data/kdd12/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    stringlookups.update({field:lookup})\n",
    "    vocab_lengths.update({field:len(vocab)+1})\n",
    "\n",
    "# Define numerical feature columns\n",
    "numerical_columns = [\n",
    "    'Depth',\n",
    "    'Position'\n",
    "]\n",
    "# Extract scaler dicts for all datasets\n",
    "dist_stats = pd.read_csv('./data/kdd12/means_variances.csv')\n",
    "scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    scaler.build((1,))\n",
    "    scalers.update({field:scaler})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7b97f76-bcaf-41f6-b9e3-474ead6e4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define feature mappings\n",
    "kdd12_fixlen_feature_columns = [SparseFeat(feat.lower(), vocabulary_size=vocab_lengths[feat], embedding_dim=4) for feat in categorical_columns]\\\n",
    "+ [DenseFeat(feat.lower(),1) for feat in numerical_columns]\n",
    "\n",
    "## Generate the dnn and linear feature columns\n",
    "kdd12_dnn_feature_columns = kdd12_fixlen_feature_columns\n",
    "kdd12_linear_feature_columns = kdd12_fixlen_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f193b18-57cf-42af-ac03-28dada1a4f50",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c726443-dc54-4fc4-91be-3eedf3c1d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping callback\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=5\n",
    ")\n",
    "# Define the precision, recall and auc metrics\n",
    "precision = tf.keras.metrics.Precision(thresholds=0.5,name='precision')\n",
    "recall = tf.keras.metrics.Recall(thresholds=0.5,name='recall')\n",
    "auc = tf.keras.metrics.AUC(name='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4309484-3e8f-4dfb-b904-62c79823f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that returns compiled model\n",
    "def get_model(\n",
    "    dnn_hidden_units=[400,400],\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True,\n",
    "    cross_num=2\n",
    "):\n",
    "    model = DCN(\n",
    "        kdd12_linear_feature_columns,\n",
    "        kdd12_dnn_feature_columns,\n",
    "        dnn_hidden_units=dnn_hidden_units,\n",
    "        dnn_dropout=dnn_dropout,\n",
    "        l2_reg_dnn=l2_reg_dnn,\n",
    "        l2_reg_linear=l2_reg_linear,\n",
    "        l2_reg_embedding=l2_reg_embedding,\n",
    "        dnn_use_bn=dnn_use_bn,\n",
    "        cross_num=cross_num,\n",
    "    )\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        \"adam\", \n",
    "        \"binary_crossentropy\", \n",
    "        metrics=[\n",
    "            'binary_crossentropy',\n",
    "            'binary_accuracy',\n",
    "            precision,\n",
    "            recall,\n",
    "            auc\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60484444-4373-4df6-ac33-635327681f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossNet parameterization: vector\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "751d18d7-50bc-4aed-ae59-b8209c123424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f0aa05cfb20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the weights\n",
    "model.load_weights('models/final_rl_model/rl_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85ec024f-d270-431a-887e-43583bb85611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossNet parameterization: vector\n"
     ]
    }
   ],
   "source": [
    "target_model = get_model()\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654c92a-d581-47f7-a6e6-d32086e57f5a",
   "metadata": {},
   "source": [
    "# Define Reinfocement Learning environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02db99e4-e6a9-48fc-9572-740e7ea2353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RL env object that simpulates the Ad search platform\n",
    "class RLenv:\n",
    "    \"\"\"\n",
    "    Base class for Reinforcement Learning environment that simulates the search session\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,):\n",
    "        self.session_no = -1\n",
    "        self.userid = \"\"\n",
    "        self.queryid = \"\"\n",
    "        self.adlist = pd.DataFrame()\n",
    "        self.max_clicks = 0\n",
    "\n",
    "    def newSession(self,):\n",
    "        self.session_no += 1\n",
    "        query_input = pd.read_sql(f\"select userid, queryid from kdd12.offline_rl_queries where rn={str(self.session_no +1)}\",conn)\n",
    "        self.userid = query_input[\"userid\"].values[0]\n",
    "        self.queryid = query_input[\"queryid\"].values[0]\n",
    "        ad_list_df = pd.read_sql(f\"select * from kdd12.offline_rl_testing where userid='{self.userid}' and queryid='{self.queryid}'\",conn)\n",
    "        ad_list_df['clicks'] = ad_list_df.clicks/ad_list_df.impression\n",
    "        self.max_clicks = np.where(ad_list_df.clicks>=0.5,1.0,0.0).sum()\n",
    "        self.adlist = ad_list_df.drop(columns=['impression']).sort_values(by=['clicks'],ascending=[False]).reset_index(drop=True)\n",
    "        return self.adlist.copy().drop(columns=['clicks']), self.max_clicks\n",
    "\n",
    "    def showAd(self, ad_index):\n",
    "        ctr = self.adlist.loc[ad_index].clicks\n",
    "        if ctr>=0.5:\n",
    "            ctr_reward = 1.\n",
    "        else:\n",
    "            ctr_reward = 0.\n",
    "\n",
    "        # return the CTR\n",
    "        return ctr_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d28eb9-042b-41a4-8266-97397a32b133",
   "metadata": {},
   "source": [
    "# Define preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61605b09-33a3-4640-9793-1611bc8ec599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and Scale the datasets\n",
    "def encode_scale(element):\n",
    "    out = element.copy()\n",
    "    for field in categorical_columns:\n",
    "        out[field.lower()] = stringlookups[field](element[field.lower()])\n",
    "    for field in numerical_columns:\n",
    "        out[field.lower()] = tf.squeeze(scalers[field](element[field.lower()]),axis=-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d87af05-4094-4295-b972-d6c27b41bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for preprocessing the ad list\n",
    "def preprocess(ad_list,batch_size=1):\n",
    "    out_df = ad_list.copy()\n",
    "    # Convert position and depth to floats\n",
    "    out_df['position'] = out_df.position.astype('float32')\n",
    "    out_df['depth'] = out_df.depth.astype('float32')\n",
    "    # Convert to tf dataset\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(dict(out_df))\n",
    "    # Apply categorical encoding and numerical scaling\n",
    "    tf_dataset = tf_dataset.map(encode_scale)\n",
    "    # Add batch dim\n",
    "    tf_dataset = tf_dataset.batch(batch_size)\n",
    "    # Clean up\n",
    "    del out_df\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10a3e120-ca7c-4935-933a-7e4c72ff6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and Scale the datasets\n",
    "def encode_scale_2(element,labels):\n",
    "    out = element.copy()\n",
    "    for field in categorical_columns:\n",
    "        out[field.lower()] = stringlookups[field](element[field.lower()])\n",
    "    for field in numerical_columns:\n",
    "        out[field.lower()] = tf.squeeze(scalers[field](element[field.lower()]),axis=-1)\n",
    "    return out, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfe419cd-0719-4639-8094-8ea927073aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for preprocessing the ad list\n",
    "def preprocess_2(ad_list,labels,batch_size=1):\n",
    "    out_df = ad_list.copy()\n",
    "    # Convert position and depth to floats\n",
    "    out_df['position'] = out_df.position.astype('float32')\n",
    "    out_df['depth'] = out_df.depth.astype('float32')\n",
    "    # Convert to tf dataset\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((dict(out_df),labels))\n",
    "    # Apply categorical encoding and numerical scaling\n",
    "    tf_dataset = tf_dataset.map(encode_scale_2)\n",
    "    # Add batch dim\n",
    "    tf_dataset = tf_dataset.batch(batch_size)\n",
    "    # Clean up\n",
    "    del out_df\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b9b21-d779-4386-a552-af4a8c8c696c",
   "metadata": {},
   "source": [
    "# Simulation Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5d0ce5f-8a02-4698-a912-5027dedbb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "K = 10 # Total episodes\n",
    "L = 6 # session list Length\n",
    "R = 10 # sessions per episode\n",
    "H = L*R # Episode Time horizon\n",
    "sample_size = 30\n",
    "alpha = 1.0 # Explore network noise variable\n",
    "gamma = 0.1 # Future value discount\n",
    "current_episode = 0\n",
    "memory = pd.DataFrame()\n",
    "C = 5 # Target model update\n",
    "current_session = 0\n",
    "\n",
    "# Initialize the RL env\n",
    "rl_env = RLenv()\n",
    "\n",
    "if H%L>0:\n",
    "    total_sessions = H//L +1\n",
    "else:\n",
    "    total_sessions = H//L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738cccb-3991-408b-9572-f7527c44b825",
   "metadata": {},
   "source": [
    "# RL Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05b7f6a0-634c-4d09-abdd-9a74543470ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rl_simlulation direcotry if it doesn't exist\n",
    "if not os.path.exists('drl_simulation'):\n",
    "    os.mkdir('drl_simulation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95144838-0f61-4fa6-a291-2581857933da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Episode 1 of 10\n",
      "\n",
      "Starting session 1 of 10 at stage 1\n",
      "CrossNet parameterization: vector\n",
      "Starting session 2 of 10 at stage 7\n",
      "CrossNet parameterization: vector\n",
      "Starting session 3 of 10 at stage 13\n",
      "CrossNet parameterization: vector\n",
      "Starting session 4 of 10 at stage 19\n",
      "CrossNet parameterization: vector\n",
      "Starting session 5 of 10 at stage 25\n",
      "CrossNet parameterization: vector\n",
      "Starting session 6 of 10 at stage 31\n",
      "CrossNet parameterization: vector\n",
      "Starting session 7 of 10 at stage 37\n",
      "CrossNet parameterization: vector\n",
      "Starting session 8 of 10 at stage 43\n",
      "CrossNet parameterization: vector\n",
      "Starting session 9 of 10 at stage 49\n",
      "CrossNet parameterization: vector\n",
      "Starting session 10 of 10 at stage 55\n",
      "CrossNet parameterization: vector\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2022 - binary_crossentropy: 0.1728 - binary_accuracy: 0.0333 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8103\n",
      "---Episode 2 of 10\n",
      "\n",
      "Starting session 1 of 10 at stage 1\n",
      "CrossNet parameterization: vector\n",
      "Starting session 2 of 10 at stage 7\n",
      "CrossNet parameterization: vector\n",
      "Starting session 3 of 10 at stage 13\n",
      "CrossNet parameterization: vector\n",
      "Starting session 4 of 10 at stage 19\n",
      "CrossNet parameterization: vector\n",
      "Starting session 5 of 10 at stage 25\n",
      "CrossNet parameterization: vector\n",
      "Starting session 6 of 10 at stage 31\n",
      "CrossNet parameterization: vector\n",
      "Starting session 7 of 10 at stage 37\n",
      "CrossNet parameterization: vector\n",
      "Starting session 8 of 10 at stage 43\n",
      "CrossNet parameterization: vector\n",
      "Starting session 9 of 10 at stage 49\n",
      "CrossNet parameterization: vector\n",
      "Explore session outperformed base model. Setting model weights...\n",
      "Starting session 10 of 10 at stage 55\n",
      "CrossNet parameterization: vector\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1330 - binary_crossentropy: 0.0944 - binary_accuracy: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.0000e+00\n",
      "---Episode 3 of 10\n",
      "\n",
      "Starting session 1 of 10 at stage 1\n",
      "CrossNet parameterization: vector\n",
      "Starting session 2 of 10 at stage 7\n",
      "CrossNet parameterization: vector\n",
      "Starting session 3 of 10 at stage 13\n",
      "CrossNet parameterization: vector\n",
      "Starting session 4 of 10 at stage 19\n",
      "CrossNet parameterization: vector\n",
      "Starting session 5 of 10 at stage 25\n",
      "CrossNet parameterization: vector\n",
      "Starting session 6 of 10 at stage 31\n",
      "CrossNet parameterization: vector\n",
      "Starting session 7 of 10 at stage 37\n",
      "CrossNet parameterization: vector\n",
      "Starting session 8 of 10 at stage 43\n",
      "CrossNet parameterization: vector\n",
      "Starting session 9 of 10 at stage 49\n",
      "CrossNet parameterization: vector\n",
      "Starting session 10 of 10 at stage 55\n",
      "CrossNet parameterization: vector\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.1917 - binary_crossentropy: 0.1537 - binary_accuracy: 0.0333 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7069\n",
      "---Episode 4 of 10\n",
      "\n",
      "Starting session 1 of 10 at stage 1\n",
      "CrossNet parameterization: vector\n",
      "Starting session 2 of 10 at stage 7\n",
      "CrossNet parameterization: vector\n",
      "Starting session 3 of 10 at stage 13\n",
      "CrossNet parameterization: vector\n",
      "Starting session 4 of 10 at stage 19\n",
      "CrossNet parameterization: vector\n",
      "Starting session 5 of 10 at stage 25\n",
      "CrossNet parameterization: vector\n",
      "Starting session 6 of 10 at stage 31\n",
      "CrossNet parameterization: vector\n",
      "Starting session 7 of 10 at stage 37\n",
      "CrossNet parameterization: vector\n",
      "Starting session 8 of 10 at stage 43\n",
      "CrossNet parameterization: vector\n",
      "Starting session 9 of 10 at stage 49\n",
      "CrossNet parameterization: vector\n",
      "Explore session outperformed base model. Setting model weights...\n",
      "Starting session 10 of 10 at stage 55\n",
      "CrossNet parameterization: vector\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1647 - binary_crossentropy: 0.1164 - binary_accuracy: 0.0667 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7589\n",
      "---Episode 5 of 10\n",
      "\n",
      "Resetting target model\n",
      "CrossNet parameterization: vector\n",
      "Starting session 1 of 10 at stage 1\n",
      "CrossNet parameterization: vector\n",
      "Starting session 2 of 10 at stage 7\n",
      "CrossNet parameterization: vector\n",
      "Starting session 3 of 10 at stage 13\n",
      "CrossNet parameterization: vector\n",
      "Starting session 4 of 10 at stage 19\n",
      "CrossNet parameterization: vector\n",
      "Starting session 5 of 10 at stage 25\n",
      "CrossNet parameterization: vector\n",
      "Starting session 6 of 10 at stage 31\n",
      "CrossNet parameterization: vector\n",
      "Starting session 7 of 10 at stage 37\n",
      "CrossNet parameterization: vector\n",
      "Starting session 8 of 10 at stage 43\n",
      "CrossNet parameterization: vector\n",
      "Starting session 9 of 10 at stage 49\n",
      "CrossNet parameterization: vector\n",
      "Starting session 10 of 10 at stage 55\n",
      "CrossNet parameterization: vector\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.1581 - binary_crossentropy: 0.1109 - binary_accuracy: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.0000e+00\n",
      "---Episode 6 of 10\n",
      "\n",
      "Starting session 1 of 10 at stage 1\n",
      "CrossNet parameterization: vector\n",
      "Starting session 2 of 10 at stage 7\n",
      "CrossNet parameterization: vector\n",
      "Starting session 3 of 10 at stage 13\n",
      "CrossNet parameterization: vector\n",
      "Starting session 4 of 10 at stage 19\n",
      "CrossNet parameterization: vector\n",
      "Starting session 5 of 10 at stage 25\n",
      "CrossNet parameterization: vector\n",
      "Starting session 6 of 10 at stage 31\n",
      "CrossNet parameterization: vector\n",
      "Starting session 7 of 10 at stage 37\n",
      "CrossNet parameterization: vector\n",
      "Starting session 8 of 10 at stage 43\n",
      "CrossNet parameterization: vector\n",
      "Starting session 9 of 10 at stage 49\n",
      "CrossNet parameterization: vector\n",
      "Starting session 10 of 10 at stage 55\n",
      "CrossNet parameterization: vector\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.2162 - binary_crossentropy: 0.1706 - binary_accuracy: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.0000e+00\n",
      "---Episode 7 of 10\n",
      "\n",
      "Starting session 1 of 10 at stage 1\n",
      "CrossNet parameterization: vector\n",
      "Starting session 2 of 10 at stage 7\n",
      "CrossNet parameterization: vector\n",
      "Starting session 3 of 10 at stage 13\n",
      "CrossNet parameterization: vector\n",
      "Starting session 4 of 10 at stage 19\n",
      "CrossNet parameterization: vector\n",
      "Starting session 5 of 10 at stage 25\n",
      "CrossNet parameterization: vector\n",
      "Explore session outperformed base model. Setting model weights...\n",
      "Starting session 6 of 10 at stage 31\n",
      "CrossNet parameterization: vector\n",
      "Starting session 7 of 10 at stage 37\n",
      "CrossNet parameterization: vector\n",
      "Starting session 8 of 10 at stage 43\n",
      "CrossNet parameterization: vector\n",
      "Starting session 9 of 10 at stage 49\n",
      "CrossNet parameterization: vector\n",
      "Starting session 10 of 10 at stage 55\n",
      "CrossNet parameterization: vector\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.2276 - binary_crossentropy: 0.1679 - binary_accuracy: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.0000e+00\n",
      "---Episode 8 of 10\n",
      "\n",
      "Starting session 1 of 10 at stage 1\n",
      "CrossNet parameterization: vector\n",
      "Starting session 2 of 10 at stage 7\n",
      "CrossNet parameterization: vector\n",
      "Starting session 3 of 10 at stage 13\n",
      "CrossNet parameterization: vector\n",
      "Starting session 4 of 10 at stage 19\n",
      "CrossNet parameterization: vector\n",
      "Starting session 5 of 10 at stage 25\n",
      "CrossNet parameterization: vector\n",
      "Starting session 6 of 10 at stage 31\n",
      "CrossNet parameterization: vector\n",
      "Starting session 7 of 10 at stage 37\n",
      "CrossNet parameterization: vector\n",
      "Starting session 8 of 10 at stage 43\n",
      "CrossNet parameterization: vector\n",
      "Starting session 9 of 10 at stage 49\n",
      "CrossNet parameterization: vector\n",
      "Starting session 10 of 10 at stage 55\n",
      "CrossNet parameterization: vector\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.2308 - binary_crossentropy: 0.1723 - binary_accuracy: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.0000e+00\n",
      "---Episode 9 of 10\n",
      "\n",
      "Starting session 1 of 10 at stage 1\n",
      "CrossNet parameterization: vector\n",
      "Starting session 2 of 10 at stage 7\n",
      "CrossNet parameterization: vector\n",
      "Starting session 3 of 10 at stage 13\n",
      "CrossNet parameterization: vector\n",
      "Starting session 4 of 10 at stage 19\n",
      "CrossNet parameterization: vector\n",
      "Starting session 5 of 10 at stage 25\n",
      "CrossNet parameterization: vector\n",
      "Starting session 6 of 10 at stage 31\n",
      "CrossNet parameterization: vector\n",
      "Explore session outperformed base model. Setting model weights...\n",
      "Starting session 7 of 10 at stage 37\n",
      "CrossNet parameterization: vector\n",
      "Starting session 8 of 10 at stage 43\n",
      "CrossNet parameterization: vector\n",
      "Starting session 9 of 10 at stage 49\n",
      "CrossNet parameterization: vector\n",
      "Starting session 10 of 10 at stage 55\n",
      "CrossNet parameterization: vector\n",
      "Explore session outperformed base model. Setting model weights...\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1454 - binary_crossentropy: 0.0492 - binary_accuracy: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.0000e+00\n",
      "---Episode 10 of 10\n",
      "\n",
      "Resetting target model\n",
      "CrossNet parameterization: vector\n",
      "Starting session 1 of 10 at stage 1\n",
      "CrossNet parameterization: vector\n",
      "Starting session 2 of 10 at stage 7\n",
      "CrossNet parameterization: vector\n",
      "Starting session 3 of 10 at stage 13\n",
      "CrossNet parameterization: vector\n",
      "Starting session 4 of 10 at stage 19\n",
      "CrossNet parameterization: vector\n",
      "Explore session outperformed base model. Setting model weights...\n",
      "Starting session 5 of 10 at stage 25\n",
      "CrossNet parameterization: vector\n",
      "Starting session 6 of 10 at stage 31\n",
      "CrossNet parameterization: vector\n",
      "Starting session 7 of 10 at stage 37\n",
      "CrossNet parameterization: vector\n",
      "Starting session 8 of 10 at stage 43\n",
      "CrossNet parameterization: vector\n",
      "Explore session outperformed base model. Setting model weights...\n",
      "Starting session 9 of 10 at stage 49\n",
      "CrossNet parameterization: vector\n",
      "Starting session 10 of 10 at stage 55\n",
      "CrossNet parameterization: vector\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.3267 - binary_crossentropy: 0.1558 - binary_accuracy: 0.0333 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7069\n",
      "Simulation complete. Exporting data...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "for episode in range(K):\n",
    "    print(f\"---Episode {str(episode+1)} of {K}\\n\")\n",
    "    \n",
    "    # Initialize current stage and episode memory\n",
    "    stage = 0\n",
    "    episode_memory = pd.DataFrame()\n",
    "\n",
    "    # Reset the target model if its time\n",
    "    if (episode+1)%C == 0:\n",
    "        print(\"Resetting target model\")\n",
    "        target_model = get_model()\n",
    "        target_model.set_weights(model.get_weights())\n",
    "\n",
    "    # Run minor update algorithm after each session\n",
    "    for session in range(R):\n",
    "        print(f\"Starting session {str(session+1)} of {str(R)} at stage {str(stage+1)}\")\n",
    "        \n",
    "        # Initialize new session\n",
    "        session_ad_list, session_max_clicks = rl_env.newSession()\n",
    "        \n",
    "        # Create the explore model\n",
    "        explore_model = get_model()\n",
    "        explore_model.set_weights(model.get_weights())\n",
    "        for layer in explore_model.trainable_weights:\n",
    "            noise = tf.multiply(tf.multiply(alpha,tf.random.uniform(shape=layer.shape,minval=-1., maxval=1.)),layer)\n",
    "            layer.assign_add(noise)\n",
    "        \n",
    "        # Add base, target and explore model scoring to the session ad list\n",
    "        pred_features = [\n",
    "            'displayurl',\n",
    "            'adid',\n",
    "            'advertiserid',\n",
    "            'position',\n",
    "            'depth',\n",
    "            'keywordid',\n",
    "            'titleid',\n",
    "            'descriptionid',\n",
    "            'queryid',\n",
    "            'userid'\n",
    "        ]\n",
    "        session_ad_ds = preprocess(session_ad_list)\n",
    "        session_ad_list['base_score'] = model.predict(session_ad_ds, verbose=False)\n",
    "        session_ad_list['explore_score'] = explore_model.predict(session_ad_ds, verbose=False)\n",
    "        session_ad_list['target_score'] = target_model.predict(session_ad_ds, verbose=False)\n",
    "        \n",
    "        stages = []\n",
    "        sessions = []\n",
    "        episodes = []\n",
    "        actions = []\n",
    "        clicks = []\n",
    "        Q_futures = []\n",
    "        selection_models = []\n",
    "        list_data = pd.DataFrame(columns=pred_features)\n",
    "        for pos in range(L):\n",
    "            Q_future = session_ad_list[session_ad_list.base_score == session_ad_list.base_score.max()].head(1).target_score.values[0]\n",
    "            Q_futures.append(Q_future)\n",
    "            selection_model = np.random.choice(['base','explore'],size=1)[0]\n",
    "            selection_models.append(selection_model)\n",
    "            if selection_model == 'base':\n",
    "                next_action = session_ad_list[session_ad_list.base_score == session_ad_list.base_score.max()].head(1).index.values[0]\n",
    "            else:\n",
    "                next_action = session_ad_list[session_ad_list.explore_score == session_ad_list.explore_score.max()].head(1).index.values[0]\n",
    "            actions.append(next_action)\n",
    "            list_data.loc[pos] = session_ad_list.loc[next_action]\n",
    "            click = rl_env.showAd(next_action)\n",
    "            clicks.append(click)\n",
    "            session_ad_list = session_ad_list.loc[session_ad_list.index != next_action]\n",
    "            stages.append(stage)\n",
    "            sessions.append(session)\n",
    "            episodes.append(episode)\n",
    "            stage += 1\n",
    "        \n",
    "        list_data['stage'] = stages\n",
    "        list_data['session'] = sessions\n",
    "        list_data['episode'] = episodes\n",
    "        list_data['session_max_clicks'] = [session_max_clicks]*L\n",
    "        list_data['action']=actions\n",
    "        list_data['click']=clicks\n",
    "        list_data['Q_future']=Q_futures\n",
    "        list_data['selection_model']=selection_models\n",
    "        list_data = list_data.reset_index(names='list_pos')\n",
    "        if list_data[list_data.selection_model == \"explore\"].click.mean() > list_data[list_data.selection_model == \"base\"].click.mean():\n",
    "            print(\"Explore session outperformed base model. Setting model weights...\")\n",
    "            model.set_weights(explore_model.get_weights())\n",
    "\n",
    "        # Concatinate list data to episode memory\n",
    "        episode_memory = pd.concat([episode_memory,list_data],ignore_index=True)\n",
    "    \n",
    "    # Calculate the total reward\n",
    "    episode_memory['reward'] = (1. - gamma)*episode_memory.click + gamma*episode_memory.Q_future.shift(-1).fillna(0.0)\n",
    "\n",
    "    # Concatinate episode memory to total memory\n",
    "    memory = pd.concat([memory, episode_memory], ignore_index=True)\n",
    "\n",
    "    # Execute major update using experience replay\n",
    "    memory_sample = memory.sample(sample_size)\n",
    "    model_input_ds = preprocess_2(memory_sample[pred_features],memory_sample.reward.to_numpy(),batch_size=sample_size)\n",
    "    model.fit(model_input_ds)\n",
    "\n",
    "# Export the memory data\n",
    "print(\"Simulation complete. Exporting data...\")\n",
    "memory.to_csv('drl_simulation/drl_memory.csv',index=False)\n",
    "print(\"Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlds_gpu)",
   "language": "python",
   "name": "mlds_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
