{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a7b750-f92d-4fbd-bd2b-26de63322724",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "156b5355-fa78-41dc-ad54-b3cc11f50caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "## General\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## In order to run calculations on AWS GPU, need to explicitly specify CUDA lib directory in the environment variables\n",
    "os.environ[\"XLA_FLAGS\"]=\"--xla_gpu_cuda_data_dir=/home/sagemaker-user/.conda/envs/mlds_gpu\"\n",
    "\n",
    "## Data manipulation and preprocessing\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from tensorflow.keras.layers import StringLookup, Normalization\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "\n",
    "## Modelling\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "## Import DeepCTR code\n",
    "## This is done by cloning the github repository instead of installing with pip. This is because of an incompatibility issue\n",
    "## with TF 2.14 that I had to manually fix in the DeepCTR code\n",
    "deepctr_path = '/home/sagemaker-user/drl-ad-personalization/DeepCTR'\n",
    "if deepctr_path not in sys.path:\n",
    "    sys.path.append(deepctr_path)\n",
    "\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr.models.dcn import DCN\n",
    "\n",
    "## We want to be able to query the list of available adverts from athena, so we need a PyAthena connection\n",
    "from pyathena import connect\n",
    "conn = connect(s3_staging_dir='s3://mlds-final-project-bucket/athena_output/',\n",
    "               region_name='eu-west-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a4dc1-c6dd-4daa-bdc4-0f7ad1b60b23",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4331cf3-c32b-4fe3-96e7-a390f55d84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of categorical colums for each dataset\n",
    "categorical_columns = [\n",
    "    'DisplayURL',\n",
    "    'AdID',\n",
    "    'AdvertiserID',\n",
    "    'QueryID',\n",
    "    'KeywordID',\n",
    "    'TitleID',\n",
    "    'DescriptionID',\n",
    "    'UserID'\n",
    "]\n",
    "\n",
    "# Import categorical feature mappings and define stringloohup objects for each dataset\n",
    "stringlookups = {}\n",
    "vocab_lengths = {}\n",
    "for field in categorical_columns:\n",
    "    df = pd.read_csv(f'./data/kdd12/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    stringlookups.update({field:lookup})\n",
    "    vocab_lengths.update({field:len(vocab)+1})\n",
    "\n",
    "# Define numerical feature columns\n",
    "numerical_columns = [\n",
    "    'Depth',\n",
    "    'Position'\n",
    "]\n",
    "# Extract scaler dicts for all datasets\n",
    "dist_stats = pd.read_csv('./data/kdd12/means_variances.csv')\n",
    "scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    scaler.build((1,))\n",
    "    scalers.update({field:scaler})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b97f76-bcaf-41f6-b9e3-474ead6e4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define feature mappings\n",
    "kdd12_fixlen_feature_columns = [SparseFeat(feat.lower(), vocabulary_size=vocab_lengths[feat], embedding_dim=4) for feat in categorical_columns]\\\n",
    "+ [DenseFeat(feat.lower(),1) for feat in numerical_columns]\n",
    "\n",
    "## Generate the dnn and linear feature columns\n",
    "kdd12_dnn_feature_columns = kdd12_fixlen_feature_columns\n",
    "kdd12_linear_feature_columns = kdd12_fixlen_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f193b18-57cf-42af-ac03-28dada1a4f50",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c726443-dc54-4fc4-91be-3eedf3c1d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping callback\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=5\n",
    ")\n",
    "# Define the precision, recall and auc metrics\n",
    "precision = tf.keras.metrics.Precision(thresholds=0.5,name='precision')\n",
    "recall = tf.keras.metrics.Recall(thresholds=0.5,name='recall')\n",
    "auc = tf.keras.metrics.AUC(name='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4309484-3e8f-4dfb-b904-62c79823f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that returns compiled model\n",
    "def get_model(\n",
    "    dnn_hidden_units=[400,300,200],\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    "):\n",
    "    model = DCN(\n",
    "        kdd12_linear_feature_columns,\n",
    "        kdd12_dnn_feature_columns,\n",
    "        dnn_hidden_units=dnn_hidden_units,\n",
    "        dnn_dropout=dnn_dropout,\n",
    "        l2_reg_dnn=l2_reg_dnn,\n",
    "        l2_reg_linear=l2_reg_linear,\n",
    "        l2_reg_embedding=l2_reg_embedding,\n",
    "        dnn_use_bn=dnn_use_bn\n",
    "    )\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        \"adam\", \n",
    "        \"binary_crossentropy\", \n",
    "        metrics=[\n",
    "            'binary_crossentropy',\n",
    "            'binary_accuracy',\n",
    "            precision,\n",
    "            recall,\n",
    "            auc\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60484444-4373-4df6-ac33-635327681f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751d18d7-50bc-4aed-ae59-b8209c123424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f90fbb784f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the weights\n",
    "model.load_weights('models/final_rl_model/rl_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ec024f-d270-431a-887e-43583bb85611",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = get_model()\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654c92a-d581-47f7-a6e6-d32086e57f5a",
   "metadata": {},
   "source": [
    "# Define Reinfocement Learning environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02db99e4-e6a9-48fc-9572-740e7ea2353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RL env object that simpulates the Ad search platform\n",
    "class RLenv:\n",
    "    \"\"\"\n",
    "    Base class for Reinforcement Learning environment that simulates the search session\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,):\n",
    "        self.session_no = -1\n",
    "        self.userid = \"\"\n",
    "        self.queryid = \"\"\n",
    "        self.adlist = pd.DataFrame()\n",
    "        self.max_clicks = 0\n",
    "\n",
    "    def newSession(self,):\n",
    "        self.session_no += 1\n",
    "        query_input = pd.read_sql(f\"select userid, queryid from kdd12.offline_rl_queries where rn={str(self.session_no +1)}\",conn)\n",
    "        self.userid = query_input[\"userid\"].values[0]\n",
    "        self.queryid = query_input[\"queryid\"].values[0]\n",
    "        ad_list_df = pd.read_sql(f\"select * from kdd12.offline_rl_testing where userid='{self.userid}' and queryid='{self.queryid}'\",conn)\n",
    "        ad_list_df['clicks'] = ad_list_df.clicks/ad_list_df.impression\n",
    "        self.max_clicks = np.where(ad_list_df.clicks>=0.5,1.0,0.0).sum()\n",
    "        self.adlist = ad_list_df.drop(columns=['impression']).sort_values(by=['clicks'],ascending=[False]).reset_index(drop=True)\n",
    "        return self.adlist.copy().drop(columns=['clicks']), self.max_clicks\n",
    "\n",
    "    def showAd(self, ad_index):\n",
    "        ctr = self.adlist.loc[ad_index].clicks\n",
    "        if ctr>=0.5:\n",
    "            ctr_reward = 1.\n",
    "        else:\n",
    "            ctr_reward = 0.\n",
    "\n",
    "        # return the CTR\n",
    "        return ctr_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d28eb9-042b-41a4-8266-97397a32b133",
   "metadata": {},
   "source": [
    "# Define preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61605b09-33a3-4640-9793-1611bc8ec599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and Scale the datasets\n",
    "def encode_scale(element):\n",
    "    out = element.copy()\n",
    "    for field in categorical_columns:\n",
    "        out[field.lower()] = stringlookups[field](element[field.lower()])\n",
    "    for field in numerical_columns:\n",
    "        out[field.lower()] = tf.squeeze(scalers[field](element[field.lower()]),axis=-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d87af05-4094-4295-b972-d6c27b41bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for preprocessing the ad list\n",
    "def preprocess(ad_list,batch_size=1):\n",
    "    out_df = ad_list.copy()\n",
    "    # Convert position and depth to floats\n",
    "    out_df['position'] = out_df.position.astype('float32')\n",
    "    out_df['depth'] = out_df.depth.astype('float32')\n",
    "    # Convert to tf dataset\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(dict(out_df))\n",
    "    # Apply categorical encoding and numerical scaling\n",
    "    tf_dataset = tf_dataset.map(encode_scale)\n",
    "    # Add batch dim\n",
    "    tf_dataset = tf_dataset.batch(batch_size)\n",
    "    # Clean up\n",
    "    del out_df\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a3e120-ca7c-4935-933a-7e4c72ff6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and Scale the datasets\n",
    "def encode_scale_2(element,labels):\n",
    "    out = element.copy()\n",
    "    for field in categorical_columns:\n",
    "        out[field.lower()] = stringlookups[field](element[field.lower()])\n",
    "    for field in numerical_columns:\n",
    "        out[field.lower()] = tf.squeeze(scalers[field](element[field.lower()]),axis=-1)\n",
    "    return out, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe419cd-0719-4639-8094-8ea927073aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for preprocessing the ad list\n",
    "def preprocess_2(ad_list,labels,batch_size=1):\n",
    "    out_df = ad_list.copy()\n",
    "    # Convert position and depth to floats\n",
    "    out_df['position'] = out_df.position.astype('float32')\n",
    "    out_df['depth'] = out_df.depth.astype('float32')\n",
    "    # Convert to tf dataset\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((dict(out_df),labels))\n",
    "    # Apply categorical encoding and numerical scaling\n",
    "    tf_dataset = tf_dataset.map(encode_scale_2)\n",
    "    # Add batch dim\n",
    "    tf_dataset = tf_dataset.batch(batch_size)\n",
    "    # Clean up\n",
    "    del out_df\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b9b21-d779-4386-a552-af4a8c8c696c",
   "metadata": {},
   "source": [
    "# Minor Update Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d0ce5f-8a02-4698-a912-5027dedbb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "K = 10 # Total episodes\n",
    "H = 150 # Episode Time horizon\n",
    "sample_size = 100\n",
    "alpha = 1.0 # Explore network noise variable\n",
    "gamma = 0.1 # Future value discount\n",
    "L = 6 # List Length\n",
    "current_episode = 0\n",
    "N = 1000 # Max dataset length\n",
    "memory = pd.DataFrame()\n",
    "C = 5 # Target model update\n",
    "current_session = 0\n",
    "\n",
    "# Initialize the RL env\n",
    "rl_env = RLenv()\n",
    "\n",
    "if H%L>0:\n",
    "    total_sessions = H//L +1\n",
    "else:\n",
    "    total_sessions = H//L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95144838-0f61-4fa6-a291-2581857933da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 of 25\n",
      "Current list length:  6\n",
      "Session 2 of 25\n",
      "Current list length:  6\n",
      "Session 3 of 25\n",
      "Current list length:  6\n",
      "Session 4 of 25\n",
      "Current list length:  6\n",
      "Session 5 of 25\n",
      "Current list length:  6\n",
      "Session 6 of 25\n",
      "Current list length:  6\n",
      "Session 7 of 25\n",
      "Current list length:  6\n",
      "Session 8 of 25\n",
      "Current list length:  6\n",
      "Session 9 of 25\n",
      "Current list length:  6\n",
      "Session 10 of 25\n",
      "Current list length:  6\n",
      "Session 11 of 25\n",
      "Current list length:  6\n",
      "Session 12 of 25\n",
      "Current list length:  6\n",
      "Session 13 of 25\n",
      "Current list length:  6\n",
      "Session 14 of 25\n",
      "Current list length:  6\n",
      "Session 15 of 25\n",
      "Current list length:  6\n",
      "Session 16 of 25\n",
      "Current list length:  6\n",
      "Session 17 of 25\n",
      "Current list length:  6\n",
      "Session 18 of 25\n",
      "Current list length:  6\n",
      "Session 19 of 25\n",
      "Current list length:  6\n",
      "Session 20 of 25\n",
      "Current list length:  6\n",
      "Session 21 of 25\n",
      "Current list length:  6\n",
      "Session 22 of 25\n",
      "Current list length:  6\n",
      "Session 23 of 25\n",
      "Current list length:  6\n",
      "Session 24 of 25\n",
      "Current list length:  6\n",
      "Session 25 of 25\n",
      "Current list length:  6\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pred_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m memory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m gamma)\u001b[38;5;241m*\u001b[39mmemory\u001b[38;5;241m.\u001b[39mclick \u001b[38;5;241m+\u001b[39m gamma\u001b[38;5;241m*\u001b[39mmemory\u001b[38;5;241m.\u001b[39mclick\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m     70\u001b[0m memory_sample \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39msample(sample_size)\n\u001b[0;32m---> 71\u001b[0m model_input_ds \u001b[38;5;241m=\u001b[39m preprocess_2(memory_sample[\u001b[43mpred_features\u001b[49m],memory_sample\u001b[38;5;241m.\u001b[39mreward\u001b[38;5;241m.\u001b[39mto_numpy(),batch_size\u001b[38;5;241m=\u001b[39msample_size)\n\u001b[1;32m     72\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(model_input_ds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_features' is not defined"
     ]
    }
   ],
   "source": [
    "while current_episode<H:\n",
    "    current_session += 1\n",
    "    if current_episode > (H - L - 1):\n",
    "        current_list = H%current_episode\n",
    "    else:\n",
    "        current_list = L\n",
    "    print(f\"Session {current_session} of {total_sessions}\")    \n",
    "    print(\"Current list length: \",current_list)\n",
    "    \n",
    "    # Initialize new session\n",
    "    session_ad_list, session_max_clicks = rl_env.newSession()\n",
    "    \n",
    "    # Create the explore model\n",
    "    explore_model = get_model()\n",
    "    explore_model.set_weights(model.get_weights())\n",
    "    for layer in explore_model.trainable_weights:\n",
    "        noise = tf.multiply(tf.multiply(alpha,tf.random.uniform(shape=layer.shape,minval=-1., maxval=1.)),layer)\n",
    "        layer.assign_add(noise)\n",
    "    \n",
    "    # Add base, target and explore model scoring to the session ad list\n",
    "    pred_features = [\n",
    "        'displayurl',\n",
    "        'adid',\n",
    "        'advertiserid',\n",
    "        'position',\n",
    "        'depth',\n",
    "        'keywordid',\n",
    "        'titleid',\n",
    "        'descriptionid',\n",
    "        'queryid',\n",
    "        'userid'\n",
    "    ]\n",
    "    session_ad_ds = preprocess(session_ad_list)\n",
    "    session_ad_list['base_score'] = model.predict(session_ad_ds, verbose=False)\n",
    "    session_ad_list['explore_score'] = explore_model.predict(session_ad_ds, verbose=False)\n",
    "    session_ad_list['target_score'] = target_model.predict(session_ad_ds, verbose=False)\n",
    "    \n",
    "    actions = []\n",
    "    clicks = []\n",
    "    Q_futures = []\n",
    "    selection_models = []\n",
    "    list_data = pd.DataFrame(columns=features)\n",
    "    for pos in range(current_list):\n",
    "        Q_future = session_ad_list[session_ad_list.base_score == session_ad_list.base_score.max()].head(1).target_score.values[0]\n",
    "        Q_futures.append(Q_future)\n",
    "        selection_model = np.random.choice(['base','explore'],size=1)[0]\n",
    "        selection_models.append(selection_model)\n",
    "        if selection_model == 'base':\n",
    "            next_action = session_ad_list[session_ad_list.base_score == session_ad_list.base_score.max()].head(1).index.values[0]\n",
    "        else:\n",
    "            next_action = session_ad_list[session_ad_list.explore_score == session_ad_list.explore_score.max()].head(1).index.values[0]\n",
    "        actions.append(next_action)\n",
    "        list_data.loc[pos] = session_ad_list.loc[next_action]\n",
    "        click = rl_env.showAd(next_action)\n",
    "        clicks.append(click)\n",
    "        session_ad_list = session_ad_list.loc[session_ad_list.index != next_action]\n",
    "        current_episode += 1\n",
    "    \n",
    "    list_data['action']=actions\n",
    "    list_data['click']=clicks\n",
    "    list_data['Q_future']=Q_futures\n",
    "    list_data['selection_model']=selection_models\n",
    "    list_data = list_data.reset_index(names='list_pos')\n",
    "    if list_data[list_data.selection_model == \"explore\"].click.mean() > list_data[list_data.selection_model == \"base\"].click.mean():\n",
    "        model.set_weights(explore_model.get_weights())\n",
    "    \n",
    "    memory = pd.concat([memory,list_data],ignore_index=True)\n",
    "\n",
    "memory['reward'] = (1. - gamma)*memory.click + gamma*memory.click.shift(-1).fillna(0.0)\n",
    "memory_sample = memory.sample(sample_size)\n",
    "model_input_ds = preprocess_2(memory_sample[pred_features],memory_sample.reward.to_numpy(),batch_size=sample_size)\n",
    "model.fit(model_input_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlds_gpu)",
   "language": "python",
   "name": "mlds_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
