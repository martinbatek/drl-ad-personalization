{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "## General\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "\n",
    "## Data manipulation and preprocessing\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer,LabelEncoder, MinMaxScaler,OrdinalEncoder\n",
    "#import missingno as msno\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, MissingIndicator\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from scipy import sparse\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup, Normalization\n",
    "\n",
    "## Pre-processing functions\n",
    "def criteo_log_transform(z):\n",
    "    if z>2:\n",
    "        return np.log(z)**2\n",
    "    else:\n",
    "        return z\n",
    "def map_criteo_log_transform(x):\n",
    "    return x.map(criteo_log_transform)\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "\n",
    "## Modelling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "from fastFM import sgd\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from deepctr.models.fnn import FNN\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr.models.pnn import PNN\n",
    "from deepctr.models.wdl import WDL\n",
    "from deepctr.models.deepfm import DeepFM\n",
    "from deepctr.models.autoint import AutoInt\n",
    "from deepctr.models.fgcnn import FGCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As outlined in the Feasibility secition of my Milestone 3 submission, I have segmented the work over the next few weeks according to the following timeline\n",
    "\n",
    "![](figures/timeline.png)\n",
    "\n",
    "Below I proceed by structuring my response according to the following sections\n",
    "- Model Specific Research\n",
    "- Data exploration, preprocessing and environment setup\n",
    "- Model Replication and Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Specific Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Fill out details from readings__*\n",
    "\n",
    "On the basis of my reasearch and published results, I have chosen to investigate the following models:\n",
    "\n",
    "__Shallow__\n",
    "- Logistic Regression\n",
    "- Factorization Machines\n",
    "\n",
    "__Deep__\n",
    "- Factorization Supported Neural Networks\n",
    "- Product Based Neural Networks\n",
    "- Wide and Deep\n",
    "- DeepFM\n",
    "- Feature Generation by Convolutional Neural Networks\n",
    "- Automatic Feature Interaction (AutoInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration, Caching, Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load benchmark datasets as tf.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_folder(bucket_name, folder_name):\n",
    "    out = []\n",
    "    # Create a Boto3 client for S3\n",
    "    s3_client = boto3.client('s3')\n",
    " \n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder_name)\n",
    " \n",
    " \n",
    "    if 'Contents' in response:\n",
    "    \n",
    "        for obj in response['Contents']:\n",
    "            out.append(obj['Key'])\n",
    "        out = out[1:]\n",
    "        return out\n",
    "    else:\n",
    "        print(\"Folder is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get avazu file names\n",
    "bucket_name = 'mlds-final-project-bucket'\n",
    "folder_name = 'kdd12/train/train_split/'\n",
    "keys = list_files_in_folder(bucket_name, folder_name)\n",
    "filenames = [f\"s3://{bucket_name}/{key}\" for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 14:17:49.884443: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-08-03 14:17:49.887664: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-03 14:17:49.895763: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Load the kdd12_train dataset\n",
    "kdd12_train = tf.data.experimental.make_csv_dataset(\n",
    "    filenames,\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=['int32','int32','string','string','string','int32','int32','string','string','string','string','string'],\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get avazu file names\n",
    "bucket_name = 'mlds-final-project-bucket'\n",
    "folder_name = 'kdd12/train/test_split/'\n",
    "keys = list_files_in_folder(bucket_name, folder_name)\n",
    "filenames = [f\"s3://{bucket_name}/{key}\" for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the kdd12_train dataset\n",
    "kdd12_val = tf.data.experimental.make_csv_dataset(\n",
    "    filenames,\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=['int32','int32','string','string','string','int32','int32','string','string','string','string','string'],\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get avazu file names\n",
    "bucket_name = 'mlds-final-project-bucket'\n",
    "folder_name = 'avazu/train/train_split/'\n",
    "keys = list_files_in_folder(bucket_name, folder_name)\n",
    "filenames = [f\"s3://{bucket_name}/{key}\" for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_defaults=[\n",
    "    'string',\n",
    "    'int32',\n",
    "    'int32',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string'\n",
    "]\n",
    "avazu_train = tf.data.experimental.make_csv_dataset(\n",
    "    filenames,\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=column_defaults,\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get avazu file names\n",
    "bucket_name = 'mlds-final-project-bucket'\n",
    "folder_name = 'avazu/train/test_split/'\n",
    "keys = list_files_in_folder(bucket_name, folder_name)\n",
    "filenames = [f\"s3://{bucket_name}/{key}\" for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_defaults=[\n",
    "    'string',\n",
    "    'int32',\n",
    "    'int32',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string'\n",
    "]\n",
    "\n",
    "# Load the Avazu train dataset\n",
    "avazu_val = tf.data.experimental.make_csv_dataset(\n",
    "    filenames,\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=column_defaults,\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID variable\n",
    "def drop_id(features, label):\n",
    "    out_features = features.copy()\n",
    "    del out_features['id']\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function drop_id at 0x7f11291df4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function drop_id at 0x7f11291df4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "avazu_train = avazu_train.map(drop_id)\n",
    "avazu_val = avazu_val.map(drop_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get criteo file names\n",
    "bucket_name = 'mlds-final-project-bucket'\n",
    "folder_name = 'dac/train/train_split/'\n",
    "keys = list_files_in_folder(bucket_name, folder_name)\n",
    "filenames = [f\"s3://{bucket_name}/{key}\" for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Criteo train dataset\n",
    "\n",
    "column_defaults = ['int32'] + 13*['int32'] + 26*['string']\n",
    "\n",
    "criteo_train = tf.data.experimental.make_csv_dataset(\n",
    "    filenames,\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    label_name='click',\n",
    "    column_defaults=column_defaults,\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get criteo file names\n",
    "bucket_name = 'mlds-final-project-bucket'\n",
    "folder_name = 'dac/train/test_split/'\n",
    "keys = list_files_in_folder(bucket_name, folder_name)\n",
    "filenames = [f\"s3://{bucket_name}/{key}\" for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Criteo train dataset\n",
    "\n",
    "column_defaults = ['int32'] + 13*['int32'] + 26*['string']\n",
    "\n",
    "criteo_val = tf.data.experimental.make_csv_dataset(\n",
    "    filenames,\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    label_name='click',\n",
    "    column_defaults=column_defaults,\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first perform an exploratory data analysis to show the type of preprocessing that needs to be made. In order to simplify the data processing requirements, I do this on the basis of $n=10000$ row samples of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I construct the preprocessing pipeline for the tensorflow datasets on the basis of the exploration functions above. I proceed by:\n",
    "\n",
    "- Enconding the sparse catecorically features using the Ordinal Encoders specified above.\n",
    "- Standardizing the numerical features unsing the Scalers and Transformer specified above.\n",
    "- Generating the feature columns, as specified in the [DeepCTR Quickstart Guide](https://deepctr-doc.readthedocs.io/en/latest/Quick-Start.html#step-3-generate-feature-columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create lists of categorical colums for each dataset\n",
    "kdd12_categorical_columns = [\n",
    "    'DisplayURL',\n",
    "    'AdID',\n",
    "    'AdvertiserID',\n",
    "    'QueryID',\n",
    "    'KeywordID',\n",
    "    'TitleID',\n",
    "    'DescriptionID',\n",
    "    'UserID'\n",
    "]\n",
    "\n",
    "avazu_categorical_columns = [\n",
    "    'C1',\n",
    "    'banner_pos',\n",
    "    'site_id',\n",
    "    'site_domain',\n",
    "    'site_category',\n",
    "    'app_id',\n",
    "    'app_domain',\n",
    "    'app_category',\n",
    "    'device_id',\n",
    "    'device_ip',\n",
    "    'device_model',\n",
    "    'device_type',\n",
    "    'device_conn_type',\n",
    "    'C14',\n",
    "    'C15',\n",
    "    'C16',\n",
    "    'C17',\n",
    "    'C18',\n",
    "    'C19',\n",
    "    'C20',\n",
    "    'C21'\n",
    "]\n",
    "\n",
    "criteo_numerical_columns = [f'int_{i}' for i in np.arange(1,14)]\n",
    "criteo_categorical_columns = [f'cat_{i}' for i in np.arange(1,27)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import categorical feature mappings and define stringloohup objects for each dataset\n",
    "kdd12_stringlookups = {}\n",
    "for field in kdd12_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/kdd12/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    kdd12_stringlookups.update({field:lookup})\n",
    "\n",
    "avazu_stringlookups = {}\n",
    "for field in avazu_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/avazu/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    avazu_stringlookups.update({field:lookup})\n",
    "\n",
    "criteo_stringlookups = {}\n",
    "for field in criteo_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/criteo/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    criteo_stringlookups.update({field:lookup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical encoding function\n",
    "@tf.function\n",
    "def kdd12_categorical_encoding(features,label):\n",
    "    # Create copy of features, because modifying inputs causes a ValueError\n",
    "    out_features = features.copy()\n",
    "    # Iteratively map the categical feature columns using the corresponging Lookup layer\n",
    "    for f in kdd12_categorical_columns:\n",
    "        lookup = kdd12_stringlookups[f]\n",
    "        out_features[f.lower()] = lookup(features[f.lower()])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def avazu_categorical_encoding(features,label):\n",
    "    # Create copy of features, because modifying inputs causes a ValueError\n",
    "    out_features = features.copy()\n",
    "    # Iteratively map the categical feature columns using the corresponging Lookup layer\n",
    "    for f in avazu_categorical_columns:\n",
    "        lookup = avazu_stringlookups[f]\n",
    "        out_features[f] = lookup(features[f])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def criteo_categorical_encoding(features,label):\n",
    "    # Create copy of features\n",
    "    out_features = features.copy()\n",
    "    # Iteratively map the categorical feature columns using the corresponding Lookup layer\n",
    "    for f in criteo_categorical_columns:\n",
    "        lookup = criteo_stringlookups[f]\n",
    "        out_features[f] = lookup(features[f])\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function kdd12_categorical_encoding at 0x7f111833f700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function kdd12_categorical_encoding at 0x7f111833f700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function avazu_categorical_encoding at 0x7f111833f8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function avazu_categorical_encoding at 0x7f111833f8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function criteo_categorical_encoding at 0x7f111833fa60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function criteo_categorical_encoding at 0x7f111833fa60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# Apply mapping over each dataset\n",
    "\n",
    "kdd12_train_encoded = kdd12_train.map(kdd12_categorical_encoding)\n",
    "avazu_train_encoded = avazu_train.map(avazu_categorical_encoding)\n",
    "criteo_train_encoded = criteo_train.map(criteo_categorical_encoding)\n",
    "\n",
    "kdd12_val_encoded = kdd12_val.map(kdd12_categorical_encoding)\n",
    "avazu_val_encoded = avazu_val.map(avazu_categorical_encoding)\n",
    "criteo_val_encoded = criteo_val.map(criteo_categorical_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Variable Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical feature columns\n",
    "kdd12_numerical_columns = [\n",
    "    'Depth',\n",
    "    'Position',\n",
    "    'Impression'\n",
    "]\n",
    "\n",
    "avazu_numerical_columns = [\n",
    "    'hour'\n",
    "]\n",
    "\n",
    "criteo_numerical_columns = [f'int_{i}' for i in np.arange(1,14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scaler dicts for all datasets\n",
    "dist_stats = pd.read_csv('./data/kdd12/means_variances.csv')\n",
    "kdd12_scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    kdd12_scalers.update({field:scaler})\n",
    "\n",
    "dist_stats = pd.read_csv('./data/avazu/means_variances.csv')\n",
    "avazu_scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    avazu_scalers.update({field:scaler})\n",
    "\n",
    "dist_stats = pd.read_csv('./data/criteo/means_variances.csv')\n",
    "criteo_scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    criteo_scalers.update({field:scaler})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaler functions for all datasets\n",
    "\n",
    "@tf.function\n",
    "def kdd12_numerical_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in kdd12_numerical_columns:\n",
    "        scaler = kdd12_scalers[f]\n",
    "        out_features[f.lower()] = scaler(features[f.lower()])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def avazu_numerical_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in avazu_numerical_columns:\n",
    "        scaler = avazu_scalers[f]\n",
    "        out_features[f] = scaler(features[f])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def criteo_numerical_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in criteo_numerical_columns:\n",
    "        scaler = criteo_scalers[f]\n",
    "        out_features[f] = scaler(features[f])\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function kdd12_numerical_scaling at 0x7f11182c39d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function kdd12_numerical_scaling at 0x7f11182c39d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function avazu_numerical_scaling at 0x7f11182c3a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function avazu_numerical_scaling at 0x7f11182c3a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function criteo_numerical_scaling at 0x7f11182c3b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function criteo_numerical_scaling at 0x7f11182c3b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# Apply the numerical scaling to all datasets\n",
    "kdd12_train_scaled = kdd12_train_encoded.map(kdd12_numerical_scaling)\n",
    "avazu_train_scaled = avazu_train_encoded.map(avazu_numerical_scaling)\n",
    "criteo_train_scaled = criteo_train_encoded.map(criteo_numerical_scaling)\n",
    "\n",
    "kdd12_val_scaled = kdd12_val_encoded.map(kdd12_numerical_scaling)\n",
    "avazu_val_scaled = avazu_val_encoded.map(avazu_numerical_scaling)\n",
    "criteo_val_scaled = criteo_val_encoded.map(criteo_numerical_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KDD12:\n",
      "(OrderedDict([('impression', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('displayurl', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('adid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('advertiserid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('depth', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('position', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('keywordid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('titleid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('descriptionid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('queryid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('userid', TensorSpec(shape=(None,), dtype=tf.int64, name=None))]), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "\n",
      "Avazu:\n",
      "(OrderedDict([('hour', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('C1', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('banner_pos', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('site_id', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('site_domain', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('site_category', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('app_id', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('app_domain', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('app_category', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('device_id', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('device_ip', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('device_model', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('device_type', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('device_conn_type', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C14', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C15', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C16', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C17', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C18', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C19', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C20', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C21', TensorSpec(shape=(None,), dtype=tf.int64, name=None))]), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "\n",
      "Criteo:\n",
      "(OrderedDict([('int_1', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_2', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_3', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_4', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_5', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_6', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_7', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_8', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_9', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_10', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_11', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_12', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('int_13', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), ('cat_1', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_2', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_3', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_4', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_5', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_6', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_7', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_8', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_9', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_10', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_11', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_12', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_13', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_14', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_15', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_16', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_17', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_18', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_19', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_20', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_21', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_22', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_23', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_24', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_25', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_26', TensorSpec(shape=(None,), dtype=tf.int64, name=None))]), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "# Print the element specs\n",
    "print('KDD12:')\n",
    "print(kdd12_train_scaled.element_spec)\n",
    "print()\n",
    "print('Avazu:')\n",
    "print(avazu_train_scaled.element_spec)\n",
    "print()\n",
    "print('Criteo:')\n",
    "print(criteo_train_scaled.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to criteo datase\n",
    "\n",
    "@tf.function\n",
    "def criteo_log_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in criteo_numerical_columns:\n",
    "        x = features[f]\n",
    "        out_features[f] = tf.where(x>2,tf.math.square(tf.math.log(x)),x)\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function criteo_log_scaling at 0x7f11181759d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function criteo_log_scaling at 0x7f11181759d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "criteo_train_scaled = criteo_train_scaled.map(criteo_log_scaling)\n",
    "criteo_val_scaled = criteo_val_scaled.map(criteo_log_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model inputs\n",
    "kdd12_train_model_input = kdd12_train_scaled.take(157440).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "avazu_train_model_input = avazu_train_scaled.take(157440).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "criteo_train_model_input = criteo_train_scaled.take(157440).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "kdd12_val_model_input = kdd12_val_scaled.take(39360).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "avazu_val_model_input = avazu_val_scaled.take(39360).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "criteo_val_model_input = criteo_val_scaled.take(39360).batch(256).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define feature mappings\n",
    "kdd12_fixlen_feature_columns = [SparseFeat(feat.lower(), vocabulary_size=len(kdd12_stringlookups[feat].get_vocabulary()), embedding_dim=4) for feat in kdd12_categorical_columns]\\\n",
    "+ [DenseFeat(feat.lower(),1) for feat in kdd12_numerical_columns]\n",
    "avazu_fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=len(avazu_stringlookups[feat].get_vocabulary()), embedding_dim=4) for feat in avazu_categorical_columns]\\\n",
    "+ [DenseFeat(feat,1) for feat in avazu_numerical_columns]\n",
    "criteo_fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=len(criteo_stringlookups[feat].get_vocabulary()), embedding_dim=4) for feat in criteo_categorical_columns]\\\n",
    "+ [DenseFeat(feat,1) for feat in criteo_numerical_columns]\n",
    "\n",
    "## Generate the dnn and linear feature columns\n",
    "kdd12_dnn_feature_columns = kdd12_fixlen_feature_columns\n",
    "kdd12_linear_feature_columns = kdd12_fixlen_feature_columns\n",
    "\n",
    "avazu_dnn_feature_columns = avazu_fixlen_feature_columns\n",
    "avazu_linear_feature_columns = avazu_fixlen_feature_columns\n",
    "\n",
    "criteo_dnn_feature_columns = criteo_fixlen_feature_columns\n",
    "criteo_linear_feature_columns = criteo_fixlen_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Replication and Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to attain representitive results from beyond the realm of deep learning, below I evaluate two of the most poplarly used shallow models for CTR prediction:\n",
    "\n",
    "- [Logistic regression](https://www.tensorflow.org/guide/core/logistic_regression_core)\n",
    "- Factorization machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KDD12:\n",
      "Log loss: 0.162493229115702\n",
      "ROC AUC: 0.69918173566772\n",
      "Accuracy: 0.95825\n",
      "\n",
      "Avazu:\n",
      "Log loss: 0.41220123857803254\n",
      "ROC AUC: 0.7187724509322894\n",
      "Accuracy: 0.83205\n",
      "\n",
      "Criteo:\n",
      "Log loss: 0.4926044086201233\n",
      "ROC AUC: 0.7461917357547045\n",
      "Accuracy: 0.76775\n"
     ]
    }
   ],
   "source": [
    "%run -i scripts/modelling/fit_lr_models.py\n",
    "%run -i scripts/modelling/score_lr_models.py\n",
    "%run -i scripts/modelling/save_lr_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KDD12:\n",
      "Log loss: 0.31685987446578695\n",
      "ROC AUC: 0.5336269490760196\n",
      "Accuracy: 0.95825\n",
      "\n",
      "Avazu:\n",
      "Log loss: 10.974548402513749\n",
      "ROC AUC: 0.5212335152327499\n",
      "Accuracy: 0.66185\n",
      "\n",
      "Criteo:\n",
      "Log loss: 26.85071959222282\n",
      "ROC AUC: 0.5000335570469798\n",
      "Accuracy: 0.25505\n"
     ]
    }
   ],
   "source": [
    "%run -i scripts/modelling/fit_fm_models.py\n",
    "%run -i scripts/modelling/score_fm_models.py\n",
    "%run -i scripts/modelling/save_fm_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, I will be exploring the following deep models:\n",
    "\n",
    "- Factorization Supported Neural Networks\n",
    "- Product Based Neural Networks\n",
    "- Wide and Deep\n",
    "- DeepFM\n",
    "- Feature Generation by Convolutional Neural Networks\n",
    "- Automatic Feature Interaction (AutoInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization-Machine Supported Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7f10e5e74d90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7f10e5e74d90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# Compile the models\n",
    "kdd12_fnn_model = FNN(kdd12_linear_feature_columns, kdd12_dnn_feature_columns, task='binary')\n",
    "kdd12_fnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "avazu_fnn_model = FNN(avazu_linear_feature_columns, avazu_dnn_feature_columns, task='binary')\n",
    "avazu_fnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "criteo_fnn_model = FNN(criteo_linear_feature_columns, criteo_dnn_feature_columns, task='binary')\n",
    "criteo_fnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_fnn_csvLogger = CSVLogger('logs/kdd12_fnn.csv')\n",
    "avazu_fnn_csvLogger = CSVLogger('logs/avazu_fnn.csv')\n",
    "criteo_fnn_csvLogger = CSVLogger('logs/criteo_fnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_fnn_modelCheckpoint = ModelCheckpoint('models/fnn/kdd12_fnn_model.keras',save_best_only=True)\n",
    "avazu_fnn_modelCheckpoint = ModelCheckpoint('models/fnn/avazu_fnn_model.keras',save_best_only=True)\n",
    "criteo_fnn_modelCheckpoint = ModelCheckpoint('models/fnn/criteo_fnn_model.keras',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 14:25:19.278989: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2024-08-03 14:25:19.302715: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2899990000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "615/615 [==============================] - 154s 246ms/step - loss: 0.2364 - binary_crossentropy: 0.2355 - val_loss: 0.1402 - val_binary_crossentropy: 0.1355\n",
      "Epoch 2/15\n",
      "615/615 [==============================] - 149s 242ms/step - loss: 0.1208 - binary_crossentropy: 0.1149 - val_loss: 0.1829 - val_binary_crossentropy: 0.1738\n",
      "Epoch 3/15\n",
      "615/615 [==============================] - 150s 243ms/step - loss: -0.0030 - binary_crossentropy: -0.0114 - val_loss: 0.2515 - val_binary_crossentropy: 0.2450\n",
      "Epoch 4/15\n",
      "615/615 [==============================] - 151s 245ms/step - loss: -0.0348 - binary_crossentropy: -0.0411 - val_loss: 0.2413 - val_binary_crossentropy: 0.2361\n",
      "Epoch 5/15\n",
      "615/615 [==============================] - 150s 244ms/step - loss: -0.0454 - binary_crossentropy: -0.0505 - val_loss: 0.2962 - val_binary_crossentropy: 0.2916\n",
      "Epoch 6/15\n",
      "615/615 [==============================] - 151s 245ms/step - loss: -0.0495 - binary_crossentropy: -0.0539 - val_loss: 0.3381 - val_binary_crossentropy: 0.3340\n",
      "Epoch 7/15\n",
      "615/615 [==============================] - 152s 248ms/step - loss: -0.0515 - binary_crossentropy: -0.0554 - val_loss: 0.3712 - val_binary_crossentropy: 0.3672\n",
      "Epoch 8/15\n",
      "615/615 [==============================] - 153s 249ms/step - loss: -0.0515 - binary_crossentropy: -0.0553 - val_loss: 0.3997 - val_binary_crossentropy: 0.3959\n",
      "Epoch 9/15\n",
      "615/615 [==============================] - 151s 245ms/step - loss: -0.0529 - binary_crossentropy: -0.0566 - val_loss: 0.4133 - val_binary_crossentropy: 0.4095\n",
      "Epoch 10/15\n",
      "615/615 [==============================] - 151s 246ms/step - loss: -0.0532 - binary_crossentropy: -0.0570 - val_loss: 0.4207 - val_binary_crossentropy: 0.4171\n",
      "Epoch 11/15\n",
      "615/615 [==============================] - 151s 246ms/step - loss: -0.0534 - binary_crossentropy: -0.0570 - val_loss: 0.4652 - val_binary_crossentropy: 0.4616\n",
      "Epoch 12/15\n",
      "615/615 [==============================] - 151s 245ms/step - loss: -0.0509 - binary_crossentropy: -0.0546 - val_loss: 0.3927 - val_binary_crossentropy: 0.3890\n",
      "Epoch 13/15\n",
      "615/615 [==============================] - 152s 247ms/step - loss: -0.0510 - binary_crossentropy: -0.0548 - val_loss: 0.4369 - val_binary_crossentropy: 0.4331\n",
      "Epoch 14/15\n",
      "615/615 [==============================] - 152s 247ms/step - loss: -0.0525 - binary_crossentropy: -0.0562 - val_loss: 0.4558 - val_binary_crossentropy: 0.4522\n",
      "Epoch 15/15\n",
      "615/615 [==============================] - 153s 248ms/step - loss: -0.0543 - binary_crossentropy: -0.0579 - val_loss: 0.4782 - val_binary_crossentropy: 0.4746\n",
      "Epoch 1/15\n",
      "615/615 [==============================] - 74s 112ms/step - loss: 0.4643 - binary_crossentropy: 0.4641 - val_loss: 0.4031 - val_binary_crossentropy: 0.4018\n",
      "Epoch 2/15\n",
      "615/615 [==============================] - 67s 109ms/step - loss: 0.3774 - binary_crossentropy: 0.3755 - val_loss: 0.4312 - val_binary_crossentropy: 0.4271\n",
      "Epoch 3/15\n",
      "615/615 [==============================] - 68s 110ms/step - loss: 0.2905 - binary_crossentropy: 0.2859 - val_loss: 0.4975 - val_binary_crossentropy: 0.4920\n",
      "Epoch 4/15\n",
      "615/615 [==============================] - 68s 109ms/step - loss: 0.2659 - binary_crossentropy: 0.2600 - val_loss: 0.5390 - val_binary_crossentropy: 0.5326\n",
      "Epoch 5/15\n",
      "615/615 [==============================] - 68s 111ms/step - loss: 0.2371 - binary_crossentropy: 0.2306 - val_loss: 0.5841 - val_binary_crossentropy: 0.5774\n",
      "Epoch 6/15\n",
      "615/615 [==============================] - 68s 110ms/step - loss: 0.2223 - binary_crossentropy: 0.2157 - val_loss: 0.6316 - val_binary_crossentropy: 0.6249\n",
      "Epoch 7/15\n",
      "615/615 [==============================] - 70s 113ms/step - loss: 0.2107 - binary_crossentropy: 0.2040 - val_loss: 0.6726 - val_binary_crossentropy: 0.6659\n",
      "Epoch 8/15\n",
      "615/615 [==============================] - 68s 110ms/step - loss: 0.2018 - binary_crossentropy: 0.1952 - val_loss: 0.7199 - val_binary_crossentropy: 0.7132\n",
      "Epoch 9/15\n",
      "615/615 [==============================] - 67s 109ms/step - loss: 0.1941 - binary_crossentropy: 0.1873 - val_loss: 0.7839 - val_binary_crossentropy: 0.7771\n",
      "Epoch 10/15\n",
      "615/615 [==============================] - 68s 111ms/step - loss: 0.1906 - binary_crossentropy: 0.1838 - val_loss: 0.8246 - val_binary_crossentropy: 0.8177\n",
      "Epoch 11/15\n",
      "615/615 [==============================] - 68s 109ms/step - loss: 0.1905 - binary_crossentropy: 0.1836 - val_loss: 0.8128 - val_binary_crossentropy: 0.8057\n",
      "Epoch 12/15\n",
      "615/615 [==============================] - 68s 111ms/step - loss: 0.1890 - binary_crossentropy: 0.1818 - val_loss: 0.8261 - val_binary_crossentropy: 0.8188\n",
      "Epoch 13/15\n",
      "615/615 [==============================] - 68s 111ms/step - loss: 0.1843 - binary_crossentropy: 0.1769 - val_loss: 0.7859 - val_binary_crossentropy: 0.7784\n",
      "Epoch 14/15\n",
      "615/615 [==============================] - 67s 109ms/step - loss: 0.1796 - binary_crossentropy: 0.1721 - val_loss: 0.7648 - val_binary_crossentropy: 0.7572\n",
      "Epoch 15/15\n",
      "615/615 [==============================] - 70s 113ms/step - loss: 0.1747 - binary_crossentropy: 0.1671 - val_loss: 0.8171 - val_binary_crossentropy: 0.8095\n",
      "Epoch 1/15\n",
      "615/615 [==============================] - 353s 566ms/step - loss: 0.5608 - binary_crossentropy: 0.5602 - val_loss: 0.5297 - val_binary_crossentropy: 0.5269\n",
      "Epoch 2/15\n",
      "615/615 [==============================] - 336s 545ms/step - loss: 0.4946 - binary_crossentropy: 0.4905 - val_loss: 0.5319 - val_binary_crossentropy: 0.5235\n",
      "Epoch 3/15\n",
      "615/615 [==============================] - 319s 518ms/step - loss: 0.4472 - binary_crossentropy: 0.4375 - val_loss: 0.5653 - val_binary_crossentropy: 0.5520\n",
      "Epoch 4/15\n",
      "615/615 [==============================] - 314s 511ms/step - loss: 0.3973 - binary_crossentropy: 0.3833 - val_loss: 0.6195 - val_binary_crossentropy: 0.6039\n",
      "Epoch 5/15\n",
      "364/615 [================>.............] - ETA: 1:34 - loss: 0.3500 - binary_crossentropy: 0.3342"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m kdd12_fnn_history \u001b[38;5;241m=\u001b[39m kdd12_fnn_model\u001b[38;5;241m.\u001b[39mfit(kdd12_train_model_input, validation_data\u001b[38;5;241m=\u001b[39mkdd12_val_model_input, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[kdd12_fnn_csvLogger,kdd12_fnn_modelCheckpoint])\n\u001b[1;32m      3\u001b[0m avazu_fnn_history \u001b[38;5;241m=\u001b[39m avazu_fnn_model\u001b[38;5;241m.\u001b[39mfit(avazu_train_model_input, validation_data\u001b[38;5;241m=\u001b[39mavazu_val_model_input, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[avazu_fnn_csvLogger,avazu_fnn_modelCheckpoint])\n\u001b[0;32m----> 4\u001b[0m criteo_fnn_history \u001b[38;5;241m=\u001b[39m \u001b[43mcriteo_fnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriteo_train_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriteo_val_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcriteo_fnn_csvLogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriteo_fnn_modelCheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mlds/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/mlds/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/.conda/envs/mlds/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    852\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    853\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    858\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/mlds/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[1;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mlds/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m     args,\n\u001b[1;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1923\u001b[0m     executing_eagerly)\n\u001b[1;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/mlds/lib/python3.9/site-packages/tensorflow/python/eager/function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/mlds/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the models\n",
    "kdd12_fnn_history = kdd12_fnn_model.fit(kdd12_train_model_input, validation_data=kdd12_val_model_input, batch_size=256, epochs=15, callbacks=[kdd12_fnn_csvLogger,kdd12_fnn_modelCheckpoint])\n",
    "avazu_fnn_history = avazu_fnn_model.fit(avazu_train_model_input, validation_data=avazu_val_model_input, batch_size=256, epochs=15, callbacks=[avazu_fnn_csvLogger,avazu_fnn_modelCheckpoint])\n",
    "criteo_fnn_history = criteo_fnn_model.fit(criteo_train_model_input, validation_data=criteo_val_model_input, batch_size=256, epochs=15, callbacks=[criteo_fnn_csvLogger,criteo_fnn_modelCheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_fnn_history = pd.read_csv('logs/kdd12_fnn.csv')\n",
    "avazu_fnn_history = pd.read_csv('logs/avazu_fnn.csv')\n",
    "criteo_fnn_history = pd.read_csv('logs/criteo_fnn.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_fnn_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_fnn_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_fnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_fnn_history['binary_crossentropy'])\n",
    "plt.plot(avazu_fnn_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_fnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_fnn_history['binary_crossentropy'])\n",
    "plt.plot(criteo_fnn_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_fnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the models\n",
    "kdd12_fnn_score = kdd12_fnn_model.evaluate(kdd12_val_model_input, batch_size=256)\n",
    "avazu_fnn_score = avazu_fnn_model.evaluate(avazu_val_model_input, batch_size=256)\n",
    "criteo_fnn_score = criteo_fnn_model.evaluate(criteo_val_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Based Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_pnn_csvLogger = CSVLogger('logs/kdd12_pnn.csv')\n",
    "avazu_pnn_csvLogger = CSVLogger('logs/avazu_pnn.csv')\n",
    "criteo_pnn_csvLogger = CSVLogger('logs/criteo_pnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_pnn_modelCheckpoint = ModelCheckpoint('models/pnn/kdd12_pnn_model.keras',save_best_only=True)\n",
    "avazu_pnn_modelCheckpoint = ModelCheckpoint('models/pnn/avazu_pnn_model.keras',save_best_only=True)\n",
    "criteo_pnn_modelCheckpoint = ModelCheckpoint('models/pnn/criteo_pnn_model.keras',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the PNN models\n",
    "kdd12_pnn_model = PNN(kdd12_dnn_feature_columns, task='binary')\n",
    "kdd12_pnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "avazu_pnn_model = PNN(avazu_dnn_feature_columns, task='binary')\n",
    "avazu_pnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "criteo_pnn_model = PNN(criteo_dnn_feature_columns, task='binary')\n",
    "criteo_pnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "kdd12_pnn_history = kdd12_pnn_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data=kdd12_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=40, \n",
    "    callbacks=[kdd12_pnn_csvLogger,kdd12_pnn_modelCheckpoint]\n",
    ")\n",
    "avazu_pnn_history = avazu_pnn_model.fit(\n",
    "    avazu_train_model_input,\n",
    "    validation_data=avazu_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=40,\n",
    "    callbacks=[avazu_pnn_csvLogger,avazu_pnn_modelCheckpoint]\n",
    ")\n",
    "criteo_pnn_history = criteo_pnn_model.fit(\n",
    "    criteo_train_model_input,\n",
    "    validation_data=criteo_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=40,\n",
    "    callbacks=[criteo_pnn_csvLogger,criteo_pnn_modelCheckpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_history = pd.read_csv('logs/kdd12_pnn.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_pnn.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_pnn.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_pnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_pnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_pnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide and Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the WDL models\n",
    "kdd12_wdl_model = WDL(kdd12_linear_feature_columns, kdd12_dnn_feature_columns, task='binary')\n",
    "kdd12_wdl_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "avazu_wdl_model = WDL(avazu_linear_feature_columns, avazu_dnn_feature_columns, task='binary')\n",
    "avazu_wdl_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "criteo_wdl_model = WDL(criteo_linear_feature_columns, criteo_dnn_feature_columns, task='binary')\n",
    "criteo_wdl_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_wdl_csvLogger = CSVLogger('logs/kdd12_wdl.csv')\n",
    "avazu_wdl_csvLogger = CSVLogger('logs/avazu_wdl.csv')\n",
    "criteo_wdl_csvLogger = CSVLogger('logs/criteo_wdl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_wdl_modelCheckpoint = ModelCheckpoint('models/wdl/kdd12_wdl_model.keras',save_best_only=True)\n",
    "avazu_wdl_modelCheckpoint = ModelCheckpoint('models/wdl/avazu_wdl_model.keras',save_best_only=True)\n",
    "criteo_wdl_modelCheckpoint = ModelCheckpoint('models/wdl/criteo_wdl_model.keras',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Wide and Deep models\n",
    "kdd12_wdl_history = kdd12_wdl_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data=kdd12_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[kdd12_wdl_csvLogger,kdd12_wdl_modelCheckpoint]\n",
    ")\n",
    "avazu_wdl_history = avazu_wdl_model.fit(\n",
    "    avazu_train_model_input, \n",
    "    validation_data=avazu_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[avazu_wdl_csvLogger,avazu_wdl_modelCheckpoint]\n",
    ")\n",
    "criteo_wdl_history = criteo_wdl_model.fit(\n",
    "    criteo_train_model_input, \n",
    "    validation_data=criteo_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[criteo_wdl_csvLogger, criteo_wdl_modelCheckpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_history = pd.read_csv('logs/kdd12_wdl.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_wdl.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_wdl.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_wdl_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_wdl_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_wdl_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the DeepFM models\n",
    "kdd12_dfm_model = DeepFM(kdd12_linear_feature_columns, kdd12_dnn_feature_columns, task='binary')\n",
    "kdd12_dfm_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "avazu_dfm_model = DeepFM(avazu_linear_feature_columns, avazu_dnn_feature_columns, task='binary')\n",
    "avazu_dfm_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "criteo_dfm_model = DeepFM(criteo_linear_feature_columns, criteo_dnn_feature_columns, task='binary')\n",
    "criteo_dfm_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_dfm_csvLogger = CSVLogger('logs/kdd12_dfm.csv')\n",
    "avazu_dfm_csvLogger = CSVLogger('logs/avazu_dfm.csv')\n",
    "criteo_dfm_csvLogger = CSVLogger('logs/criteo_dfm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_dfm_modelCheckpoint = ModelCheckpoint('models/dfm/kdd12_dfm_model.keras',save_best_only=True)\n",
    "avazu_dfm_modelCheckpoint = ModelCheckpoint('models/dfm/avazu_dfm_model.keras',save_best_only=True)\n",
    "criteo_dfm_modelCheckpoint = ModelCheckpoint('models/dfm/criteo_dfm_model.keras',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the DeepFM models\n",
    "kdd12_dfm_history = kdd12_dfm_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data = kdd12_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[kdd12_dfm_csvLogger,kdd12_wdl_modelCheckpoint]\n",
    ")\n",
    "avazu_dfm_history = avazu_dfm_model.fit(\n",
    "    avazu_train_model_input, \n",
    "    validation_data= avazu_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[avazu_dfm_csvLogger,avazu_wdl_modelCheckpoint]\n",
    ")\n",
    "criteo_dfm_history = criteo_dfm_model.fit(\n",
    "    criteo_train_model_input, \n",
    "    validation_data=criteo_val_model_input, \n",
    "    batch_size=256, epochs=15,\n",
    "    callbacks=[criteo_dfm_csvLogger,criteo_dfm_modelCheckpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_dfm_history = pd.read_csv('logs/kdd12_dfm.csv')\n",
    "avazu_dfm_history = pd.read_csv('logs/avazu_dfm.csv')\n",
    "criteo_dfm_history = pd.read_csv('logs/criteo_dfm.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_dfm_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_dfm_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_dfm_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_dfm_history['binary_crossentropy'])\n",
    "plt.plot(avazu_dfm_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_dfm_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_dfm_history['binary_crossentropy'])\n",
    "plt.plot(criteo_dfm_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_dfm_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Feature Interaction (AutoInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the AutoInt Models\n",
    "kdd12_autoint_model = AutoInt(kdd12_linear_feature_columns, kdd12_dnn_feature_columns, task='binary')\n",
    "kdd12_autoint_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "avazu_autoint_model = AutoInt(avazu_linear_feature_columns, avazu_dnn_feature_columns, task='binary')\n",
    "avazu_autoint_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "criteo_autoint_model = AutoInt(criteo_linear_feature_columns, criteo_dnn_feature_columns, task='binary')\n",
    "criteo_autoint_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AutoInt CSVLogger callbacks\n",
    "kdd12_autoint_csvLogger = CSVLogger('logs/kdd12_autoint.csv')\n",
    "avazu_autoint_csvLogger = CSVLogger('logs/avazu_autoint.csv')\n",
    "criteo_autoint_csvLogger = CSVLogger('logs/criteo_autoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_autoint_modelCheckpoint = ModelCheckpoint('models/autoint/kdd12_autoint_model.keras',save_best_only=True)\n",
    "avazu_autoint_modelCheckpoint = ModelCheckpoint('models/autoint/avazu_autoint_model.keras',save_best_only=True)\n",
    "criteo_autoint_modelCheckpoint = ModelCheckpoint('models/autoint/criteo_autoint_model.keras',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the AutoInt models\n",
    "kdd12_autoint_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data= kdd12_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15, \n",
    "    callbacks=[kdd12_autoint_csvLogger, kdd12_autoint_modelCheckpoint]\n",
    ")\n",
    "avazu_autoint_model.fit(\n",
    "    avazu_train_model_input,\n",
    "    validation_data=avazu_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=15, \n",
    "    callbacks=[avazu_autoint_csvLogger, avazu_autoint_modelCheckpoint]\n",
    ")\n",
    "criteo_autoint_model.fit(\n",
    "    criteo_train_model_input, \n",
    "    validation_data=criteo_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[criteo_autoint_csvLogger,criteo_autoint_modelCheckpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_autoint_history = pd.read_csv('logs/kdd12_autoint.csv')\n",
    "avazu_autoint_history = pd.read_csv('logs/avazu_autoint.csv')\n",
    "criteo_autoint_history = pd.read_csv('logs/criteo_autoint.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_autoint_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_autoint_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_autoint_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_autoint_history['binary_crossentropy'])\n",
    "plt.plot(avazu_autoint_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_autoint_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_autoint_history['binary_crossentropy'])\n",
    "plt.plot(criteo_autoint_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_autoint_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation by convolutional Neural Network - FGCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the FGCNN models\n",
    "\n",
    "#kdd12_fgcnn_model = FGCNN(kdd12_linear_feature_columns, kdd12_dnn_feature_columns, task='binary')\n",
    "#kdd12_fgcnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "avazu_fgcnn_model = FGCNN(\n",
    "    avazu_linear_feature_columns, \n",
    "    avazu_dnn_feature_columns,\n",
    "    conv_kernel_width=[7,7,7,7],\n",
    "    conv_filters=[14,16,18,20],\n",
    "    new_maps=[3,3,3,3],\n",
    "    dnn_hidden_units=[4096,2048,1024,512,1],# This was different\n",
    "    task='binary'\n",
    ")\n",
    "avazu_fgcnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "criteo_fgcnn_model = FGCNN(\n",
    "    avazu_linear_feature_columns, \n",
    "    avazu_dnn_feature_columns,\n",
    "    conv_kernel_width=[9,9,9,9],\n",
    "    conv_filters=[38,40,42,44],\n",
    "    new_maps=[3,3,3,3],\n",
    "    dnn_hidden_units=[4096,2048,1],# This was different\n",
    "    task='binary'\n",
    ")\n",
    "criteo_fgcnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AutoInt CSVLogger callbacks\n",
    "kdd12_fgcnn_csvLogger = CSVLogger('logs/kdd12_fgcnn.csv')\n",
    "avazu_fgcnn_csvLogger = CSVLogger('logs/avazu_fgcnn.csv')\n",
    "criteo_fgcnn_csvLogger = CSVLogger('logs/criteo_fgcnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_fgcnn_modelCheckpoint = ModelCheckpoint('models/fgcnn/kdd12_fgcnn_model.keras',save_best_only=True)\n",
    "avazu_fgcnn_modelCheckpoint = ModelCheckpoint('models/fgcnn/avazu_fgcnn_model.keras',save_best_only=True)\n",
    "criteo_fgcnn_modelCheckpoint = ModelCheckpoint('models/fgcnn/criteo_fgcnn_model.keras',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avazu_fgcnn_history = avazu_fgcnn_model.fit(\n",
    "    avazu_train_model_input,\n",
    "    validation_data=avazu_val_model_input,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[avazu_fgcnn_csvLogger,avazu_fgcnn_modelCheckpoint]\n",
    ")\n",
    "criteo_fgcnn_history = criteo_fgcnn_model.fit(\n",
    "    criteo_train_model_input,\n",
    "    validation_data=criteo_val_model_input,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[criteo_fgcnn_csvLogger,criteo_fgcnn_modelCheckpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "#kdd12_history = pd.read_csv('logs/kdd12_wdl.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_fgcnn.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_fgcnn.csv')\n",
    "\n",
    "# Plot the training history\n",
    "'''\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_wdl_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_fgcnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_fgcnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlds)",
   "language": "python",
   "name": "mlds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
