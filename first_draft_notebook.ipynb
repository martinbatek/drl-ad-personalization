{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 15:37:11.251553: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-06 15:37:11.251603: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-06 15:37:11.251613: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-06 15:37:11.259139: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "## General\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "## Data manipulation and preprocessing\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer,LabelEncoder, MinMaxScaler,OrdinalEncoder\n",
    "#import missingno as msno\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, MissingIndicator\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from scipy import sparse\n",
    "from tensorflow.keras.layers import StringLookup, Normalization\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "\n",
    "## Modelling\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input, Concatenate, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import DeepCTR code\n",
    "deepctr_path = '/home/sagemaker-user/drl-ad-personalization/DeepCTR'\n",
    "if deepctr_path not in sys.path:\n",
    "    sys.path.append(deepctr_path)\n",
    "\n",
    "from deepctr.models.fnn import FNN\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr.models.pnn import PNN\n",
    "from deepctr.models.wdl import WDL\n",
    "from deepctr.models.deepfm import DeepFM\n",
    "from deepctr.models.autoint import AutoInt\n",
    "from deepctr.models.fgcnn import FGCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "gpu_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The global digital advertising market is worth approximately \\\\$602 billion today. Due to the increasing rate of of online participation since the \n",
    "COVID-19 pandemic, this number has been rapidly increasing and is expected to reach \\\\$871 billion by the end of 2027 \\citep{RefWorks:emarketer2023digital}.\n",
    "Many of the of the major Ad platforms such as Google, Facebook and Amazon operate on a cost-per-user-engagement pricing model, which usually means that \n",
    "advertisers get charged for every time a user clicks on an advertisment. This means that these platforms are incentivized to make sure that the content \n",
    "shown to each user is as relevent as possible in order to maximize the number of clicks in the long term. Attaining accurate Click-Through Rate (CTR) \n",
    "prediction is a necessary first step for Ad persionalization, which is why study of CTR prediction methods have been an extremely active part of \n",
    "Machine Learning research over the past through years.\n",
    "\n",
    "Initially, shallow prediction methods such as Logistic Regression, Factorization Machines (Rendel, 2020) and Field-Aware Factorization \n",
    "Machines \\citep{RefWorks:juan2016field-aware} have been used for CTR prediction. However, these methods have often been shown to be unable to capture the \n",
    "higher order feature interactions in the sparse multi-value categorical Ad Marketplace datasets \\citep{RefWorks:zhang2021deep}. Since then, Deep Learning methods have been \n",
    "shown to show superior predictive ability on these datasets. The focus of my reasearch project is therefore to explore the merits of different Deep \n",
    "Learning architechtures for click-through rate prediction.\n",
    "\n",
    "A number of Deep Learning models have been proposed for CTR prediction, some of which will be explored in this report. Each of these models\n",
    "outperform their shallow counterparts in terms of predictive ability. In a static environment, these models are able to serve the CTR prediction\n",
    "function of Ad personalization, but in a dynamic environment, the model must be able to adapt to the changing user preferences. This is where\n",
    "Reinforcement Learning comes in. Reinforcement Learning is a type of Machine Learning that is used to make a sequence of decisions in an environment\n",
    "in order to maximize some notion of cumulative reward. In the context of Ad personalization, the environment is composed of the user, the Ad platform\n",
    "and the advertisments, whereas the reward is the users' engagement with the advertisments and with the Ad platform.\n",
    "\n",
    "\n",
    "As outlined in the Feasibility secition of my Milestone 3 submission, I have segmented the work over the next few weeks according to the following timeline\n",
    "\n",
    "![](figures/timeline.png)\n",
    "\n",
    "Below I proceed by structuring my response according to the following sections\n",
    "- General Background to Deep CTR and Deep Renforcement Learning\n",
    "- Data exploration, preprocessing and environment setup\n",
    "- Model Introduction, Replication and Experimentation\n",
    "\n",
    "Please note that this notebook illustrates my current work in progress, and certain sections are far from finished. I have also concurrently begun sections of my final write up using the LaTeX template provided. This is available along with the rest of my code in the following Github repository: https://github.com/martinbatek/drl-ad-personalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CTR Prediction\n",
    "\n",
    "In their respective surveys on the use of Deep Learning methods for CTR prediction, (Gu, 2021) and (Zhang et al., 2021) outline the \n",
    "problem of CTR prediction as one that essentially boils down to a binary (click/no-click) classification problem utilizing user/ad-view event level \n",
    "online session records. The goal of CTR prediction is to predict the probability of a user clicking on an advertisment given the information available\n",
    "about the user, advertisment and the context in which the advertisement is shown. Suppose that $\\mathbf{x} \\in \\mathbb{R}^n$\n",
    "is a vector of features that describes the user, ad and platform for a given instance, and\n",
    "$y \\in \\{0, 1\\}$ is the binary label indicating whether the user clicked on the ad or not. The goal of CTR prediction is to learn a function\n",
    "$f: \\mathbb{R}^n \\rightarrow (0,1)$ such that:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbb{P}(y = 1 | \\mathbf{x}) = \\mathbb{P}(\\text{click}| \\mathbf{x})\n",
    "$$\n",
    "\n",
    "In other words, for a given set of features $\\mathbf{x}$, the model should output the probability that the user will click on the ad.\n",
    "A defining characteristic for this type of data is that many of the features are multi-value categories with a high degree of of cardinality \\citep{RefWorks:he2017neural}. \n",
    "This in turn means that the ad marketplace datasets used for CTR predictions can be extremely sparse, which increases the difficulty of the classification problem at \n",
    "hand \\citep{RefWorks:gu2021ad}.\n",
    "\n",
    "A key requirement for CTR modelling is therefore working out which of the many sparse features and feature \n",
    "interactions (combinations of two or more features) are significant for determining the correct prediction \n",
    "(Gu, 2021). Factorization Machines (Rendle, 2010) and Field-aware Factorization Machines (Juan et al., 2016) \n",
    "were popularized shallow modelling methods that explicitly account for first order interactions between features. \n",
    "However, these techniques do not capture higher order interactions (combinations of three or more features) and \n",
    "have thus been known to perform poorly in scenarios with highly sparse data (Zhang et al., 2021). Since 2015 the \n",
    "research cummunity has been increasingly turning to Deep Learning techniques to enhance prior CTR prediction \n",
    "techniques (such as in the case of DeepFM (Guo et al., 2017)), as well as to develop novel approaches. Neural based \n",
    "network models excel at simulataneoulsy extracting high-order and low-order feature interations virtue to the use \n",
    "of pooling layers, multiple hidden layers and activation units (Gu, 2021). Due the aforementioned importance of \n",
    "feature interation modelling, a number of Feature Interaction Operator layers have been developed to explicitly \n",
    "capture the key combinations of features. These layers are then typically incorporated with a supplimentary Deep \n",
    "Neural Network in a single or dual tower architecture, as shown in the Figure below:\n",
    "\n",
    "![DNN_Architecture](figures/single_dual_dnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Interaction Operators can be categorized as either Product Operators, Convolutional Operators or Attention Operators. Product Operators such as the Product-based Neural Network (PNN) (Qu et al., 2016), Neural Factorization Machines (NFM) (He & Chua, 2017), Deep and Cross Network (Wang et al., 2017) and Gated Deep Cross Network (GDCN) (Wang, F. et al., 2023) introduce a product layer between the categorical feature embedding layer and the rest of the neural network in order to explicitly model the important feature interactions. Convolutional Operators such as the Convolutional Click Prediction Model (CCPM) (Liu et al., 2015) utilized convolution, pooling and non-linear activation in order to calculate arbitrary-order interactions. Finally, Attention Operators such as Attentional Factorization Machines (AFM) (Xiao et al., 2017), AutoInt (Song et al., 2019) and Interpretable CTR prediction model with Hierarchical Attention (InterHAt) (Li et al., 2020) utilize the attention mechanism to enable different feature interactions to contribute differently to the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their survey, (Wang, X. et al., 2024) describe how deep reinforcement learning combines the aforementioned feature extraction capabilities of DNN’s with the decision-making capability of reinforcement learning, which aims to learn an optimal state-action policy which maximizes the expected reward gained in a given environment. In the context of recommendation systems, a significant amount of research has been dedicated to formulating the recommendation problem as a Contextual Multi-Armed Bandit (MAB) problem setting, where the context consists of user, site and item features (Bouneffouf, Bouzeghoub & Gancarski, 2012; Li, L. et al., 2010; Zeng et al., 2016). However, a shortcoming for the MAB approach is that it does not explicitly model the future expected reward for the policy, which may be detrimental in the longer term (Zheng et al., 2018). Markov Decision Process (MDP) models solve for this issue by modelling the state-action progression as a Markov Process, allowing for the stochastic valuation of the future potential rewards for a given recommendation policy (Lu & Yang, 2016; Mahmood & Ricci, 2007). DRN (Zheng et al., 2018) is a MDP framework that leverages a Deep Neural Network to approximate the expected total user response for each recommendation at each state. The two major advantages of DRN are firstly that it is composed on the basis of a continuous state and action representation, meaning that it can be scaled to large and sparse datasets, and secondly that the proposed reward function consists of both the immediate reward (user click) as well as the future expected reward (long term user engagement), thereby allowing for better recommendations over a user’s lifetime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration, Caching, Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD - Include data Exploration sections from Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load benchmark datasets as tf.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the kdd12_train dataset\n",
    "kdd12_train = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/kdd12/train_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=['int32','int32','string','string','string','int32','int32','string','string','string','string','string'],\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the kdd12_train dataset\n",
    "kdd12_val = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/kdd12/test_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=['int32','int32','string','string','string','int32','int32','string','string','string','string','string'],\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_defaults=[\n",
    "    'string',\n",
    "    'int32',\n",
    "    'int32',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string'\n",
    "]\n",
    "avazu_train = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/avazu/train_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=column_defaults,\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_defaults=[\n",
    "    'string',\n",
    "    'int32',\n",
    "    'int32',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string'\n",
    "]\n",
    "\n",
    "# Load the Avazu train dataset\n",
    "avazu_val = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/avazu/test_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=column_defaults,\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID variable\n",
    "def drop_id(features, label):\n",
    "    out_features = features.copy()\n",
    "    del out_features['id']\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "avazu_train = avazu_train.map(drop_id)\n",
    "avazu_val = avazu_val.map(drop_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sagemaker-user/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/readers.py:573: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.ignore_errors` instead.\n"
     ]
    }
   ],
   "source": [
    "# Load the Criteo train dataset\n",
    "\n",
    "column_defaults = ['int32'] + 13*['int32'] + 26*['string']\n",
    "\n",
    "criteo_train = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/criteo/train_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    label_name='click',\n",
    "    column_defaults=column_defaults,\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Criteo train dataset\n",
    "\n",
    "column_defaults = ['int32'] + 13*['int32'] + 26*['string']\n",
    "\n",
    "criteo_val = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/criteo/test_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    label_name='click',\n",
    "    column_defaults=column_defaults,\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I construct the preprocessing pipeline for the tensorflow datasets on the basis of the exploration functions above. I proceed by:\n",
    "\n",
    "- Enconding the sparse catecorically features using the Ordinal Encoders specified above.\n",
    "- Standardizing the numerical features unsing the Scalers and Transformer specified above.\n",
    "- Generating the feature columns, as specified in the [DeepCTR Quickstart Guide](https://deepctr-doc.readthedocs.io/en/latest/Quick-Start.html#step-3-generate-feature-columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create lists of categorical colums for each dataset\n",
    "kdd12_categorical_columns = [\n",
    "    'DisplayURL',\n",
    "    'AdID',\n",
    "    'AdvertiserID',\n",
    "    'QueryID',\n",
    "    'KeywordID',\n",
    "    'TitleID',\n",
    "    'DescriptionID',\n",
    "    'UserID'\n",
    "]\n",
    "\n",
    "avazu_categorical_columns = [\n",
    "    'C1',\n",
    "    'banner_pos',\n",
    "    'site_id',\n",
    "    'site_domain',\n",
    "    'site_category',\n",
    "    'app_id',\n",
    "    'app_domain',\n",
    "    'app_category',\n",
    "    'device_id',\n",
    "    'device_ip',\n",
    "    'device_model',\n",
    "    'device_type',\n",
    "    'device_conn_type',\n",
    "    'C14',\n",
    "    'C15',\n",
    "    'C16',\n",
    "    'C17',\n",
    "    'C18',\n",
    "    'C19',\n",
    "    'C20',\n",
    "    'C21'\n",
    "]\n",
    "\n",
    "criteo_numerical_columns = [f'int_{i}' for i in np.arange(1,14)]\n",
    "criteo_categorical_columns = [f'cat_{i}' for i in np.arange(1,27)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import categorical feature mappings and define stringloohup objects for each dataset\n",
    "kdd12_stringlookups = {}\n",
    "kdd12_vocab_lengths = {}\n",
    "for field in kdd12_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/kdd12/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    kdd12_stringlookups.update({field:lookup})\n",
    "    kdd12_vocab_lengths.update({field:len(vocab)+1})\n",
    "\n",
    "avazu_stringlookups = {}\n",
    "avazu_vocab_lengths = {}\n",
    "for field in avazu_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/avazu/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    avazu_stringlookups.update({field:lookup})\n",
    "    avazu_vocab_lengths.update({field:len(vocab)+1})\n",
    "\n",
    "criteo_stringlookups = {}\n",
    "criteo_vocab_lengths = {}\n",
    "for field in criteo_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/criteo/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    criteo_stringlookups.update({field:lookup})\n",
    "    criteo_vocab_lengths.update({field:len(vocab)+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical encoding function\n",
    "@tf.function\n",
    "def kdd12_categorical_encoding(features,label):\n",
    "    # Create copy of features, because modifying inputs causes a ValueError\n",
    "    out_features = features.copy()\n",
    "    # Iteratively map the categical feature columns using the corresponging Lookup layer\n",
    "    for f in kdd12_categorical_columns:\n",
    "        lookup = kdd12_stringlookups[f]\n",
    "        out_features[f.lower()] = lookup(features[f.lower()])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def avazu_categorical_encoding(features,label):\n",
    "    # Create copy of features, because modifying inputs causes a ValueError\n",
    "    out_features = features.copy()\n",
    "    # Iteratively map the categical feature columns using the corresponging Lookup layer\n",
    "    for f in avazu_categorical_columns:\n",
    "        lookup = avazu_stringlookups[f]\n",
    "        out_features[f] = lookup(features[f])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def criteo_categorical_encoding(features,label):\n",
    "    # Create copy of features\n",
    "    out_features = features.copy()\n",
    "    # Iteratively map the categorical feature columns using the corresponding Lookup layer\n",
    "    for f in criteo_categorical_columns:\n",
    "        lookup = criteo_stringlookups[f]\n",
    "        out_features[f] = lookup(features[f])\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mapping over each dataset\n",
    "\n",
    "kdd12_train_encoded = kdd12_train.map(kdd12_categorical_encoding)\n",
    "avazu_train_encoded = avazu_train.map(avazu_categorical_encoding)\n",
    "criteo_train_encoded = criteo_train.map(criteo_categorical_encoding)\n",
    "\n",
    "kdd12_val_encoded = kdd12_val.map(kdd12_categorical_encoding)\n",
    "avazu_val_encoded = avazu_val.map(avazu_categorical_encoding)\n",
    "criteo_val_encoded = criteo_val.map(criteo_categorical_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Variable Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical feature columns\n",
    "kdd12_numerical_columns = [\n",
    "    'Depth',\n",
    "    'Position',\n",
    "    'Impression'\n",
    "]\n",
    "\n",
    "avazu_numerical_columns = [\n",
    "    'hour'\n",
    "]\n",
    "\n",
    "criteo_numerical_columns = [f'int_{i}' for i in np.arange(1,14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scaler dicts for all datasets\n",
    "dist_stats = pd.read_csv('./data/kdd12/means_variances.csv')\n",
    "kdd12_scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    scaler.build((1,))\n",
    "    kdd12_scalers.update({field:scaler})\n",
    "\n",
    "dist_stats = pd.read_csv('./data/avazu/means_variances.csv')\n",
    "avazu_scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    scaler.build((1,))\n",
    "    avazu_scalers.update({field:scaler})\n",
    "\n",
    "dist_stats = pd.read_csv('./data/criteo/means_variances.csv')\n",
    "criteo_scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    scaler.build((1,))\n",
    "    criteo_scalers.update({field:scaler})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaler functions for all datasets\n",
    "\n",
    "@tf.function\n",
    "def kdd12_numerical_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in kdd12_numerical_columns:\n",
    "        scaler = kdd12_scalers[f]\n",
    "        out_features[f.lower()] = scaler(features[f.lower()])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def avazu_numerical_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in avazu_numerical_columns:\n",
    "        scaler = avazu_scalers[f]\n",
    "        out_features[f] = scaler(features[f])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def criteo_numerical_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in criteo_numerical_columns:\n",
    "        scaler = criteo_scalers[f]\n",
    "        out_features[f] = scaler(features[f])\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the numerical scaling to all datasets\n",
    "kdd12_train_scaled = kdd12_train_encoded.map(kdd12_numerical_scaling)\n",
    "avazu_train_scaled = avazu_train_encoded.map(avazu_numerical_scaling)\n",
    "criteo_train_scaled = criteo_train_encoded.map(criteo_numerical_scaling)\n",
    "\n",
    "kdd12_val_scaled = kdd12_val_encoded.map(kdd12_numerical_scaling)\n",
    "avazu_val_scaled = avazu_val_encoded.map(avazu_numerical_scaling)\n",
    "criteo_val_scaled = criteo_val_encoded.map(criteo_numerical_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KDD12:\n",
      "(OrderedDict([('impression', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('displayurl', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('adid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('advertiserid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('depth', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('position', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('keywordid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('titleid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('descriptionid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('queryid', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('userid', TensorSpec(shape=(None,), dtype=tf.int64, name=None))]), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "\n",
      "Avazu:\n",
      "(OrderedDict([('hour', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('C1', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('banner_pos', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('site_id', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('site_domain', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('site_category', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('app_id', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('app_domain', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('app_category', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('device_id', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('device_ip', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('device_model', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('device_type', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('device_conn_type', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C14', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C15', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C16', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C17', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C18', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C19', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C20', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('C21', TensorSpec(shape=(None,), dtype=tf.int64, name=None))]), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "\n",
      "Criteo:\n",
      "(OrderedDict([('int_1', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_2', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_3', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_4', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_5', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_6', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_7', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_8', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_9', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_10', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_11', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_12', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('int_13', TensorSpec(shape=(None,), dtype=tf.float32, name=None)), ('cat_1', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_2', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_3', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_4', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_5', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_6', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_7', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_8', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_9', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_10', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_11', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_12', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_13', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_14', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_15', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_16', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_17', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_18', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_19', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_20', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_21', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_22', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_23', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_24', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_25', TensorSpec(shape=(None,), dtype=tf.int64, name=None)), ('cat_26', TensorSpec(shape=(None,), dtype=tf.int64, name=None))]), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "# Print the element specs\n",
    "print('KDD12:')\n",
    "print(kdd12_train_scaled.element_spec)\n",
    "print()\n",
    "print('Avazu:')\n",
    "print(avazu_train_scaled.element_spec)\n",
    "print()\n",
    "print('Criteo:')\n",
    "print(criteo_train_scaled.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to criteo datase\n",
    "\n",
    "@tf.function\n",
    "def criteo_log_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in criteo_numerical_columns:\n",
    "        x = features[f]\n",
    "        out_features[f] = tf.where(x>2,tf.math.square(tf.math.log(x)),x)\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criteo_train_scaled = criteo_train_scaled.map(criteo_log_scaling)\n",
    "criteo_val_scaled = criteo_val_scaled.map(criteo_log_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model inputs\n",
    "kdd12_train_model_input = kdd12_train_scaled.take(157440).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "avazu_train_model_input = avazu_train_scaled.take(157440).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "criteo_train_model_input = criteo_train_scaled.take(157440).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "kdd12_val_model_input = kdd12_val_scaled.take(39360).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "avazu_val_model_input = avazu_val_scaled.take(39360).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "criteo_val_model_input = criteo_val_scaled.take(39360).batch(256).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate the features from the ordered dictionary structure for the shallow model section\n",
    "## Define concat function\n",
    "@tf.function\n",
    "def concat_features(features,label,numerical_cols):\n",
    "    out_features_list = []\n",
    "    field_list = list(features.keys())\n",
    "    for field in field_list:\n",
    "        out_features_list.append(tf.cast(features[field],tf.float32))\n",
    "    out_features = Concatenate()(out_features_list)\n",
    "    return out_features, label\n",
    "\n",
    "## Map the function to each of the datasets to concatinate the features into a single tensor\n",
    "kdd12_numerical_lower = [x.lower() for x in kdd12_numerical_columns]\n",
    "kdd12_train_concat = kdd12_train_model_input.map(lambda x, y: concat_features(x,y,kdd12_numerical_lower))\n",
    "kdd12_val_concat = kdd12_val_model_input.map(lambda x, y: concat_features(x,y,kdd12_numerical_lower))\n",
    "avazu_train_concat = avazu_train_model_input.map(lambda x, y: concat_features(x, y, avazu_numerical_columns))\n",
    "avazu_val_concat = avazu_val_model_input.map(lambda x, y: concat_features(x, y, avazu_numerical_columns))\n",
    "criteo_train_concat = criteo_train_model_input.map(lambda x, y: concat_features(x, y, criteo_numerical_columns))\n",
    "criteo_val_concat = criteo_val_model_input.map(lambda x, y: concat_features(x, y, criteo_numerical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot encode the categorical columns\n",
    "@tf.function\n",
    "def one_hot_categorical(features, labels, categorical_columns):\n",
    "    features_out = features.copy()\n",
    "    for field in categorical_columns:\n",
    "        features_out[field] = tf.squeeze(tf.one_hot(features[field],depth=50,dtype=tf.float32),axis=-2)\n",
    "    return features_out, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the datasets\n",
    "kdd12_categorical_lower = [x.lower() for x in kdd12_categorical_columns]\n",
    "\n",
    "kdd12_train_ohe = kdd12_train_model_input.map(lambda x, y: one_hot_categorical(x,y,kdd12_categorical_lower))\n",
    "kdd12_val_ohe = kdd12_val_model_input.map(lambda x, y: one_hot_categorical(x,y,kdd12_categorical_lower))\n",
    "avazu_train_ohe = avazu_train_model_input.map(lambda x, y: one_hot_categorical(x,y,avazu_categorical_columns))\n",
    "avazu_val_ohe = avazu_val_model_input.map(lambda x, y: one_hot_categorical(x,y,avazu_categorical_columns))\n",
    "criteo_train_ohe = criteo_train_model_input.map(lambda x, y: one_hot_categorical(x,y,criteo_categorical_columns))\n",
    "criteo_val_ohe = criteo_val_model_input.map(lambda x, y: one_hot_categorical(x,y,criteo_categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to concatinate all features together\n",
    "@tf.function\n",
    "def combine(features, labels):\n",
    "    columns = list(features.keys())\n",
    "    out_cols = []\n",
    "    for field in columns:\n",
    "        out_cols.append(features[field])\n",
    "    features_out = tf.concat(out_cols,axis=-1)\n",
    "    return features_out, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd12_train_ohe = kdd12_train_ohe.map(combine)\n",
    "kdd12_val_ohe = kdd12_val_ohe.map(combine)\n",
    "avazu_train_ohe = avazu_train_ohe.map(combine)\n",
    "avazu_val_ohe = avazu_val_ohe.map(combine)\n",
    "criteo_train_ohe = criteo_train_ohe.map(combine)\n",
    "criteo_val_ohe = criteo_val_ohe.map(combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define feature mappings\n",
    "kdd12_fixlen_feature_columns = [SparseFeat(feat.lower(), vocabulary_size=kdd12_vocab_lengths[feat], embedding_dim=4) for feat in kdd12_categorical_columns]\\\n",
    "+ [DenseFeat(feat.lower(),1) for feat in kdd12_numerical_columns]\n",
    "avazu_fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=avazu_vocab_lengths[feat], embedding_dim=4) for feat in avazu_categorical_columns]\\\n",
    "+ [DenseFeat(feat,1) for feat in avazu_numerical_columns]\n",
    "criteo_fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=criteo_vocab_lengths[feat], embedding_dim=4) for feat in criteo_categorical_columns]\\\n",
    "+ [DenseFeat(feat,1) for feat in criteo_numerical_columns]\n",
    "\n",
    "## Generate the dnn and linear feature columns\n",
    "kdd12_dnn_feature_columns = kdd12_fixlen_feature_columns\n",
    "kdd12_linear_feature_columns = kdd12_fixlen_feature_columns\n",
    "\n",
    "avazu_dnn_feature_columns = avazu_fixlen_feature_columns\n",
    "avazu_linear_feature_columns = avazu_fixlen_feature_columns\n",
    "\n",
    "criteo_dnn_feature_columns = criteo_fixlen_feature_columns\n",
    "criteo_linear_feature_columns = criteo_fixlen_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Replication and Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model checkpoint directories if they do not exist\n",
    "for model in ['lr','fm','fnn','pnn','wdl','dfm','autoint']:\n",
    "    for dataset in ['kdd12','avazu','criteo']:\n",
    "        if not os.path.exists(f'./models/{model}/{dataset}'):\n",
    "            os.makedirs(f'./models/{model}/{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping callback\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "# Define the precision, recall and auc metrics\n",
    "precision = tf.keras.metrics.Precision(thresholds=0.5,name='precision')\n",
    "recall = tf.keras.metrics.Recall(thresholds=0.5,name='recall')\n",
    "auc = tf.keras.metrics.AUC(name='auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Models\n",
    "\n",
    "In order assess the effectiveness of Deep Learning methods for CTR prediction, it makes sense to attain a model score using more traditional model architectures in order to use these as a comparison. (Zhang et al., 2021) briefly discuss the how CTR applications steadily progressed from Shallow to Deep models. The most basic shallow model for the CTR binary classification task is Logistic Regression (Richardson, Dominowska & Ragno, 2007) which benefits from “high efficiency and ease for fast deployment” (Zhang et al., 2021). However, as it became apparent that identifying key feature interactions is crucial for the CTR prediction task, architectures that explicitly capture these interactions such as the Factorization Machine (Juan et al., 2016; Rendle, 2010) became increasingly popular in the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to attain representitive results from beyond the realm of deep learning, below I evaluate two of the most poplarly used shallow models for CTR prediction:\n",
    "\n",
    "- [Logistic regression](https://www.tensorflow.org/guide/core/logistic_regression_core)\n",
    "- Factorization machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "In logistic regression, we use the logistic (a.k.a. sigmoid) function as our function $f_\\theta$, i.e:\n",
    "$$\n",
    "\\mathcal{P}\\left(\\mathrm{Click}\\right)=\\frac{e^{\\beta_0+\\sum_{i}{\\beta_ix_i}}}{1+e^{\\beta_0+\\sum_{i}{\\beta_ix_i}}}\n",
    "$$\n",
    "(James et al., 2021) show that the equation above can be rearranged as follows:\n",
    "$$\n",
    "\\beta_0+\\sum_{i}{\\beta_ix_i}=log{\\left(\\frac{\\mathcal{P}\\left(\\mathrm{Click}\\right)}{1-\\mathcal{P}\\left(\\mathrm{Click}\\right)}\\right)}\n",
    "$$\n",
    "Logistic regression therefore effectively amounts to fitting a linear regression function to the log odds of a that there is a click for a given instance, parameterized by $\\theta=\\left(\\beta_0,\\beta_1,\\ldots,\\beta_n\\right)$. (Richardson, Dominowska & Ragno, 2007) showed that it is possible to fit a logistic regression model on search advertising data in order to provide accurate click-through rate predictions for each ad, and even demonstrated that the used of this model “improves convergence and performance on the advertising system”. As stated previously, a major benefit of the logistic model over all of the others mentioned here is that since it works on the basis of a simple linear combination of the given features, it is the simplest model to fit and deploy. However, a major shortcoming of this model is the fact that it does not account for feature interactions, and it therefore fails to pick up on discriminating patterns given by the cross-features in the highly sparse categorical ad marketplace data (Zhang et al., 2021).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I replicate the Logistic regression model in Tensorflow by defining a `Sequential` model class with a single `Dense` layer with sigmoid activation. The Dense layer simply implements the linear combination $\\beta_0+\\sum_{i}{\\beta_ix_i}$ from above, before passing the result through the sigmoid function $\\sigma(x) = \\frac{e^x}{1+e^x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_lr_csvLogger = CSVLogger('logs/kdd12_lr.csv')\n",
    "avazu_lr_csvLogger = CSVLogger('logs/avazu_lr.csv')\n",
    "criteo_lr_csvLogger = CSVLogger('logs/criteo_lr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "# Define the model saving checkpoints\n",
    "kdd12_lr_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/lr/kdd12/kdd12_lr.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_lr_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/lr/avazu/avazu_lr.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_dfm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/lr/criteo/criteo_lr.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single dense layer sequential model using sigmoid activation function\n",
    "\n",
    "kdd12_lr_model = Sequential([\n",
    "    Input(shape=(len(kdd12_train_model_input.element_spec[0],))),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])\n",
    "avazu_lr_model = Sequential([\n",
    "    Input(shape=(len(avazu_train_model_input.element_spec[0],))),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])\n",
    "criteo_lr_model = Sequential([\n",
    "    Input(shape=(len(criteo_train_model_input.element_spec[0],))),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "\n",
    "kdd12_lr_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")\n",
    "avazu_lr_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")\n",
    "criteo_lr_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "615/615 [==============================] - 30s 47ms/step - loss: 91551664.0000 - binary_crossentropy: 91551664.0000 - binary_accuracy: 0.9080 - precision: 0.0455 - recall: 0.0523 - auc: 0.5003 - val_loss: 37284160.0000 - val_binary_crossentropy: 37284160.0000 - val_binary_accuracy: 0.9553 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000\n",
      "Epoch 2/15\n",
      "615/615 [==============================] - ETA: 0s - loss: 91879928.0000 - binary_crossentropy: 91879928.0000 - binary_accuracy: 0.9093 - precision: 0.0434 - recall: 0.0484 - auc: 0.4991"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the models to the datasets\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mkdd12_lr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkdd12_train_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkdd12_val_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkdd12_lr_csvLogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkdd12_lr_modelCheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlystopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m avazu_lr_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     11\u001b[0m     avazu_train_concat,\n\u001b[1;32m     12\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mavazu_val_concat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[avazu_lr_csvLogger, avazu_lr_modelCheckpoint, earlystopping]\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m criteo_lr_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     18\u001b[0m     criteo_train_concat,\n\u001b[1;32m     19\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mcriteo_val_concat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[criteo_lr_csvLogger, criteo_lr_modelCheckpoint, earlystopping]\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/engine/training.py:1832\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1818\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1819\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1830\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[1;32m   1831\u001b[0m     )\n\u001b[0;32m-> 1832\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1847\u001b[0m }\n\u001b[1;32m   1848\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/engine/training.py:2272\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2269\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2270\u001b[0m             ):\n\u001b[1;32m   2271\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2272\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2274\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2276\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2277\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2279\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2280\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/engine/training.py:4079\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   4078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[0;32m-> 4079\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   4081\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:876\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 876\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    880\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    881\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the models to the datasets\n",
    "\n",
    "kdd12_lr_model.fit(\n",
    "    kdd12_train_concat,\n",
    "    validation_data=kdd12_val_concat,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[kdd12_lr_csvLogger, kdd12_lr_modelCheckpoint, earlystopping]\n",
    ")\n",
    "avazu_lr_model.fit(\n",
    "    avazu_train_concat,\n",
    "    validation_data=avazu_val_concat,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[avazu_lr_csvLogger, avazu_lr_modelCheckpoint, earlystopping]\n",
    ")\n",
    "criteo_lr_model.fit(\n",
    "    criteo_train_concat,\n",
    "    validation_data=criteo_val_concat,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[criteo_lr_csvLogger, criteo_lr_modelCheckpoint, earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses for each epoch\n",
    "# Read in the history logs\n",
    "kdd12_history = pd.read_csv('logs/kdd12_lr.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_lr.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_lr.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorization Machines were first introduced in \\citep{RefWorks:rendle2010factorization} as\n",
    "a model class that ``combines the advantages of Support Vector Machines (SVM) with factorization models''.\n",
    "The model is able to capture the second order feature interactions in the data, which is a key advantage over\n",
    "Logistic Regression. The model is defined as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y}(\\mathbf{x}) = w_0 + \\sum_{i=1}^{n} w_i x_i + \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j\n",
    "$$\n",
    "\n",
    "where $w_0$ is the bias term, $w_i$ are the weights for the $i$-th feature, $\\mathbf{v}_i$ are the latent vectors for the $i$-th feature.\n",
    "Rendel (2020) shows that the learned biases and weights of the FM model can be\n",
    "computed in linear time, ``and can be learned efficiently by gradient descent methods'', such as Stochastic Gradient Descent (SGD).\n",
    "\n",
    "In the code below, I replicate the FM model by first creating a custom `tf.keras.layers.Layer` class that carries out the\n",
    "forward calculation above. The trainable weights of the layer are then optimized using Stochastic Gradient Descent with \n",
    "`learning_rate=0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom FactorizationMachine layer\n",
    "class FactorizationMachine(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, name=\"FM_layer\"):\n",
    "        super(FactorizationMachine, self).__init__(name=name)\n",
    "        self.k = k\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w0 = self.add_weight(\"bias\", shape=(1,), initializer=tf.keras.initializers.GlorotNormal())\n",
    "        self.W = self.add_weight(\"weights\", shape=(input_shape[-1],), initializer=tf.keras.initializers.GlorotNormal())\n",
    "        self.V = self.add_weight(\"interaction_factors\", shape=(self.k, input_shape[-1]), initializer=tf.keras.initializers.GlorotNormal())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        linear_terms = tf.add(\n",
    "            self.w0,\n",
    "            tf.reduce_sum(\n",
    "                tf.multiply(self.W, inputs),\n",
    "                axis=1,\n",
    "                keepdims=True\n",
    "            )\n",
    "        )\n",
    "        interaction_terms = tf.multiply(\n",
    "            0.5,\n",
    "            tf.reduce_sum(\n",
    "                tf.math.subtract(\n",
    "                    tf.pow(tf.matmul(inputs, tf.transpose(self.V)), 2),\n",
    "                    tf.matmul(tf.pow(inputs, 2), tf.transpose(tf.pow(self.V, 2)))\n",
    "                ),\n",
    "                axis=1, \n",
    "                keepdims=True\n",
    "            )\n",
    "        )\n",
    "        return tf.add(linear_terms,interaction_terms)\n",
    "        \n",
    "    # Have to overwite the get_config method to save the model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'k':self.k})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the input shapes for each of the datasets\n",
    "kdd12_ohe_shape = (len(kdd12_categorical_columns)*50 + len(kdd12_numerical_columns),)\n",
    "avazu_ohe_shape = (len(avazu_categorical_columns)*50 + len(avazu_numerical_columns),)\n",
    "criteo_ohe_shape = (len(criteo_categorical_columns)*50 + len(criteo_numerical_columns),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequential Factorization Machine Models\n",
    "kdd12_fm_model = Sequential([\n",
    "    Input(shape=kdd12_ohe_shape),\n",
    "    FactorizationMachine(5),\n",
    "    Activation('sigmoid')\n",
    "])\n",
    "avazu_fm_model = Sequential([\n",
    "    Input(shape=avazu_ohe_shape),\n",
    "    FactorizationMachine(5),\n",
    "    Activation('sigmoid')\n",
    "])\n",
    "criteo_fm_model = Sequential([\n",
    "    Input(shape=criteo_ohe_shape),\n",
    "    FactorizationMachine(5),\n",
    "    Activation('sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_fm_csvLogger = CSVLogger('logs/kdd12_fm.csv')\n",
    "avazu_fm_csvLogger = CSVLogger('logs/avazu_fm.csv')\n",
    "criteo_fm_csvLogger = CSVLogger('logs/criteo_fm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_fm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fm/kdd12/kdd12_fm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_fm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fm/avazu/avazu_fm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_fm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fm/criteo/criteo_fm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "\n",
    "kdd12_fm_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")\n",
    "avazu_fm_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")\n",
    "criteo_fm_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "615/615 [==============================] - 36s 57ms/step - loss: 0.4417 - binary_crossentropy: 0.4417 - binary_accuracy: 0.8433 - precision: 0.0468 - recall: 0.1283 - auc: 0.5269 - val_loss: 0.2637 - val_binary_crossentropy: 0.2637 - val_binary_accuracy: 0.9553 - val_precision: 1.0000 - val_recall: 5.6850e-04 - val_auc: 0.5747\n",
      "Epoch 2/15\n",
      "615/615 [==============================] - 28s 46ms/step - loss: 0.2350 - binary_crossentropy: 0.2350 - binary_accuracy: 0.9551 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5716 - val_loss: 0.1920 - val_binary_crossentropy: 0.1920 - val_binary_accuracy: 0.9553 - val_precision: 1.0000 - val_recall: 5.6850e-04 - val_auc: 0.5833\n",
      "Epoch 3/15\n",
      "168/615 [=======>......................] - ETA: 16s - loss: 0.2142 - binary_crossentropy: 0.2142 - binary_accuracy: 0.9548 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5792"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mkdd12_fm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkdd12_train_ohe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkdd12_val_ohe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkdd12_fm_csvLogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkdd12_fm_modelCheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlystopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m avazu_fm_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     10\u001b[0m     avazu_train_ohe,\n\u001b[1;32m     11\u001b[0m     validation_data \u001b[38;5;241m=\u001b[39m avazu_val_ohe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[avazu_fm_csvLogger, avazu_fm_modelCheckpoint, earlystopping]\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m criteo_fm_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     17\u001b[0m     criteo_train_ohe,\n\u001b[1;32m     18\u001b[0m     validation_data \u001b[38;5;241m=\u001b[39m criteo_val_ohe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[criteo_fm_csvLogger, criteo_fm_modelCheckpoint, earlystopping]\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the models\n",
    "kdd12_fm_model.fit(\n",
    "    kdd12_train_ohe,\n",
    "    validation_data = kdd12_val_ohe,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[kdd12_fm_csvLogger, kdd12_fm_modelCheckpoint, earlystopping]\n",
    ")\n",
    "avazu_fm_model.fit(\n",
    "    avazu_train_ohe,\n",
    "    validation_data = avazu_val_ohe,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[avazu_fm_csvLogger, avazu_fm_modelCheckpoint, earlystopping]\n",
    ")\n",
    "criteo_fm_model.fit(\n",
    "    criteo_train_ohe,\n",
    "    validation_data = criteo_val_ohe,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[criteo_fm_csvLogger, criteo_fm_modelCheckpoint, earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses for each epoch\n",
    "# Read in the history logs\n",
    "kdd12_history = pd.read_csv('logs/kdd12_fm.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_fm.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_fm.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, I will explore a number of deep learning models. I selected five popular models on the basis of the following criteria\n",
    "\n",
    "- Competitive predition accuracy in the KDD12, Criteo and Avazu datasets as published on [PapersWithCode](https://paperswithcode.com/)\n",
    "- Ideally, I was looking for a representitive set of models for each model type as discussed in (Zhang et. al. 2021). Therefore I was looking for models that employed Product Interaction Opetators, Attention Operators and Factorization Machines as a basis.\n",
    "- The code for the model has to be accessible and intuitive to use.\n",
    "\n",
    "On the basis of the above critea, I have chosen the following models to explore:\n",
    "\n",
    "- Factorization Supported Neural Networks\n",
    "- Product Based Neural Networks\n",
    "- Wide and Deep\n",
    "- DeepFM\n",
    "- Automatic Feature Interaction (AutoInt)\n",
    "\n",
    "In the section below, I briefly introduce each of the models, and evaluate against the benchmark datasets loaded and preprocessed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization-Machine Supported Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Deep Learning model that we will consider is the Factorization Supported\n",
    "Neural Network (FNN) model proposed by Zhang et. al. (2016). The model works by first training a Factorization Machine\n",
    "model on the sparse-encoded categorical input features. It then uses the latent vectors learned by the FM model (see $\\mathbf{v}_i$ in the equation above)\n",
    "as inputs to a Neural Network, as shown in the figure below. In doing so, the FNN model is effectively using the FM latent factors to initialize the embedding layer of the Neural Network.\n",
    "The DNN is then able to learn the higher order feature interactions in the data, which the FM model is unable to capture.\n",
    "\n",
    "![FNN](figures/fnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "kdd12_fnn_model = FNN(\n",
    "    kdd12_linear_feature_columns, \n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005\n",
    "    )\n",
    "kdd12_fnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "avazu_fnn_model = FNN(\n",
    "    avazu_linear_feature_columns, \n",
    "    avazu_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005\n",
    "    )\n",
    "avazu_fnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "criteo_fnn_model = FNN(\n",
    "    criteo_linear_feature_columns, \n",
    "    criteo_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005\n",
    "    )\n",
    "criteo_fnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_fnn_csvLogger = CSVLogger('logs/kdd12_fnn.csv')\n",
    "avazu_fnn_csvLogger = CSVLogger('logs/avazu_fnn.csv')\n",
    "criteo_fnn_csvLogger = CSVLogger('logs/criteo_fnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_fnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fnn/kdd12/kdd12_fnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_fnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fnn/avazu/avazu_fnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_fnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fnn/criteo/criteo_fnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "     62/Unknown - 53s 795ms/step - loss: 1.4021 - binary_crossentropy: 0.3387 - binary_accuracy: 0.9015 - precision: 0.0428 - recall: 0.0536 - auc: 0.5319"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m kdd12_fnn_history \u001b[38;5;241m=\u001b[39m \u001b[43mkdd12_fnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkdd12_train_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkdd12_val_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkdd12_fnn_csvLogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkdd12_fnn_modelCheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearlystopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m avazu_fnn_history \u001b[38;5;241m=\u001b[39m avazu_fnn_model\u001b[38;5;241m.\u001b[39mfit(avazu_train_model_input, validation_data\u001b[38;5;241m=\u001b[39mavazu_val_model_input, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[avazu_fnn_csvLogger,avazu_fnn_modelCheckpoint,earlystopping])\n\u001b[1;32m      4\u001b[0m criteo_fnn_history \u001b[38;5;241m=\u001b[39m criteo_fnn_model\u001b[38;5;241m.\u001b[39mfit(criteo_train_model_input, validation_data\u001b[38;5;241m=\u001b[39mcriteo_val_model_input, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[criteo_fnn_csvLogger,criteo_fnn_modelCheckpoint,earlystopping])\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the models\n",
    "kdd12_fnn_history = kdd12_fnn_model.fit(kdd12_train_model_input, validation_data=kdd12_val_model_input, batch_size=256, epochs=15, callbacks=[kdd12_fnn_csvLogger,kdd12_fnn_modelCheckpoint,earlystopping])\n",
    "avazu_fnn_history = avazu_fnn_model.fit(avazu_train_model_input, validation_data=avazu_val_model_input, batch_size=256, epochs=15, callbacks=[avazu_fnn_csvLogger,avazu_fnn_modelCheckpoint,earlystopping])\n",
    "criteo_fnn_history = criteo_fnn_model.fit(criteo_train_model_input, validation_data=criteo_val_model_input, batch_size=256, epochs=15, callbacks=[criteo_fnn_csvLogger,criteo_fnn_modelCheckpoint,earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_fnn_history = pd.read_csv('logs/kdd12_fnn.csv')\n",
    "avazu_fnn_history = pd.read_csv('logs/avazu_fnn.csv')\n",
    "criteo_fnn_history = pd.read_csv('logs/criteo_fnn.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_fnn_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_fnn_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_fnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_fnn_history['binary_crossentropy'])\n",
    "plt.plot(avazu_fnn_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_fnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_fnn_history['binary_crossentropy'])\n",
    "plt.plot(criteo_fnn_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_fnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the models\n",
    "kdd12_fnn_score = kdd12_fnn_model.evaluate(kdd12_val_model_input, batch_size=256)\n",
    "avazu_fnn_score = avazu_fnn_model.evaluate(avazu_val_model_input, batch_size=256)\n",
    "criteo_fnn_score = criteo_fnn_model.evaluate(criteo_val_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Based Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Product Based Neural Network (PNN) model\n",
    "proposed by Qu et. al. (2016) is another Deep Learning\n",
    "model that was developed around the same time as the FNN model. The key \n",
    "innovation of the PNN moel is the use of a pair-wisely connected Product Layer\n",
    "after a field-wise connected embetting layer for the categorical features, as shown\n",
    "in the figure below. The Product Layer is able to directly model inter-field feature\n",
    "interaction by means of either an inner product or outer production operation, and then further\n",
    "distill higher feature inturactions by passing the output of the Product Layer through fully\n",
    "connected MLP layers.\n",
    "\n",
    "![PNN](figures/pnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_pnn_csvLogger = CSVLogger('logs/kdd12_pnn.csv')\n",
    "avazu_pnn_csvLogger = CSVLogger('logs/avazu_pnn.csv')\n",
    "criteo_pnn_csvLogger = CSVLogger('logs/criteo_pnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_pnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/pnn/kdd12/kdd12_pnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_pnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/pnn/avazu/avazu_pnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_pnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/pnn/criteo/criteo_pnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the PNN models\n",
    "kdd12_pnn_model = PNN(\n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "kdd12_pnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "avazu_pnn_model = PNN(\n",
    "    avazu_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "avazu_pnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "criteo_pnn_model = PNN(\n",
    "    criteo_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "criteo_pnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "kdd12_pnn_history = kdd12_pnn_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data=kdd12_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=40, \n",
    "    callbacks=[kdd12_pnn_csvLogger,kdd12_pnn_modelCheckpoint,earlystopping]\n",
    ")\n",
    "avazu_pnn_history = avazu_pnn_model.fit(\n",
    "    avazu_train_model_input,\n",
    "    validation_data=avazu_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=40,\n",
    "    callbacks=[avazu_pnn_csvLogger,avazu_pnn_modelCheckpoint,earlystopping]\n",
    ")\n",
    "criteo_pnn_history = criteo_pnn_model.fit(\n",
    "    criteo_train_model_input,\n",
    "    validation_data=criteo_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=40,\n",
    "    callbacks=[criteo_pnn_csvLogger,criteo_pnn_modelCheckpoint,earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_history = pd.read_csv('logs/kdd12_pnn.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_pnn.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_pnn.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_pnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_pnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_pnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide and Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wide \\& Deep Learning (WDL) model proposed by Cheng et. al. (2016) introduces the concept\n",
    "of dual-tower model architecture (Zhang et. al. 2021). While both the FNN and the PNN models\n",
    "generally tend to be constructed as a single fully connected DNN model, the Wide \\& Deep model\n",
    "consists of a wide component, consisting of a three layer Deep Neural Network that takes the concatinated\n",
    "embedding vectors of the categorical features as input, and a deep component, consisting of a cross product\n",
    "transformation of selected sparse categorical features. The logits from the wide and deep components are added\n",
    "together to produce the final prediction. The architecture of the WDL model is shown in the figure below.\n",
    "\n",
    "![WDL](figures/wdl.png)\n",
    "\n",
    "The purpose behind the Dual-Tower architecture is to counteract the tendancy of the fully connected\n",
    "single tower DNN models to lose the ability to capture low-order feature interactions (Zhang et. al. 2021).\n",
    "The Wide component is able to capture the low-order feature interactions, while the Deep component is able to capture\n",
    "the higher order feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the WDL models\n",
    "kdd12_wdl_model = WDL(\n",
    "    kdd12_linear_feature_columns, \n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "kdd12_wdl_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "avazu_wdl_model = WDL(\n",
    "    avazu_linear_feature_columns, \n",
    "    avazu_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "avazu_wdl_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "criteo_wdl_model = WDL(\n",
    "    criteo_linear_feature_columns, \n",
    "    criteo_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "criteo_wdl_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_wdl_csvLogger = CSVLogger('logs/kdd12_wdl.csv')\n",
    "avazu_wdl_csvLogger = CSVLogger('logs/avazu_wdl.csv')\n",
    "criteo_wdl_csvLogger = CSVLogger('logs/criteo_wdl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_wdl_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/wdl/kdd12/kdd12_wdl.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_wdl_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/wdl/avazu/avazu_wdl.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_dfm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/wdl/criteo/criteo_wdl.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Wide and Deep models\n",
    "kdd12_wdl_history = kdd12_wdl_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data=kdd12_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[kdd12_wdl_csvLogger,kdd12_wdl_modelCheckpoint,earlystopping]\n",
    ")\n",
    "avazu_wdl_history = avazu_wdl_model.fit(\n",
    "    avazu_train_model_input, \n",
    "    validation_data=avazu_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[avazu_wdl_csvLogger,avazu_wdl_modelCheckpoint,earlystopping]\n",
    ")\n",
    "criteo_wdl_history = criteo_wdl_model.fit(\n",
    "    criteo_train_model_input, \n",
    "    validation_data=criteo_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[criteo_wdl_csvLogger, criteo_wdl_modelCheckpoint,earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_history = pd.read_csv('logs/kdd12_wdl.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_wdl.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_wdl.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_wdl_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_wdl_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_wdl_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepFM model proposed by Guo et. al. 2017 can be thought of as an\n",
    "imporvement of the aforementioned FNN (Zhang et. al., 2016) and WDL (Cheng et. al., 2016) models.\n",
    "Like the FNN model, the DeepFM model usilises the Factorization Machine model (Rendel, 2010)\n",
    "to learn lower-order feature interactions. However, it also employs a dual-tower architecture\n",
    "like the WDL model, with the Wide component being the FM model and the Deep component being a fully connected\n",
    "DNN model. The DeepFM model is therefore able to avoid the limitations on capturing low-order\n",
    "interactions that are inherent in the FNN model. In addition, due the the application of the FM to all\n",
    "feature embeddings, the DeepFM model eliminates the need to choose which features \n",
    "to feed through the wide component, as is the case in the WDL model. The architecture of the DeepFM model is shown \n",
    "in the figure below.\n",
    "\n",
    "![DFM](figures/dfm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the DeepFM models\n",
    "kdd12_dfm_model = DeepFM(\n",
    "    kdd12_linear_feature_columns, \n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    ")\n",
    "kdd12_dfm_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "avazu_dfm_model = DeepFM(\n",
    "    avazu_linear_feature_columns, \n",
    "    avazu_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    ")\n",
    "avazu_dfm_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "criteo_dfm_model = DeepFM(\n",
    "    criteo_linear_feature_columns, \n",
    "    criteo_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    ")\n",
    "criteo_dfm_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_dfm_csvLogger = CSVLogger('logs/kdd12_dfm.csv')\n",
    "avazu_dfm_csvLogger = CSVLogger('logs/avazu_dfm.csv')\n",
    "criteo_dfm_csvLogger = CSVLogger('logs/criteo_dfm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_dfm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/dfm/kdd12/kdd12_dfm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_dfm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/dfm/avazu/avazu_dfm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_dfm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/dfm/criteo/criteo_dfm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the DeepFM models\n",
    "'''\n",
    "kdd12_dfm_history = kdd12_dfm_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data = kdd12_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[kdd12_dfm_csvLogger,kdd12_dfm_modelCheckpoint,earlystopping]\n",
    ")\n",
    "'''\n",
    "avazu_dfm_history = avazu_dfm_model.fit(\n",
    "    avazu_train_model_input, \n",
    "    validation_data= avazu_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[avazu_dfm_csvLogger,avazu_dfm_modelCheckpoint,earlystopping]\n",
    ")\n",
    "\n",
    "criteo_dfm_history = criteo_dfm_model.fit(\n",
    "    criteo_train_model_input, \n",
    "    validation_data=criteo_val_model_input, \n",
    "    batch_size=256, epochs=15,\n",
    "    callbacks=[criteo_dfm_csvLogger,criteo_dfm_modelCheckpoint,earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_dfm_history = pd.read_csv('logs/kdd12_dfm.csv')\n",
    "avazu_dfm_history = pd.read_csv('logs/avazu_dfm.csv')\n",
    "criteo_dfm_history = pd.read_csv('logs/criteo_dfm.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_dfm_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_dfm_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_dfm_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_dfm_history['binary_crossentropy'])\n",
    "plt.plot(avazu_dfm_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_dfm_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_dfm_history['binary_crossentropy'])\n",
    "plt.plot(criteo_dfm_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_dfm_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Feature Interaction (AutoInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Autotomatic Feature Interaction Learning (AutoInt) model proposed by\n",
    "Song et. al. (2019) makes use of a multi-head self attention\n",
    "network to model the important feature interactions in the data. The initial \n",
    "paper separates the model into three parts: an embedding layer, an interaction layer \n",
    "and an output layer. The embedding layer aims to project each sparse multi-value\n",
    "categorical a and dense numerical feature into a lower dimensional space, as per the below:\n",
    "\n",
    "$$\n",
    "\\mathbf{e_i} = \\frac{1}{q} \\mathbf{V_i x_i}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{V_i}$ is the embedding matrix for the $i$-th field, $x_i$ is a multi-hot vector, and $q$ \n",
    "is the number of non-zero values in $x_i$. The interaction layer employs the multi-head\n",
    "mechanism to determine which higher order feature interaction are meaningful in the data. This not only\n",
    "improves the efficiency of model traning, but it also improves the model's explainability. Lastly,\n",
    "the output layer is a fully connected layer that takes in the concatinated output \n",
    "of the interaction layer, and applies the sigmoid activation function to produce the final prediction.\n",
    "The architecture of the AutoInt model is shown in Figure below.\n",
    "\n",
    "![AUTOINT](figures/autoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the AutoInt Models\n",
    "kdd12_autoint_model = AutoInt(\n",
    "    kdd12_linear_feature_columns, \n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    "    )\n",
    "kdd12_autoint_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "avazu_autoint_model = AutoInt(\n",
    "    avazu_linear_feature_columns, \n",
    "    avazu_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    "    )\n",
    "avazu_autoint_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "criteo_autoint_model = AutoInt(\n",
    "    criteo_linear_feature_columns, \n",
    "    criteo_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    "    )\n",
    "criteo_autoint_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AutoInt CSVLogger callbacks\n",
    "kdd12_autoint_csvLogger = CSVLogger('logs/kdd12_autoint.csv')\n",
    "avazu_autoint_csvLogger = CSVLogger('logs/avazu_autoint.csv')\n",
    "criteo_autoint_csvLogger = CSVLogger('logs/criteo_autoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_autoint_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/autoint/kdd12/kdd12_autoint.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_autoint_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/autoint/avazu/avazu_autoint.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_autoint_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/autoint/criteo/criteo_autoint.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the AutoInt models\n",
    "kdd12_autoint_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data= kdd12_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15, \n",
    "    callbacks=[kdd12_autoint_csvLogger, kdd12_autoint_modelCheckpoint,earlystopping]\n",
    ")\n",
    "avazu_autoint_model.fit(\n",
    "    avazu_train_model_input,\n",
    "    validation_data=avazu_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=15, \n",
    "    callbacks=[avazu_autoint_csvLogger, avazu_autoint_modelCheckpoint,earlystopping]\n",
    ")\n",
    "criteo_autoint_model.fit(\n",
    "    criteo_train_model_input, \n",
    "    validation_data=criteo_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[criteo_autoint_csvLogger,criteo_autoint_modelCheckpoint,earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_autoint_history = pd.read_csv('logs/kdd12_autoint.csv')\n",
    "avazu_autoint_history = pd.read_csv('logs/avazu_autoint.csv')\n",
    "criteo_autoint_history = pd.read_csv('logs/criteo_autoint.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_autoint_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_autoint_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_autoint_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_autoint_history['binary_crossentropy'])\n",
    "plt.plot(avazu_autoint_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_autoint_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_autoint_history['binary_crossentropy'])\n",
    "plt.plot(criteo_autoint_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_autoint_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results and Discussion\n",
    "\n",
    "Below I construct and display a pandas dataframe of the best results in each of the logs in terms of validation loss. The accompanying validation metrics are also shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pandas dataframe of the model scores for the kdd12 dataset\n",
    "models = ['lr','fm','fnn','pnn','wdl','dfm','autoint']\n",
    "datasets = ['kdd12','avazu','criteo']\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.DataFrame()\n",
    "    for model in models:\n",
    "        log_file = f'logs/{dataset}_{model}.csv'\n",
    "        history = pd.read_csv(log_file)\n",
    "        min_loss = history[history['val_loss']==history['val_loss'].min()]\n",
    "        min_loss_val = min_loss.loc[:,['epoch','val_binary_crossentropy','val_binary_accuracy','val_precision','val_recall']]\n",
    "        min_loss_val['model'] = model\n",
    "        df = pd.concat([df,min_loss_val],axis=0)\n",
    "    df = df.set_index('model')\n",
    "    df.to_csv(f'logs/{dataset}_model_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the the model scores by data set\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'logs/{dataset}_model_scores.csv')\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(f'{dataset.capitalize()} Model Scores')\n",
    "    \n",
    "    # Plot a scatter plot of binary crossentropy loss vs binary accuracy\n",
    "    sns.scatterplot(ax=axs[0], data=df, x='val_binary_crossentropy', y='val_binary_accuracy', hue='model', palette='tab10')\n",
    "    axs[0].set_title('Binary Crossentropy vs Binary Accuracy')\n",
    "    axs[0].set_xlabel('Binary Crossentropy')\n",
    "    axs[0].set_ylabel('Binary Accuracy')\n",
    "\n",
    "    # Plot a scatter plot of Precision vs Recall\n",
    "    sns.scatterplot(ax=axs[1], data=df, x='val_precision', y='val_recall', hue='model', palette='tab10')\n",
    "    axs[1].set_title('Precision vs Recall')\n",
    "    axs[1].set_xlabel('Precision')\n",
    "    axs[1].set_ylabel('Recall')\n",
    "\n",
    "    # Plot a bar chart of epoch number by model\n",
    "    sns.barplot(ax=axs[2], data=df, x='model', y='epoch', palette='tab10')\n",
    "    axs[2].set_title('Epochs to Convergence')\n",
    "    axs[2].set_xlabel('Model')\n",
    "    axs[2].set_ylabel('Epochs')\n",
    "\n",
    "    # Arrange the subplots so that the axis labels do not overlap\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    # Save the figure in the figures folder and show the plot\n",
    "    plt.savefig(f'figures/{dataset}_model_scores.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same except without the LR model\n",
    "\n",
    "# Plot the the model scores by data set\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'logs/{dataset}_model_scores.csv')\n",
    "    df = df[df['model']!='lr']\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(f'{dataset.capitalize()} Model Scores')\n",
    "    \n",
    "    # Plot a scatter plot of binary crossentropy loss vs binary accuracy\n",
    "    sns.scatterplot(ax=axs[0], data=df, x='val_binary_crossentropy', y='val_binary_accuracy', hue='model', palette='tab10')\n",
    "    axs[0].set_title('Binary Crossentropy vs Binary Accuracy')\n",
    "    axs[0].set_xlabel('Binary Crossentropy')\n",
    "    axs[0].set_ylabel('Binary Accuracy')\n",
    "\n",
    "    # Plot a scatter plot of Precision vs Recall\n",
    "    sns.scatterplot(ax=axs[1], data=df, x='val_precision', y='val_recall', hue='model', palette='tab10')\n",
    "    axs[1].set_title('Precision vs Recall')\n",
    "    axs[1].set_xlabel('Precision')\n",
    "    axs[1].set_ylabel('Recall')\n",
    "\n",
    "    # Plot a bar chart of epoch number by model\n",
    "    sns.barplot(ax=axs[2], data=df, x='model', y='epoch', palette='tab10')\n",
    "    axs[2].set_title('Epochs to Convergence')\n",
    "    axs[2].set_xlabel('Model')\n",
    "    axs[2].set_ylabel('Epochs')\n",
    "\n",
    "    # Arrange the subplots so that the axis labels do not overlap\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    # Show the plot\n",
    "    #plt.savefig(f'figures/{dataset}_model_scores.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Ad Personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the kdd12 dfm model from the saved weights\n",
    "## Create lists of categorical colums for each dataset\n",
    "kdd12_categorical_columns = [\n",
    "    'DisplayURL',\n",
    "    'AdID',\n",
    "    'AdvertiserID',\n",
    "    'QueryID',\n",
    "    'KeywordID',\n",
    "    'TitleID',\n",
    "    'DescriptionID',\n",
    "    'UserID'\n",
    "]\n",
    "\n",
    "# Import categorical feature mappings and define stringloohup objects for each dataset\n",
    "kdd12_vocab_sizes = {}\n",
    "for field in kdd12_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/kdd12/categorical_value_counts/{field}.csv')\n",
    "    vocab_size = len(df['field'])+1\n",
    "    kdd12_vocab_sizes.update({field:vocab_size})\n",
    "\n",
    "# Define numerical feature columns\n",
    "kdd12_numerical_columns = [\n",
    "    'Depth',\n",
    "    'Position',\n",
    "    'Impression'\n",
    "]\n",
    "\n",
    "## Define feature mappings\n",
    "kdd12_fixlen_feature_columns = [SparseFeat(feat.lower(), vocabulary_size=kdd12_vocab_sizes[feat], embedding_dim=4) for feat in kdd12_categorical_columns]\\\n",
    "+ [DenseFeat(feat.lower(),1) for feat in kdd12_numerical_columns]\n",
    "\n",
    "## Generate the dnn and linear feature columns\n",
    "kdd12_dnn_feature_columns = kdd12_fixlen_feature_columns\n",
    "kdd12_linear_feature_columns = kdd12_fixlen_feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 18:27:52.127379: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-08-05 18:27:52.165997: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-05 18:27:52.332718: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7fd20c42cd90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7fd20c42cd90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "precision = Precision(thresholds=0.5,name='precision')\n",
    "recall = Recall(thresholds=0.5,name='recall')\n",
    "\n",
    "# Compile the DeepFM models\n",
    "kdd12_dfm_model = DeepFM(\n",
    "    kdd12_linear_feature_columns, \n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    dnn_hidden_units=[200,200,200],\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    ")\n",
    "kdd12_dfm_model.compile(adam_optimizer, \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd20c32c610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd12_dfm_model.load_weights('models/dfm/kdd12/kdd12_dfm.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = kdd12_dfm_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fd219e67f10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd12_dfm_model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlds_gpu)",
   "language": "python",
   "name": "mlds_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
