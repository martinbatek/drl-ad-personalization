{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 12:31:53.611508: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-06 12:31:53.611556: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-06 12:31:53.611568: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-06 12:31:53.617986: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "## General\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "## Data manipulation and preprocessing\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer,LabelEncoder, MinMaxScaler,OrdinalEncoder\n",
    "#import missingno as msno\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, MissingIndicator\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from scipy import sparse\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup, Normalization\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "\n",
    "## Modelling\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input, Concatenate, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from deepctr.models.fnn import FNN\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr.models.pnn import PNN\n",
    "from deepctr.models.wdl import WDL\n",
    "from deepctr.models.deepfm import DeepFM\n",
    "from deepctr.models.autoint import AutoInt\n",
    "from deepctr.models.fgcnn import FGCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 12:31:57.869556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-06 12:31:57.899019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-06 12:31:57.900862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "gpu_devices=tf.config.list_physical_devices('GPU')\n",
    "print(gpu_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The global digital advertising market is worth approximately \\\\$602 billion today. Due to the increasing rate of of online participation since the \n",
    "COVID-19 pandemic, this number has been rapidly increasing and is expected to reach \\\\$871 billion by the end of 2027 \\citep{RefWorks:emarketer2023digital}.\n",
    "Many of the of the major Ad platforms such as Google, Facebook and Amazon operate on a cost-per-user-engagement pricing model, which usually means that \n",
    "advertisers get charged for every time a user clicks on an advertisment. This means that these platforms are incentivized to make sure that the content \n",
    "shown to each user is as relevent as possible in order to maximize the number of clicks in the long term. Attaining accurate Click-Through Rate (CTR) \n",
    "prediction is a necessary first step for Ad persionalization, which is why study of CTR prediction methods have been an extremely active part of \n",
    "Machine Learning research over the past through years.\n",
    "\n",
    "Initially, shallow prediction methods such as Logistic Regression, Factorization Machines (Rendel, 2020) and Field-Aware Factorization \n",
    "Machines \\citep{RefWorks:juan2016field-aware} have been used for CTR prediction. However, these methods have often been shown to be unable to capture the \n",
    "higher order feature interactions in the sparse multi-value categorical Ad Marketplace datasets \\citep{RefWorks:zhang2021deep}. Since then, Deep Learning methods have been \n",
    "shown to show superior predictive ability on these datasets. The focus of my reasearch project is therefore to explore the merits of different Deep \n",
    "Learning architechtures for click-through rate prediction.\n",
    "\n",
    "A number of Deep Learning models have been proposed for CTR prediction, some of which will be explored in this report. Each of these models\n",
    "outperform their shallow counterparts in terms of predictive ability. In a static environment, these models are able to serve the CTR prediction\n",
    "function of Ad personalization, but in a dynamic environment, the model must be able to adapt to the changing user preferences. This is where\n",
    "Reinforcement Learning comes in. Reinforcement Learning is a type of Machine Learning that is used to make a sequence of decisions in an environment\n",
    "in order to maximize some notion of cumulative reward. In the context of Ad personalization, the environment is composed of the user, the Ad platform\n",
    "and the advertisments, whereas the reward is the users' engagement with the advertisments and with the Ad platform.\n",
    "\n",
    "\n",
    "As outlined in the Feasibility secition of my Milestone 3 submission, I have segmented the work over the next few weeks according to the following timeline\n",
    "\n",
    "![](figures/timeline.png)\n",
    "\n",
    "Below I proceed by structuring my response according to the following sections\n",
    "- General Background to Deep CTR and Deep Renforcement Learning\n",
    "- Data exploration, preprocessing and environment setup\n",
    "- Model Introduction, Replication and Experimentation\n",
    "\n",
    "Please note that this notebook illustrates my current work in progress, and certain sections are far from finished. I have also concurrently begun sections of my final write up using the LaTeX template provided. This is available along with the rest of my code in the following Github repository: https://github.com/martinbatek/drl-ad-personalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CTR Prediction\n",
    "\n",
    "In their respective surveys on the use of Deep Learning methods for CTR prediction, (Gu, 2021) and (Zhang et al., 2021) outline the \n",
    "problem of CTR prediction as one that essentially boils down to a binary (click/no-click) classification problem utilizing user/ad-view event level \n",
    "online session records. The goal of CTR prediction is to predict the probability of a user clicking on an advertisment given the information available\n",
    "about the user, advertisment and the context in which the advertisement is shown. Suppose that $\\mathbf{x} \\in \\mathbb{R}^n$\n",
    "is a vector of features that describes the user, ad and platform for a given instance, and\n",
    "$y \\in \\{0, 1\\}$ is the binary label indicating whether the user clicked on the ad or not. The goal of CTR prediction is to learn a function\n",
    "$f: \\mathbb{R}^n \\rightarrow (0,1)$ such that:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbb{P}(y = 1 | \\mathbf{x}) = \\mathbb{P}(\\text{click}| \\mathbf{x})\n",
    "$$\n",
    "\n",
    "In other words, for a given set of features $\\mathbf{x}$, the model should output the probability that the user will click on the ad.\n",
    "A defining characteristic for this type of data is that many of the features are multi-value categories with a high degree of of cardinality \\citep{RefWorks:he2017neural}. \n",
    "This in turn means that the ad marketplace datasets used for CTR predictions can be extremely sparse, which increases the difficulty of the classification problem at \n",
    "hand \\citep{RefWorks:gu2021ad}.\n",
    "\n",
    "A key requirement for CTR modelling is therefore working out which of the many sparse features and feature \n",
    "interactions (combinations of two or more features) are significant for determining the correct prediction \n",
    "(Gu, 2021). Factorization Machines (Rendle, 2010) and Field-aware Factorization Machines (Juan et al., 2016) \n",
    "were popularized shallow modelling methods that explicitly account for first order interactions between features. \n",
    "However, these techniques do not capture higher order interactions (combinations of three or more features) and \n",
    "have thus been known to perform poorly in scenarios with highly sparse data (Zhang et al., 2021). Since 2015 the \n",
    "research cummunity has been increasingly turning to Deep Learning techniques to enhance prior CTR prediction \n",
    "techniques (such as in the case of DeepFM (Guo et al., 2017)), as well as to develop novel approaches. Neural based \n",
    "network models excel at simulataneoulsy extracting high-order and low-order feature interations virtue to the use \n",
    "of pooling layers, multiple hidden layers and activation units (Gu, 2021). Due the aforementioned importance of \n",
    "feature interation modelling, a number of Feature Interaction Operator layers have been developed to explicitly \n",
    "capture the key combinations of features. These layers are then typically incorporated with a supplimentary Deep \n",
    "Neural Network in a single or dual tower architecture, as shown in the Figure below:\n",
    "\n",
    "![DNN_Architecture](figures/single_dual_dnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Interaction Operators can be categorized as either Product Operators, Convolutional Operators or Attention Operators. Product Operators such as the Product-based Neural Network (PNN) (Qu et al., 2016), Neural Factorization Machines (NFM) (He & Chua, 2017), Deep and Cross Network (Wang et al., 2017) and Gated Deep Cross Network (GDCN) (Wang, F. et al., 2023) introduce a product layer between the categorical feature embedding layer and the rest of the neural network in order to explicitly model the important feature interactions. Convolutional Operators such as the Convolutional Click Prediction Model (CCPM) (Liu et al., 2015) utilized convolution, pooling and non-linear activation in order to calculate arbitrary-order interactions. Finally, Attention Operators such as Attentional Factorization Machines (AFM) (Xiao et al., 2017), AutoInt (Song et al., 2019) and Interpretable CTR prediction model with Hierarchical Attention (InterHAt) (Li et al., 2020) utilize the attention mechanism to enable different feature interactions to contribute differently to the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their survey, (Wang, X. et al., 2024) describe how deep reinforcement learning combines the aforementioned feature extraction capabilities of DNN’s with the decision-making capability of reinforcement learning, which aims to learn an optimal state-action policy which maximizes the expected reward gained in a given environment. In the context of recommendation systems, a significant amount of research has been dedicated to formulating the recommendation problem as a Contextual Multi-Armed Bandit (MAB) problem setting, where the context consists of user, site and item features (Bouneffouf, Bouzeghoub & Gancarski, 2012; Li, L. et al., 2010; Zeng et al., 2016). However, a shortcoming for the MAB approach is that it does not explicitly model the future expected reward for the policy, which may be detrimental in the longer term (Zheng et al., 2018). Markov Decision Process (MDP) models solve for this issue by modelling the state-action progression as a Markov Process, allowing for the stochastic valuation of the future potential rewards for a given recommendation policy (Lu & Yang, 2016; Mahmood & Ricci, 2007). DRN (Zheng et al., 2018) is a MDP framework that leverages a Deep Neural Network to approximate the expected total user response for each recommendation at each state. The two major advantages of DRN are firstly that it is composed on the basis of a continuous state and action representation, meaning that it can be scaled to large and sparse datasets, and secondly that the proposed reward function consists of both the immediate reward (user click) as well as the future expected reward (long term user engagement), thereby allowing for better recommendations over a user’s lifetime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration, Caching, Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD - Include data Exploration sections from Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load benchmark datasets as tf.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 12:32:26.297573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-06 12:32:26.301252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-06 12:32:26.304384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-06 12:32:26.468575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-06 12:32:26.470612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-06 12:32:26.472332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-06 12:32:26.474043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20723 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Load the kdd12_train dataset\n",
    "kdd12_train = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/kdd12/train_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=['int32','int32','string','string','string','int32','int32','string','string','string','string','string'],\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the kdd12_train dataset\n",
    "kdd12_val = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/kdd12/test_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=['int32','int32','string','string','string','int32','int32','string','string','string','string','string'],\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_defaults=[\n",
    "    'string',\n",
    "    'int32',\n",
    "    'int32',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string'\n",
    "]\n",
    "avazu_train = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/avazu/train_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=column_defaults,\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_defaults=[\n",
    "    'string',\n",
    "    'int32',\n",
    "    'int32',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string',\n",
    "    'string'\n",
    "]\n",
    "\n",
    "# Load the Avazu train dataset\n",
    "avazu_val = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/avazu/test_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    column_defaults=column_defaults,\n",
    "    label_name='click',\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    #shuffle_buffer_size=100,\n",
    "    #shuffle_seed=42,\n",
    "    #ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID variable\n",
    "def drop_id(features, label):\n",
    "    out_features = features.copy()\n",
    "    del out_features['id']\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "avazu_train = avazu_train.map(drop_id)\n",
    "avazu_val = avazu_val.map(drop_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sagemaker-user/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/readers.py:573: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.ignore_errors` instead.\n"
     ]
    }
   ],
   "source": [
    "# Load the Criteo train dataset\n",
    "\n",
    "column_defaults = ['int32'] + 13*['int32'] + 26*['string']\n",
    "\n",
    "criteo_train = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/criteo/train_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    label_name='click',\n",
    "    column_defaults=column_defaults,\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Criteo train dataset\n",
    "\n",
    "column_defaults = ['int32'] + 13*['int32'] + 26*['string']\n",
    "\n",
    "criteo_val = tf.data.experimental.make_csv_dataset(\n",
    "    \"data/criteo/test_split/*\",\n",
    "    batch_size=1,\n",
    "    field_delim=',',\n",
    "    header=True,\n",
    "    label_name='click',\n",
    "    column_defaults=column_defaults,\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    "    ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I construct the preprocessing pipeline for the tensorflow datasets on the basis of the exploration functions above. I proceed by:\n",
    "\n",
    "- Enconding the sparse catecorically features using the Ordinal Encoders specified above.\n",
    "- Standardizing the numerical features unsing the Scalers and Transformer specified above.\n",
    "- Generating the feature columns, as specified in the [DeepCTR Quickstart Guide](https://deepctr-doc.readthedocs.io/en/latest/Quick-Start.html#step-3-generate-feature-columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create lists of categorical colums for each dataset\n",
    "kdd12_categorical_columns = [\n",
    "    'DisplayURL',\n",
    "    'AdID',\n",
    "    'AdvertiserID',\n",
    "    'QueryID',\n",
    "    'KeywordID',\n",
    "    'TitleID',\n",
    "    'DescriptionID',\n",
    "    'UserID'\n",
    "]\n",
    "\n",
    "avazu_categorical_columns = [\n",
    "    'C1',\n",
    "    'banner_pos',\n",
    "    'site_id',\n",
    "    'site_domain',\n",
    "    'site_category',\n",
    "    'app_id',\n",
    "    'app_domain',\n",
    "    'app_category',\n",
    "    'device_id',\n",
    "    'device_ip',\n",
    "    'device_model',\n",
    "    'device_type',\n",
    "    'device_conn_type',\n",
    "    'C14',\n",
    "    'C15',\n",
    "    'C16',\n",
    "    'C17',\n",
    "    'C18',\n",
    "    'C19',\n",
    "    'C20',\n",
    "    'C21'\n",
    "]\n",
    "\n",
    "criteo_numerical_columns = [f'int_{i}' for i in np.arange(1,14)]\n",
    "criteo_categorical_columns = [f'cat_{i}' for i in np.arange(1,27)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import categorical feature mappings and define stringloohup objects for each dataset\n",
    "kdd12_stringlookups = {}\n",
    "for field in kdd12_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/kdd12/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    kdd12_stringlookups.update({field:lookup})\n",
    "\n",
    "avazu_stringlookups = {}\n",
    "for field in avazu_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/avazu/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    avazu_stringlookups.update({field:lookup})\n",
    "\n",
    "criteo_stringlookups = {}\n",
    "for field in criteo_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/criteo/categorical_value_counts/{field}.csv')\n",
    "    vocab = [elem.encode() for elem in df['field'].astype(str).to_list()]\n",
    "    lookup = StringLookup(vocabulary=vocab, mask_token=None)\n",
    "    criteo_stringlookups.update({field:lookup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical encoding function\n",
    "@tf.function\n",
    "def kdd12_categorical_encoding(features,label):\n",
    "    # Create copy of features, because modifying inputs causes a ValueError\n",
    "    out_features = features.copy()\n",
    "    # Iteratively map the categical feature columns using the corresponging Lookup layer\n",
    "    for f in kdd12_categorical_columns:\n",
    "        lookup = kdd12_stringlookups[f]\n",
    "        out_features[f.lower()] = lookup(features[f.lower()])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def avazu_categorical_encoding(features,label):\n",
    "    # Create copy of features, because modifying inputs causes a ValueError\n",
    "    out_features = features.copy()\n",
    "    # Iteratively map the categical feature columns using the corresponging Lookup layer\n",
    "    for f in avazu_categorical_columns:\n",
    "        lookup = avazu_stringlookups[f]\n",
    "        out_features[f] = lookup(features[f])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def criteo_categorical_encoding(features,label):\n",
    "    # Create copy of features\n",
    "    out_features = features.copy()\n",
    "    # Iteratively map the categorical feature columns using the corresponding Lookup layer\n",
    "    for f in criteo_categorical_columns:\n",
    "        lookup = criteo_stringlookups[f]\n",
    "        out_features[f] = lookup(features[f])\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mapping over each dataset\n",
    "\n",
    "kdd12_train_encoded = kdd12_train.map(kdd12_categorical_encoding)\n",
    "avazu_train_encoded = avazu_train.map(avazu_categorical_encoding)\n",
    "criteo_train_encoded = criteo_train.map(criteo_categorical_encoding)\n",
    "\n",
    "kdd12_val_encoded = kdd12_val.map(kdd12_categorical_encoding)\n",
    "avazu_val_encoded = avazu_val.map(avazu_categorical_encoding)\n",
    "criteo_val_encoded = criteo_val.map(criteo_categorical_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Variable Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical feature columns\n",
    "kdd12_numerical_columns = [\n",
    "    'Depth',\n",
    "    'Position',\n",
    "    'Impression'\n",
    "]\n",
    "\n",
    "avazu_numerical_columns = [\n",
    "    'hour'\n",
    "]\n",
    "\n",
    "criteo_numerical_columns = [f'int_{i}' for i in np.arange(1,14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scaler dicts for all datasets\n",
    "dist_stats = pd.read_csv('./data/kdd12/means_variances.csv')\n",
    "kdd12_scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    kdd12_scalers.update({field:scaler})\n",
    "\n",
    "dist_stats = pd.read_csv('./data/avazu/means_variances.csv')\n",
    "avazu_scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    avazu_scalers.update({field:scaler})\n",
    "\n",
    "dist_stats = pd.read_csv('./data/criteo/means_variances.csv')\n",
    "criteo_scalers = {}\n",
    "for i in range(len(dist_stats)):\n",
    "    field = dist_stats['field'][i]\n",
    "    mean = dist_stats['mean'][i]\n",
    "    variance = dist_stats['variance'][i]\n",
    "    scaler = Normalization(mean=mean, variance=variance)\n",
    "    criteo_scalers.update({field:scaler})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaler functions for all datasets\n",
    "\n",
    "@tf.function\n",
    "def kdd12_numerical_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in kdd12_numerical_columns:\n",
    "        scaler = kdd12_scalers[f]\n",
    "        out_features[f.lower()] = scaler(features[f.lower()])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def avazu_numerical_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in avazu_numerical_columns:\n",
    "        scaler = avazu_scalers[f]\n",
    "        out_features[f] = scaler(features[f])\n",
    "    return out_features, label\n",
    "\n",
    "@tf.function\n",
    "def criteo_numerical_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in criteo_numerical_columns:\n",
    "        scaler = criteo_scalers[f]\n",
    "        out_features[f] = scaler(features[f])\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_692/423484287.py\", line 8, in kdd12_numerical_scaling  *\n        out_features[f.lower()] = scaler(features[f.lower()])\n    File \"/home/sagemaker-user/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/sagemaker-user/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/layers/preprocessing/normalization.py\", line 188, in build\n        raise ValueError(\n\n    ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None], with unknown axis at index: 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the numerical scaling to all datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m kdd12_train_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mkdd12_train_encoded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkdd12_numerical_scaling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m avazu_train_scaled \u001b[38;5;241m=\u001b[39m avazu_train_encoded\u001b[38;5;241m.\u001b[39mmap(avazu_numerical_scaling)\n\u001b[1;32m      4\u001b[0m criteo_train_scaled \u001b[38;5;241m=\u001b[39m criteo_train_encoded\u001b[38;5;241m.\u001b[39mmap(criteo_numerical_scaling)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2268\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     35\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:107\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    113\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmap_dataset(\n\u001b[1;32m    114\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1222\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1221\u001b[0m   \u001b[38;5;66;03m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1222\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1192\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1196\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:694\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    690\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    691\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    692\u001b[0m )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    699\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m   _set_arg_keywords(concrete_function)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:284\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 284\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    290\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    291\u001b[0m   )\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:308\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m    304\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    305\u001b[0m       placeholder_context\n\u001b[1;32m    306\u001b[0m   )\n\u001b[0;32m--> 308\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    321\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:597\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    594\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 597\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:231\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:161\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    160\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 161\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m ret \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filejov93xcd.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__kdd12_numerical_scaling\u001b[0;34m(features, label)\u001b[0m\n\u001b[1;32m     22\u001b[0m scaler \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m f \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkdd12_numerical_columns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filejov93xcd.py:21\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__kdd12_numerical_scaling.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     19\u001b[0m f \u001b[38;5;241m=\u001b[39m itr\n\u001b[1;32m     20\u001b[0m scaler \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(kdd12_scalers)[ag__\u001b[38;5;241m.\u001b[39mld(f)]\n\u001b[0;32m---> 21\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(out_features)[ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f)\u001b[38;5;241m.\u001b[39mlower, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)] \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/layers/preprocessing/normalization.py:188\u001b[0m, in \u001b[0;36mNormalization.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keep_axis:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_shape[d] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll `axis` values to be kept must have known shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot axis: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput shape: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, with unknown axis at index: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    192\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis, input_shape, d\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Axes to be reduced.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_axis \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ndim) \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keep_axis]\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_692/423484287.py\", line 8, in kdd12_numerical_scaling  *\n        out_features[f.lower()] = scaler(features[f.lower()])\n    File \"/home/sagemaker-user/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/sagemaker-user/.conda/envs/mlds_gpu/lib/python3.10/site-packages/keras/src/layers/preprocessing/normalization.py\", line 188, in build\n        raise ValueError(\n\n    ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None], with unknown axis at index: 0\n"
     ]
    }
   ],
   "source": [
    "# Apply the numerical scaling to all datasets\n",
    "kdd12_train_scaled = kdd12_train_encoded.map(kdd12_numerical_scaling)\n",
    "avazu_train_scaled = avazu_train_encoded.map(avazu_numerical_scaling)\n",
    "criteo_train_scaled = criteo_train_encoded.map(criteo_numerical_scaling)\n",
    "\n",
    "kdd12_val_scaled = kdd12_val_encoded.map(kdd12_numerical_scaling)\n",
    "avazu_val_scaled = avazu_val_encoded.map(avazu_numerical_scaling)\n",
    "criteo_val_scaled = criteo_val_encoded.map(criteo_numerical_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the element specs\n",
    "print('KDD12:')\n",
    "print(kdd12_train_scaled.element_spec)\n",
    "print()\n",
    "print('Avazu:')\n",
    "print(avazu_train_scaled.element_spec)\n",
    "print()\n",
    "print('Criteo:')\n",
    "print(criteo_train_scaled.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to criteo datase\n",
    "\n",
    "@tf.function\n",
    "def criteo_log_scaling(features,label):\n",
    "    out_features = features.copy()\n",
    "    for f in criteo_numerical_columns:\n",
    "        x = features[f]\n",
    "        out_features[f] = tf.where(x>2,tf.math.square(tf.math.log(x)),x)\n",
    "    return out_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criteo_train_scaled = criteo_train_scaled.map(criteo_log_scaling)\n",
    "criteo_val_scaled = criteo_val_scaled.map(criteo_log_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model inputs\n",
    "kdd12_train_model_input = kdd12_train_scaled.take(157440).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "avazu_train_model_input = avazu_train_scaled.take(157440).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "criteo_train_model_input = criteo_train_scaled.take(157440).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "kdd12_val_model_input = kdd12_val_scaled.take(39360).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "avazu_val_model_input = avazu_val_scaled.take(39360).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "criteo_val_model_input = criteo_val_scaled.take(39360).batch(256).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate the features from the ordered dictionary structure for the shallow model section\n",
    "## Define concat function\n",
    "@tf.function\n",
    "def concat_features(features,label,numerical_cols):\n",
    "    out_features_list = []\n",
    "    field_list = list(features.keys())\n",
    "    for field in field_list:\n",
    "        if field in numerical_cols:\n",
    "            out_features_list.append(tf.cast(tf.squeeze(features[field],axis=[-1]),tf.float32))\n",
    "        else:\n",
    "            out_features_list.append(tf.cast(features[field],tf.float32))\n",
    "    out_features = Concatenate()(out_features_list)\n",
    "    return out_features, label\n",
    "\n",
    "## Map the function to each of the datasets to concatinate the features into a single tensor\n",
    "kdd12_numerical_lower = [x.lower() for x in kdd12_numerical_columns]\n",
    "kdd12_train_concat = kdd12_train_model_input.map(lambda x, y: concat_features(x,y,kdd12_numerical_lower))\n",
    "kdd12_val_concat = kdd12_val_model_input.map(lambda x, y: concat_features(x,y,kdd12_numerical_lower))\n",
    "avazu_train_concat = avazu_train_model_input.map(lambda x, y: concat_features(x, y, avazu_numerical_columns))\n",
    "avazu_val_concat = avazu_val_model_input.map(lambda x, y: concat_features(x, y, avazu_numerical_columns))\n",
    "criteo_train_concat = criteo_train_model_input.map(lambda x, y: concat_features(x, y, criteo_numerical_columns))\n",
    "criteo_val_concat = criteo_val_model_input.map(lambda x, y: concat_features(x, y, criteo_numerical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot encode the categorical columns\n",
    "@tf.function\n",
    "def one_hot_categorical(features, labels, categorical_columns):\n",
    "    features_out = features.copy()\n",
    "    for field in categorical_columns:\n",
    "        features_out[field] = tf.one_hot(features[field],depth=50,dtype=tf.float32)\n",
    "    return features_out, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the datasets\n",
    "kdd12_categorical_lower = [x.lower() for x in kdd12_categorical_columns]\n",
    "\n",
    "kdd12_train_ohe = kdd12_train_model_input.map(lambda x, y: one_hot_categorical(x,y,kdd12_categorical_lower))\n",
    "kdd12_val_ohe = kdd12_val_model_input.map(lambda x, y: one_hot_categorical(x,y,kdd12_categorical_lower))\n",
    "avazu_train_ohe = avazu_train_model_input.map(lambda x, y: one_hot_categorical(x,y,avazu_categorical_columns))\n",
    "avazu_val_ohe = avazu_val_model_input.map(lambda x, y: one_hot_categorical(x,y,avazu_categorical_columns))\n",
    "criteo_train_ohe = criteo_train_model_input.map(lambda x, y: one_hot_categorical(x,y,criteo_categorical_columns))\n",
    "criteo_val_ohe = criteo_val_model_input.map(lambda x, y: one_hot_categorical(x,y,criteo_categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to concatinate all features together\n",
    "@tf.function\n",
    "def combine(features, labels):\n",
    "    columns = list(features.keys())\n",
    "    out_cols = []\n",
    "    for field in columns:\n",
    "        out_cols.append(features[field])\n",
    "    features_out = tf.squeeze(tf.concat(out_cols,axis=-1),axis=-2)\n",
    "    return features_out, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd12_train_ohe = kdd12_train_ohe.map(combine)\n",
    "kdd12_val_ohe = kdd12_val_ohe.map(combine)\n",
    "avazu_train_ohe = avazu_train_ohe.map(combine)\n",
    "avazu_val_ohe = avazu_val_ohe.map(combine)\n",
    "criteo_train_ohe = criteo_train_ohe.map(combine)\n",
    "criteo_val_ohe = criteo_val_ohe.map(combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define feature mappings\n",
    "kdd12_fixlen_feature_columns = [SparseFeat(feat.lower(), vocabulary_size=len(kdd12_stringlookups[feat].get_vocabulary()), embedding_dim=4) for feat in kdd12_categorical_columns]\\\n",
    "+ [DenseFeat(feat.lower(),1) for feat in kdd12_numerical_columns]\n",
    "avazu_fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=len(avazu_stringlookups[feat].get_vocabulary()), embedding_dim=4) for feat in avazu_categorical_columns]\\\n",
    "+ [DenseFeat(feat,1) for feat in avazu_numerical_columns]\n",
    "criteo_fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=len(criteo_stringlookups[feat].get_vocabulary()), embedding_dim=4) for feat in criteo_categorical_columns]\\\n",
    "+ [DenseFeat(feat,1) for feat in criteo_numerical_columns]\n",
    "\n",
    "## Generate the dnn and linear feature columns\n",
    "kdd12_dnn_feature_columns = kdd12_fixlen_feature_columns\n",
    "kdd12_linear_feature_columns = kdd12_fixlen_feature_columns\n",
    "\n",
    "avazu_dnn_feature_columns = avazu_fixlen_feature_columns\n",
    "avazu_linear_feature_columns = avazu_fixlen_feature_columns\n",
    "\n",
    "criteo_dnn_feature_columns = criteo_fixlen_feature_columns\n",
    "criteo_linear_feature_columns = criteo_fixlen_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Replication and Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model checkpoint directories if they do not exist\n",
    "for model in ['lr','fm','fnn','pnn','wdl','dfm','autoint']:\n",
    "    for dataset in ['kdd12','avazu','criteo']:\n",
    "        if not os.path.exists(f'./models/{model}/{dataset}'):\n",
    "            os.makedirs(f'./models/{model}/{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "# Define the precision, recall and auc metrics\n",
    "precision = tf.keras.metrics.Precision(thresholds=0.5,name='precision')\n",
    "recall = tf.keras.metrics.Recall(thresholds=0.5,name='recall')\n",
    "auc = tf.keras.metrics.AUC(name='auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Models\n",
    "\n",
    "In order assess the effectiveness of Deep Learning methods for CTR prediction, it makes sense to attain a model score using more traditional model architectures in order to use these as a comparison. (Zhang et al., 2021) briefly discuss the how CTR applications steadily progressed from Shallow to Deep models. The most basic shallow model for the CTR binary classification task is Logistic Regression (Richardson, Dominowska & Ragno, 2007) which benefits from “high efficiency and ease for fast deployment” (Zhang et al., 2021). However, as it became apparent that identifying key feature interactions is crucial for the CTR prediction task, architectures that explicitly capture these interactions such as the Factorization Machine (Juan et al., 2016; Rendle, 2010) became increasingly popular in the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to attain representitive results from beyond the realm of deep learning, below I evaluate two of the most poplarly used shallow models for CTR prediction:\n",
    "\n",
    "- [Logistic regression](https://www.tensorflow.org/guide/core/logistic_regression_core)\n",
    "- Factorization machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "In logistic regression, we use the logistic (a.k.a. sigmoid) function as our function $f_\\theta$, i.e:\n",
    "$$\n",
    "\\mathcal{P}\\left(\\mathrm{Click}\\right)=\\frac{e^{\\beta_0+\\sum_{i}{\\beta_ix_i}}}{1+e^{\\beta_0+\\sum_{i}{\\beta_ix_i}}}\n",
    "$$\n",
    "(James et al., 2021) show that the equation above can be rearranged as follows:\n",
    "$$\n",
    "\\beta_0+\\sum_{i}{\\beta_ix_i}=log{\\left(\\frac{\\mathcal{P}\\left(\\mathrm{Click}\\right)}{1-\\mathcal{P}\\left(\\mathrm{Click}\\right)}\\right)}\n",
    "$$\n",
    "Logistic regression therefore effectively amounts to fitting a linear regression function to the log odds of a that there is a click for a given instance, parameterized by $\\theta=\\left(\\beta_0,\\beta_1,\\ldots,\\beta_n\\right)$. (Richardson, Dominowska & Ragno, 2007) showed that it is possible to fit a logistic regression model on search advertising data in order to provide accurate click-through rate predictions for each ad, and even demonstrated that the used of this model “improves convergence and performance on the advertising system”. As stated previously, a major benefit of the logistic model over all of the others mentioned here is that since it works on the basis of a simple linear combination of the given features, it is the simplest model to fit and deploy. However, a major shortcoming of this model is the fact that it does not account for feature interactions, and it therefore fails to pick up on discriminating patterns given by the cross-features in the highly sparse categorical ad marketplace data (Zhang et al., 2021).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I replicate the Logistic regression model in Tensorflow by defining a `Sequential` model class with a single `Dense` layer with sigmoid activation. The Dense layer simply implements the linear combination $\\beta_0+\\sum_{i}{\\beta_ix_i}$ from above, before passing the result through the sigmoid function $\\sigma(x) = \\frac{e^x}{1+e^x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_lr_csvLogger = CSVLogger('logs/kdd12_lr.csv')\n",
    "avazu_lr_csvLogger = CSVLogger('logs/avazu_lr.csv')\n",
    "criteo_lr_csvLogger = CSVLogger('logs/criteo_lr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "# Define the model saving checkpoints\n",
    "kdd12_lr_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/lr/kdd12/kdd12_lr.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_lr_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/lr/avazu/avazu_lr.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_dfm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/lr/criteo/criteo_lr.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single dense layer sequential model using sigmoid activation function\n",
    "\n",
    "kdd12_lr_model = Sequential([\n",
    "    Input(shape=(len(kdd12_train_model_input.element_spec[0],))),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])\n",
    "avazu_lr_model = Sequential([\n",
    "    Input(shape=(len(avazu_train_model_input.element_spec[0],))),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])\n",
    "criteo_lr_model = Sequential([\n",
    "    Input(shape=(len(criteo_train_model_input.element_spec[0],))),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "\n",
    "kdd12_lr_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")\n",
    "avazu_lr_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")\n",
    "criteo_lr_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models to the datasets\n",
    "\n",
    "kdd12_lr_model.fit(\n",
    "    kdd12_train_concat,\n",
    "    validation_data=kdd12_val_concat,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[kdd12_lr_csvLogger, kdd12_lr_modelCheckpoint, earlystopping]\n",
    ")\n",
    "avazu_lr_model.fit(\n",
    "    avazu_train_concat,\n",
    "    validation_data=avazu_val_concat,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[avazu_lr_csvLogger, avazu_lr_modelCheckpoint, earlystopping]\n",
    ")\n",
    "criteo_lr_model.fit(\n",
    "    criteo_train_concat,\n",
    "    validation_data=criteo_val_concat,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[criteo_lr_csvLogger, criteo_lr_modelCheckpoint, earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses for each epoch\n",
    "# Read in the history logs\n",
    "kdd12_history = pd.read_csv('logs/kdd12_lr.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_lr.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_lr.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorization Machines were first introduced in \\citep{RefWorks:rendle2010factorization} as\n",
    "a model class that ``combines the advantages of Support Vector Machines (SVM) with factorization models''.\n",
    "The model is able to capture the second order feature interactions in the data, which is a key advantage over\n",
    "Logistic Regression. The model is defined as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y}(\\mathbf{x}) = w_0 + \\sum_{i=1}^{n} w_i x_i + \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j\n",
    "$$\n",
    "\n",
    "where $w_0$ is the bias term, $w_i$ are the weights for the $i$-th feature, $\\mathbf{v}_i$ are the latent vectors for the $i$-th feature.\n",
    "Rendel (2020) shows that the learned biases and weights of the FM model can be\n",
    "computed in linear time, ``and can be learned efficiently by gradient descent methods'', such as Stochastic Gradient Descent (SGD).\n",
    "\n",
    "In the code below, I replicate the FM model by first creating a custom `tf.keras.layers.Layer` class that carries out the\n",
    "forward calculation above. The trainable weights of the layer are then optimized using Stochastic Gradient Descent with \n",
    "`learning_rate=0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom FactorizationMachine layer\n",
    "class FactorizationMachine(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, name=\"FM_layer\"):\n",
    "        super(FactorizationMachine, self).__init__(name=name)\n",
    "        self.k = k\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w0 = self.add_weight(\"bias\", shape=(1,), initializer=tf.keras.initializers.GlorotNormal())\n",
    "        self.W = self.add_weight(\"weights\", shape=(input_shape[-1],), initializer=tf.keras.initializers.GlorotNormal())\n",
    "        self.V = self.add_weight(\"interaction_factors\", shape=(self.k, input_shape[-1]), initializer=tf.keras.initializers.GlorotNormal())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        linear_terms = tf.add(\n",
    "            self.w0,\n",
    "            tf.reduce_sum(\n",
    "                tf.multiply(self.W, inputs),\n",
    "                axis=1,\n",
    "                keepdims=True\n",
    "            )\n",
    "        )\n",
    "        interaction_terms = tf.multiply(\n",
    "            0.5,\n",
    "            tf.reduce_sum(\n",
    "                tf.math.subtract(\n",
    "                    tf.pow(tf.matmul(inputs, tf.transpose(self.V)), 2),\n",
    "                    tf.matmul(tf.pow(inputs, 2), tf.transpose(tf.pow(self.V, 2)))\n",
    "                ),\n",
    "                axis=1, \n",
    "                keepdims=True\n",
    "            )\n",
    "        )\n",
    "        return tf.add(linear_terms,interaction_terms)\n",
    "        \n",
    "    # Have to overwite the get_config method to save the model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'k':self.k})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequential Factorization Machine Models\n",
    "kdd12_fm_model = Sequential([\n",
    "    Input(shape=(kdd12_train_ohe.element_spec[0].shape[-1],)),\n",
    "    FactorizationMachine(5),\n",
    "    Activation('sigmoid')\n",
    "])\n",
    "avazu_fm_model = Sequential([\n",
    "    Input(shape=(avazu_train_ohe.element_spec[0].shape[-1],)),\n",
    "    FactorizationMachine(5),\n",
    "    Activation('sigmoid')\n",
    "])\n",
    "criteo_fm_model = Sequential([\n",
    "    Input(shape=(criteo_train_ohe.element_spec[0].shape[-1],)),\n",
    "    FactorizationMachine(5),\n",
    "    Activation('sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_fm_csvLogger = CSVLogger('logs/kdd12_fm.csv')\n",
    "avazu_fm_csvLogger = CSVLogger('logs/avazu_fm.csv')\n",
    "criteo_fm_csvLogger = CSVLogger('logs/criteo_fm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_fm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fm/kdd12/kdd12_fm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_fm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fm/avazu/avazu_fm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_fm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fm/criteo/criteo_fm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "\n",
    "kdd12_fm_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")\n",
    "avazu_fm_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")\n",
    "criteo_fm_model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "kdd12_fm_model.fit(\n",
    "    kdd12_train_ohe,\n",
    "    validation_data = kdd12_val_ohe,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[kdd12_fm_csvLogger, kdd12_fm_modelCheckpoint, earlystopping,auc]\n",
    ")\n",
    "avazu_fm_model.fit(\n",
    "    avazu_train_ohe,\n",
    "    validation_data = avazu_val_ohe,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[avazu_fm_csvLogger, avazu_fm_modelCheckpoint, earlystopping,auc]\n",
    ")\n",
    "criteo_fm_model.fit(\n",
    "    criteo_train_ohe,\n",
    "    validation_data = criteo_val_ohe,\n",
    "    batch_size=256,\n",
    "    epochs=15,\n",
    "    callbacks=[criteo_fm_csvLogger, criteo_fm_modelCheckpoint, earlystopping,auc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses for each epoch\n",
    "# Read in the history logs\n",
    "kdd12_history = pd.read_csv('logs/kdd12_fm.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_fm.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_fm.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, I will explore a number of deep learning models. I selected five popular models on the basis of the following criteria\n",
    "\n",
    "- Competitive predition accuracy in the KDD12, Criteo and Avazu datasets as published on [PapersWithCode](https://paperswithcode.com/)\n",
    "- Ideally, I was looking for a representitive set of models for each model type as discussed in (Zhang et. al. 2021). Therefore I was looking for models that employed Product Interaction Opetators, Attention Operators and Factorization Machines as a basis.\n",
    "- The code for the model has to be accessible and intuitive to use.\n",
    "\n",
    "On the basis of the above critea, I have chosen the following models to explore:\n",
    "\n",
    "- Factorization Supported Neural Networks\n",
    "- Product Based Neural Networks\n",
    "- Wide and Deep\n",
    "- DeepFM\n",
    "- Automatic Feature Interaction (AutoInt)\n",
    "\n",
    "In the section below, I briefly introduce each of the models, and evaluate against the benchmark datasets loaded and preprocessed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization-Machine Supported Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Deep Learning model that we will consider is the Factorization Supported\n",
    "Neural Network (FNN) model proposed by Zhang et. al. (2016). The model works by first training a Factorization Machine\n",
    "model on the sparse-encoded categorical input features. It then uses the latent vectors learned by the FM model (see $\\mathbf{v}_i$ in the equation above)\n",
    "as inputs to a Neural Network, as shown in the figure below. In doing so, the FNN model is effectively using the FM latent factors to initialize the embedding layer of the Neural Network.\n",
    "The DNN is then able to learn the higher order feature interactions in the data, which the FM model is unable to capture.\n",
    "\n",
    "![FNN](figures/fnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "kdd12_fnn_model = FNN(\n",
    "    kdd12_linear_feature_columns, \n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    "    )\n",
    "kdd12_fnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "avazu_fnn_model = FNN(\n",
    "    avazu_linear_feature_columns, \n",
    "    avazu_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    "    )\n",
    "avazu_fnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "criteo_fnn_model = FNN(\n",
    "    criteo_linear_feature_columns, \n",
    "    criteo_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    "    )\n",
    "criteo_fnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_fnn_csvLogger = CSVLogger('logs/kdd12_fnn.csv')\n",
    "avazu_fnn_csvLogger = CSVLogger('logs/avazu_fnn.csv')\n",
    "criteo_fnn_csvLogger = CSVLogger('logs/criteo_fnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_fnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fnn/kdd12/kdd12_fnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_fnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fnn/avazu/avazu_fnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_fnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/fnn/criteo/criteo_fnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "kdd12_fnn_history = kdd12_fnn_model.fit(kdd12_train_model_input, validation_data=kdd12_val_model_input, batch_size=256, epochs=15, callbacks=[kdd12_fnn_csvLogger,kdd12_fnn_modelCheckpoint,earlystopping])\n",
    "avazu_fnn_history = avazu_fnn_model.fit(avazu_train_model_input, validation_data=avazu_val_model_input, batch_size=256, epochs=15, callbacks=[avazu_fnn_csvLogger,avazu_fnn_modelCheckpoint,earlystopping])\n",
    "criteo_fnn_history = criteo_fnn_model.fit(criteo_train_model_input, validation_data=criteo_val_model_input, batch_size=256, epochs=15, callbacks=[criteo_fnn_csvLogger,criteo_fnn_modelCheckpoint,earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_fnn_history = pd.read_csv('logs/kdd12_fnn.csv')\n",
    "avazu_fnn_history = pd.read_csv('logs/avazu_fnn.csv')\n",
    "criteo_fnn_history = pd.read_csv('logs/criteo_fnn.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_fnn_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_fnn_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_fnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_fnn_history['binary_crossentropy'])\n",
    "plt.plot(avazu_fnn_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_fnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_fnn_history['binary_crossentropy'])\n",
    "plt.plot(criteo_fnn_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_fnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the models\n",
    "kdd12_fnn_score = kdd12_fnn_model.evaluate(kdd12_val_model_input, batch_size=256)\n",
    "avazu_fnn_score = avazu_fnn_model.evaluate(avazu_val_model_input, batch_size=256)\n",
    "criteo_fnn_score = criteo_fnn_model.evaluate(criteo_val_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Based Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Product Based Neural Network (PNN) model\n",
    "proposed by Qu et. al. (2016) is another Deep Learning\n",
    "model that was developed around the same time as the FNN model. The key \n",
    "innovation of the PNN moel is the use of a pair-wisely connected Product Layer\n",
    "after a field-wise connected embetting layer for the categorical features, as shown\n",
    "in the figure below. The Product Layer is able to directly model inter-field feature\n",
    "interaction by means of either an inner product or outer production operation, and then further\n",
    "distill higher feature inturactions by passing the output of the Product Layer through fully\n",
    "connected MLP layers.\n",
    "\n",
    "![PNN](figures/pnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_pnn_csvLogger = CSVLogger('logs/kdd12_pnn.csv')\n",
    "avazu_pnn_csvLogger = CSVLogger('logs/avazu_pnn.csv')\n",
    "criteo_pnn_csvLogger = CSVLogger('logs/criteo_pnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_pnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/pnn/kdd12/kdd12_pnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_pnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/pnn/avazu/avazu_pnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_pnn_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/pnn/criteo/criteo_pnn.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the PNN models\n",
    "kdd12_pnn_model = PNN(\n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "kdd12_pnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "avazu_pnn_model = PNN(\n",
    "    avazu_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "avazu_pnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "criteo_pnn_model = PNN(\n",
    "    criteo_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "criteo_pnn_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "kdd12_pnn_history = kdd12_pnn_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data=kdd12_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=40, \n",
    "    callbacks=[kdd12_pnn_csvLogger,kdd12_pnn_modelCheckpoint,earlystopping]\n",
    ")\n",
    "avazu_pnn_history = avazu_pnn_model.fit(\n",
    "    avazu_train_model_input,\n",
    "    validation_data=avazu_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=40,\n",
    "    callbacks=[avazu_pnn_csvLogger,avazu_pnn_modelCheckpoint,earlystopping]\n",
    ")\n",
    "criteo_pnn_history = criteo_pnn_model.fit(\n",
    "    criteo_train_model_input,\n",
    "    validation_data=criteo_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=40,\n",
    "    callbacks=[criteo_pnn_csvLogger,criteo_pnn_modelCheckpoint,earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_history = pd.read_csv('logs/kdd12_pnn.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_pnn.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_pnn.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_pnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_pnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_pnn_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide and Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wide \\& Deep Learning (WDL) model proposed by Cheng et. al. (2016) introduces the concept\n",
    "of dual-tower model architecture (Zhang et. al. 2021). While both the FNN and the PNN models\n",
    "generally tend to be constructed as a single fully connected DNN model, the Wide \\& Deep model\n",
    "consists of a wide component, consisting of a three layer Deep Neural Network that takes the concatinated\n",
    "embedding vectors of the categorical features as input, and a deep component, consisting of a cross product\n",
    "transformation of selected sparse categorical features. The logits from the wide and deep components are added\n",
    "together to produce the final prediction. The architecture of the WDL model is shown in the figure below.\n",
    "\n",
    "![WDL](figures/wdl.png)\n",
    "\n",
    "The purpose behind the Dual-Tower architecture is to counteract the tendancy of the fully connected\n",
    "single tower DNN models to lose the ability to capture low-order feature interactions (Zhang et. al. 2021).\n",
    "The Wide component is able to capture the low-order feature interactions, while the Deep component is able to capture\n",
    "the higher order feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the WDL models\n",
    "kdd12_wdl_model = WDL(\n",
    "    kdd12_linear_feature_columns, \n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "kdd12_wdl_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "avazu_wdl_model = WDL(\n",
    "    avazu_linear_feature_columns, \n",
    "    avazu_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "avazu_wdl_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "criteo_wdl_model = WDL(\n",
    "    criteo_linear_feature_columns, \n",
    "    criteo_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    )\n",
    "criteo_wdl_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_wdl_csvLogger = CSVLogger('logs/kdd12_wdl.csv')\n",
    "avazu_wdl_csvLogger = CSVLogger('logs/avazu_wdl.csv')\n",
    "criteo_wdl_csvLogger = CSVLogger('logs/criteo_wdl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_wdl_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/wdl/kdd12/kdd12_wdl.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_wdl_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/wdl/avazu/avazu_wdl.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_dfm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/wdl/criteo/criteo_wdl.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Wide and Deep models\n",
    "kdd12_wdl_history = kdd12_wdl_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data=kdd12_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[kdd12_wdl_csvLogger,kdd12_wdl_modelCheckpoint,earlystopping]\n",
    ")\n",
    "avazu_wdl_history = avazu_wdl_model.fit(\n",
    "    avazu_train_model_input, \n",
    "    validation_data=avazu_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[avazu_wdl_csvLogger,avazu_wdl_modelCheckpoint,earlystopping]\n",
    ")\n",
    "criteo_wdl_history = criteo_wdl_model.fit(\n",
    "    criteo_train_model_input, \n",
    "    validation_data=criteo_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[criteo_wdl_csvLogger, criteo_wdl_modelCheckpoint,earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_history = pd.read_csv('logs/kdd12_wdl.csv')\n",
    "avazu_history = pd.read_csv('logs/avazu_wdl.csv')\n",
    "criteo_history = pd.read_csv('logs/criteo_wdl.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_wdl_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_history['binary_crossentropy'])\n",
    "plt.plot(avazu_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_wdl_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_history['binary_crossentropy'])\n",
    "plt.plot(criteo_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_wdl_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepFM model proposed by Guo et. al. 2017 can be thought of as an\n",
    "imporvement of the aforementioned FNN (Zhang et. al., 2016) and WDL (Cheng et. al., 2016) models.\n",
    "Like the FNN model, the DeepFM model usilises the Factorization Machine model (Rendel, 2010)\n",
    "to learn lower-order feature interactions. However, it also employs a dual-tower architecture\n",
    "like the WDL model, with the Wide component being the FM model and the Deep component being a fully connected\n",
    "DNN model. The DeepFM model is therefore able to avoid the limitations on capturing low-order\n",
    "interactions that are inherent in the FNN model. In addition, due the the application of the FM to all\n",
    "feature embeddings, the DeepFM model eliminates the need to choose which features \n",
    "to feed through the wide component, as is the case in the WDL model. The architecture of the DeepFM model is shown \n",
    "in the figure below.\n",
    "\n",
    "![DFM](figures/dfm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the DeepFM models\n",
    "kdd12_dfm_model = DeepFM(\n",
    "    kdd12_linear_feature_columns, \n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    ")\n",
    "kdd12_dfm_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "avazu_dfm_model = DeepFM(\n",
    "    avazu_linear_feature_columns, \n",
    "    avazu_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    ")\n",
    "avazu_dfm_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "criteo_dfm_model = DeepFM(\n",
    "    criteo_linear_feature_columns, \n",
    "    criteo_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    ")\n",
    "criteo_dfm_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSVLogger callbacks\n",
    "kdd12_dfm_csvLogger = CSVLogger('logs/kdd12_dfm.csv')\n",
    "avazu_dfm_csvLogger = CSVLogger('logs/avazu_dfm.csv')\n",
    "criteo_dfm_csvLogger = CSVLogger('logs/criteo_dfm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_dfm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/dfm/kdd12/kdd12_dfm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_dfm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/dfm/avazu/avazu_dfm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_dfm_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/dfm/criteo/criteo_dfm.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the DeepFM models\n",
    "'''\n",
    "kdd12_dfm_history = kdd12_dfm_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data = kdd12_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[kdd12_dfm_csvLogger,kdd12_dfm_modelCheckpoint,earlystopping]\n",
    ")\n",
    "'''\n",
    "avazu_dfm_history = avazu_dfm_model.fit(\n",
    "    avazu_train_model_input, \n",
    "    validation_data= avazu_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[avazu_dfm_csvLogger,avazu_dfm_modelCheckpoint,earlystopping]\n",
    ")\n",
    "\n",
    "criteo_dfm_history = criteo_dfm_model.fit(\n",
    "    criteo_train_model_input, \n",
    "    validation_data=criteo_val_model_input, \n",
    "    batch_size=256, epochs=15,\n",
    "    callbacks=[criteo_dfm_csvLogger,criteo_dfm_modelCheckpoint,earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_dfm_history = pd.read_csv('logs/kdd12_dfm.csv')\n",
    "avazu_dfm_history = pd.read_csv('logs/avazu_dfm.csv')\n",
    "criteo_dfm_history = pd.read_csv('logs/criteo_dfm.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_dfm_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_dfm_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_dfm_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_dfm_history['binary_crossentropy'])\n",
    "plt.plot(avazu_dfm_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_dfm_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_dfm_history['binary_crossentropy'])\n",
    "plt.plot(criteo_dfm_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_dfm_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Feature Interaction (AutoInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Autotomatic Feature Interaction Learning (AutoInt) model proposed by\n",
    "Song et. al. (2019) makes use of a multi-head self attention\n",
    "network to model the important feature interactions in the data. The initial \n",
    "paper separates the model into three parts: an embedding layer, an interaction layer \n",
    "and an output layer. The embedding layer aims to project each sparse multi-value\n",
    "categorical a and dense numerical feature into a lower dimensional space, as per the below:\n",
    "\n",
    "$$\n",
    "\\mathbf{e_i} = \\frac{1}{q} \\mathbf{V_i x_i}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{V_i}$ is the embedding matrix for the $i$-th field, $x_i$ is a multi-hot vector, and $q$ \n",
    "is the number of non-zero values in $x_i$. The interaction layer employs the multi-head\n",
    "mechanism to determine which higher order feature interaction are meaningful in the data. This not only\n",
    "improves the efficiency of model traning, but it also improves the model's explainability. Lastly,\n",
    "the output layer is a fully connected layer that takes in the concatinated output \n",
    "of the interaction layer, and applies the sigmoid activation function to produce the final prediction.\n",
    "The architecture of the AutoInt model is shown in Figure below.\n",
    "\n",
    "![AUTOINT](figures/autoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the AutoInt Models\n",
    "kdd12_autoint_model = AutoInt(\n",
    "    kdd12_linear_feature_columns, \n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    "    )\n",
    "kdd12_autoint_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "avazu_autoint_model = AutoInt(\n",
    "    avazu_linear_feature_columns, \n",
    "    avazu_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    "    )\n",
    "avazu_autoint_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )\n",
    "\n",
    "criteo_autoint_model = AutoInt(\n",
    "    criteo_linear_feature_columns, \n",
    "    criteo_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    "    )\n",
    "criteo_autoint_model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall,auc], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AutoInt CSVLogger callbacks\n",
    "kdd12_autoint_csvLogger = CSVLogger('logs/kdd12_autoint.csv')\n",
    "avazu_autoint_csvLogger = CSVLogger('logs/avazu_autoint.csv')\n",
    "criteo_autoint_csvLogger = CSVLogger('logs/criteo_autoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model saving checkpoints\n",
    "kdd12_autoint_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/autoint/kdd12/kdd12_autoint.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "avazu_autoint_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/autoint/avazu/avazu_autoint.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "criteo_autoint_modelCheckpoint = ModelCheckpoint(\n",
    "    'models/autoint/criteo/criteo_autoint.ckpt',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the AutoInt models\n",
    "kdd12_autoint_model.fit(\n",
    "    kdd12_train_model_input, \n",
    "    validation_data= kdd12_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15, \n",
    "    callbacks=[kdd12_autoint_csvLogger, kdd12_autoint_modelCheckpoint,earlystopping]\n",
    ")\n",
    "avazu_autoint_model.fit(\n",
    "    avazu_train_model_input,\n",
    "    validation_data=avazu_val_model_input,\n",
    "    batch_size=256, \n",
    "    epochs=15, \n",
    "    callbacks=[avazu_autoint_csvLogger, avazu_autoint_modelCheckpoint,earlystopping]\n",
    ")\n",
    "criteo_autoint_model.fit(\n",
    "    criteo_train_model_input, \n",
    "    validation_data=criteo_val_model_input, \n",
    "    batch_size=256, \n",
    "    epochs=15,\n",
    "    callbacks=[criteo_autoint_csvLogger,criteo_autoint_modelCheckpoint,earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the history logs\n",
    "kdd12_autoint_history = pd.read_csv('logs/kdd12_autoint.csv')\n",
    "avazu_autoint_history = pd.read_csv('logs/avazu_autoint.csv')\n",
    "criteo_autoint_history = pd.read_csv('logs/criteo_autoint.csv')\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(kdd12_autoint_history['binary_crossentropy'])\n",
    "plt.plot(kdd12_autoint_history['val_binary_crossentropy'])\n",
    "plt.title('kdd12_autoint_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avazu_autoint_history['binary_crossentropy'])\n",
    "plt.plot(avazu_autoint_history['val_binary_crossentropy'])\n",
    "plt.title('avazu_autoint_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(criteo_autoint_history['binary_crossentropy'])\n",
    "plt.plot(criteo_autoint_history['val_binary_crossentropy'])\n",
    "plt.title('criteo_autoint_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results and Discussion\n",
    "\n",
    "Below I construct and display a pandas dataframe of the best results in each of the logs in terms of validation loss. The accompanying validation metrics are also shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pandas dataframe of the model scores for the kdd12 dataset\n",
    "models = ['lr','fm','fnn','pnn','wdl','dfm','autoint']\n",
    "datasets = ['kdd12','avazu','criteo']\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.DataFrame()\n",
    "    for model in models:\n",
    "        log_file = f'logs/{dataset}_{model}.csv'\n",
    "        history = pd.read_csv(log_file)\n",
    "        min_loss = history[history['val_loss']==history['val_loss'].min()]\n",
    "        min_loss_val = min_loss.loc[:,['epoch','val_binary_crossentropy','val_binary_accuracy','val_precision','val_recall']]\n",
    "        min_loss_val['model'] = model\n",
    "        df = pd.concat([df,min_loss_val],axis=0)\n",
    "    df = df.set_index('model')\n",
    "    df.to_csv(f'logs/{dataset}_model_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the the model scores by data set\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'logs/{dataset}_model_scores.csv')\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(f'{dataset.capitalize()} Model Scores')\n",
    "    \n",
    "    # Plot a scatter plot of binary crossentropy loss vs binary accuracy\n",
    "    sns.scatterplot(ax=axs[0], data=df, x='val_binary_crossentropy', y='val_binary_accuracy', hue='model', palette='tab10')\n",
    "    axs[0].set_title('Binary Crossentropy vs Binary Accuracy')\n",
    "    axs[0].set_xlabel('Binary Crossentropy')\n",
    "    axs[0].set_ylabel('Binary Accuracy')\n",
    "\n",
    "    # Plot a scatter plot of Precision vs Recall\n",
    "    sns.scatterplot(ax=axs[1], data=df, x='val_precision', y='val_recall', hue='model', palette='tab10')\n",
    "    axs[1].set_title('Precision vs Recall')\n",
    "    axs[1].set_xlabel('Precision')\n",
    "    axs[1].set_ylabel('Recall')\n",
    "\n",
    "    # Plot a bar chart of epoch number by model\n",
    "    sns.barplot(ax=axs[2], data=df, x='model', y='epoch', palette='tab10')\n",
    "    axs[2].set_title('Epochs to Convergence')\n",
    "    axs[2].set_xlabel('Model')\n",
    "    axs[2].set_ylabel('Epochs')\n",
    "\n",
    "    # Arrange the subplots so that the axis labels do not overlap\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    # Save the figure in the figures folder and show the plot\n",
    "    plt.savefig(f'figures/{dataset}_model_scores.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same except without the LR model\n",
    "\n",
    "# Plot the the model scores by data set\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'logs/{dataset}_model_scores.csv')\n",
    "    df = df[df['model']!='lr']\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(f'{dataset.capitalize()} Model Scores')\n",
    "    \n",
    "    # Plot a scatter plot of binary crossentropy loss vs binary accuracy\n",
    "    sns.scatterplot(ax=axs[0], data=df, x='val_binary_crossentropy', y='val_binary_accuracy', hue='model', palette='tab10')\n",
    "    axs[0].set_title('Binary Crossentropy vs Binary Accuracy')\n",
    "    axs[0].set_xlabel('Binary Crossentropy')\n",
    "    axs[0].set_ylabel('Binary Accuracy')\n",
    "\n",
    "    # Plot a scatter plot of Precision vs Recall\n",
    "    sns.scatterplot(ax=axs[1], data=df, x='val_precision', y='val_recall', hue='model', palette='tab10')\n",
    "    axs[1].set_title('Precision vs Recall')\n",
    "    axs[1].set_xlabel('Precision')\n",
    "    axs[1].set_ylabel('Recall')\n",
    "\n",
    "    # Plot a bar chart of epoch number by model\n",
    "    sns.barplot(ax=axs[2], data=df, x='model', y='epoch', palette='tab10')\n",
    "    axs[2].set_title('Epochs to Convergence')\n",
    "    axs[2].set_xlabel('Model')\n",
    "    axs[2].set_ylabel('Epochs')\n",
    "\n",
    "    # Arrange the subplots so that the axis labels do not overlap\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    # Show the plot\n",
    "    #plt.savefig(f'figures/{dataset}_model_scores.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Ad Personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the kdd12 dfm model from the saved weights\n",
    "## Create lists of categorical colums for each dataset\n",
    "kdd12_categorical_columns = [\n",
    "    'DisplayURL',\n",
    "    'AdID',\n",
    "    'AdvertiserID',\n",
    "    'QueryID',\n",
    "    'KeywordID',\n",
    "    'TitleID',\n",
    "    'DescriptionID',\n",
    "    'UserID'\n",
    "]\n",
    "\n",
    "# Import categorical feature mappings and define stringloohup objects for each dataset\n",
    "kdd12_vocab_sizes = {}\n",
    "for field in kdd12_categorical_columns:\n",
    "    df = pd.read_csv(f'./data/kdd12/categorical_value_counts/{field}.csv')\n",
    "    vocab_size = len(df['field'])+1\n",
    "    kdd12_vocab_sizes.update({field:vocab_size})\n",
    "\n",
    "# Define numerical feature columns\n",
    "kdd12_numerical_columns = [\n",
    "    'Depth',\n",
    "    'Position',\n",
    "    'Impression'\n",
    "]\n",
    "\n",
    "## Define feature mappings\n",
    "kdd12_fixlen_feature_columns = [SparseFeat(feat.lower(), vocabulary_size=kdd12_vocab_sizes[feat], embedding_dim=4) for feat in kdd12_categorical_columns]\\\n",
    "+ [DenseFeat(feat.lower(),1) for feat in kdd12_numerical_columns]\n",
    "\n",
    "## Generate the dnn and linear feature columns\n",
    "kdd12_dnn_feature_columns = kdd12_fixlen_feature_columns\n",
    "kdd12_linear_feature_columns = kdd12_fixlen_feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 18:27:52.127379: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-08-05 18:27:52.165997: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-05 18:27:52.332718: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7fd20c42cd90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7fd20c42cd90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "precision = Precision(thresholds=0.5,name='precision')\n",
    "recall = Recall(thresholds=0.5,name='recall')\n",
    "\n",
    "# Compile the DeepFM models\n",
    "kdd12_dfm_model = DeepFM(\n",
    "    kdd12_linear_feature_columns, \n",
    "    kdd12_dnn_feature_columns, \n",
    "    task='binary',\n",
    "    dnn_dropout=0.6,\n",
    "    dnn_hidden_units=[200,200,200],\n",
    "    l2_reg_dnn=0.005,\n",
    "    l2_reg_linear = 0.005,\n",
    "    l2_reg_embedding=0.005,\n",
    "    dnn_use_bn=True\n",
    ")\n",
    "kdd12_dfm_model.compile(adam_optimizer, \"binary_crossentropy\", metrics=['binary_crossentropy','binary_accuracy',precision,recall], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd20c32c610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd12_dfm_model.load_weights('models/dfm/kdd12/kdd12_dfm.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = kdd12_dfm_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fd219e67f10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd12_dfm_model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlds_gpu)",
   "language": "python",
   "name": "mlds_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
