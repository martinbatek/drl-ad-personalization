@inbook{RefWorks:james2021classification,
	author={Gareth James and Daniela Witten and Trevor Hastie and Robert Tibshirani},
	year={2021},
	title={Classification},
	series={An Introduction to Statistical Learning: with Applications in R},
	publisher={Springer US},
	address={New York, NY},
	pages={129-195},
	abstract={The linear regression model discussed in Chap. 3assumes that the response variable Y is quantitative. But in many situations, the response variable is instead qualitative."},
	keywords={ShallowMethods},
	isbn={978-1-0716-1418-1},
	url={https://doi.org/10.1007/978-1-0716-1418-1_4},
	doi={10.1007/978-1-0716-1418-1_4}
}
@article{RefWorks:guo2017deepfm:,
	author={Huifeng Guo and Ruiming Tang and Yunming Ye and Zhenguo Li and Xiuqiang He},
	year={2017},
	title={DeepFM: A Factorization-Machine based Neural Network for CTR Prediction},
	journal={CoRR},
	volume={abs/1703.04247},
	note={1703.04247},
	url={http://arxiv.org/abs/1703.04247}
}
@inproceedings{RefWorks:mahmood2007learning,
	author={Tariq Mahmood and Francesco Ricci},
	year={2007},
	title={Learning and adaptivity in interactive recommender systems},
	booktitle={Proceedings of the Ninth International Conference on Electronic Commerce},
	series={ICEC '07},
	publisher={Association for Computing Machinery},
	address={New York, NY, USA},
	location={Minneapolis, MN, USA},
	pages={75–84},
	abstract={Recommender systems are intelligent E-commerce applications that assist users in a decision-making process by offering personalized product recommendations during an interaction session. Quite recently, conversational approaches have been introduced in order to support more interactive recommendation sessions. Notwithstanding the increased interactivity offered by these approaches, the system employs an interaction strategy that is specified apriori (at design time) and followed quite rigidly during the interaction. In this paper, we present a new type of recommender system which is capable of learning autonomously an adaptive interaction strategy for assisting the users in acquiring their interaction goals. We view the recommendation process as a sequential decision problem and we model it as a Markov Decision Process (MDP). We learn a model of the user behavior, and use it to acquire the adaptive strategy using Reinforcement Learning (RL) techniques. In this context, the system learns the optimal strategy by observing the consequences of its actions on the users and also on the final outcome of the recommendation session. We apply our approach within an existing travel recommender system which uses a rigid, non-adaptive support strategy for advising a user in refining a query to a travel product catalogue. The initial results demonstrate the value of our approach and show that our system is able to improve the non-adaptive strategy in order to learn an optimal (adaptive) recommendation strategy.},
	keywords={adaptivity; conversational recommender systems; reinforcement learning; MarkovDecisionProcess},
	isbn={9781595937001},
	url={https://doi.org/10.1145/1282100.1282114},
	doi={10.1145/1282100.1282114}
}
@article{RefWorks:lu2016partially,
	author={Zhongqi Lu and Qiang Yang},
	year={2016},
	title={Partially Observable Markov Decision Process for Recommender Systems},
	journal={CoRR},
	volume={abs/1608.07793},
	note={1608.07793},
	keywords={MarkovDecisionProcess},
	url={http://arxiv.org/abs/1608.07793}
}
@article{RefWorks:wang2024deep,
	author={Xu Wang and Sen Wang and Xingxing Liang and Dawei Zhao and Jincai Huang and Xin Xu and Bin Dai and Qiguang Miao},
	year={2024},
	title={Deep Reinforcement Learning: A Survey},
	journal={IEEE Transactions on Neural Networks and Learning Systems},
	volume={35},
	number={4},
	pages={5064-5078},
	keywords={Task analysis; Mathematical models; Deep learning; Trajectory; Behavioral sciences; Q-learning; Dynamic programming; deep reinforcement learning (DRL); imitation learning; maximum entropy deep reinforcement learning (RL); policy gradient; value function; Survey},
	doi={10.1109/TNNLS.2022.3207346}
}
@inproceedings{RefWorks:zeng2016online,
	author={Chunqiu Zeng and Qing Wang and Shekoofeh Mokhtari and Tao Li},
	year={2016},
	title={Online Context-Aware Recommendation with Time Varying Multi-Armed Bandit},
	booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	series={KDD '16},
	publisher={Association for Computing Machinery},
	address={New York, NY, USA},
	location={San Francisco, California, USA},
	pages={2025–2034},
	abstract={Contextual multi-armed bandit problems have gained increasing popularity and attention in recent years due to their capability of leveraging contextual information to deliver online personalized recommendation services (e.g., online advertising and news article selection). To predict the reward of each arm given a particular context, existing relevant research studies for contextual multi-armed bandit problems often assume the existence of a fixed yet unknown reward mapping function. However, this assumption rarely holds in practice, since real-world problems often involve underlying processes that are dynamically evolving over time.In this paper, we study the time varying contextual multi-armed problem where the reward mapping function changes over time. In particular, we propose a dynamical context drift model based on particle learning. In the proposed model, the drift on the reward mapping function is explicitly modeled as a set of random walk particles, where good fitted particles are selected to learn the mapping dynamically. Taking advantage of the fully adaptive inference strategy of particle learning, our model is able to effectively capture the context change and learn the latent parameters. In addition, those learnt parameters can be naturally integrated into existing multi-arm selection strategies such as LinUCB and Thompson sampling. Empirical studies on two real-world applications, including online personalized advertising and news recommendation, demonstrate the effectiveness of our proposed approach. The experimental results also show that our algorithm can dynamically track the changing reward over time and consequently improve the click-through rate.},
	keywords={particle learning; personalization; probability matching; recommender system; time varying contextual bandit; ContextualMAB},
	isbn={9781450342322},
	url={https://doi.org/10.1145/2939672.2939878},
	doi={10.1145/2939672.2939878}
}
@inproceedings{RefWorks:li2010contextual-bandit,
	author={Lihong Li and Wei Chu and John Langford and Robert E. Schapire},
	year={2010},
	title={A contextual-bandit approach to personalized news article recommendation},
	booktitle={Proceedings of the 19th International Conference on World Wide Web},
	series={WWW '10},
	publisher={Association for Computing Machinery},
	address={New York, NY, USA},
	location={Raleigh, North Carolina, USA},
	pages={661–670},
	abstract={Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks.The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5\% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.},
	keywords={contextual bandit; exploration/exploitation dilemma; personalization; recommender systems; web service; ContextualMAB},
	isbn={9781605587998},
	url={https://doi.org/10.1145/1772690.1772758},
	doi={10.1145/1772690.1772758}
}
@inproceedings{RefWorks:bouneffouf2012contextual-bandit,
	author={Djallel Bouneffouf and Amel Bouzeghoub and Alda Lopes Gancarski},
	editor={Tingwen Huang and Zhigang Zeng and Chu Li and ong and Chi Sing Leung},
	year={2012},
	title={A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System},
	booktitle={Neural Information Processing},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={324-331},
	abstract={Most existing approaches in Mobile Context-Aware Recommender Systems focus on recommending relevant items to users taking into account contextual information, such as time, location, or social aspects. However, none of them has considered the problem of user's content evolution. We introduce in this paper an algorithm that tackles this dynamicity. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which user's situation is most relevant for exploration or exploitation. Within a deliberately designed offline simulation framework we conduct evaluations with real online event log data. The experimental results demonstrate that our algorithm outperforms surveyed algorithms."},
	keywords={ContextualMAB},
	isbn={978-3-642-34487-9}
}
@misc{RefWorks:shen2017deepctr:,
	author = 	 {Weichen Shen},
	year = 	 {2017},
	title = 	 {DeepCTR: Easy-to-use,Modular and Extendible package of deep-learning based CTR models},
	journal = 	 {GitHub Repository},
	keywords = 	 {Code},
	url = 	 {https://github.com/shenweichen/deepctr}
}
@misc{RefWorks:xiao2017attentional,
	author = 	 {Jun Xiao and Hao Ye and Xiangnan He and Hanwang Zhang and Fei Wu and Tat-Seng Chua},
	year = 	 {2017},
	title = 	 {Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks *},
	abstract = 	 {Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&amp;Deep [Cheng et al., 2016] and Deep-Cross [Shan et al., 2016] with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: https://github. com/hexiangnan/attentional factorization machine},
	keywords = 	 {AttentionOperator; FeatureOperator; Code},
	url = 	 {https://arxiv.org/abs/1708.04617}
}
@inproceedings{RefWorks:wang2023deeper,
	author={Fangye Wang and Hansu Gu and Dongsheng Li and Tun Lu and Peng Zhang and Ning Gu},
	year={2023},
	title={Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction},
	publisher={ACM},
	abstract={Click Through Rate (CTR) prediction plays an essential role in recommender systems and online advertising. It is crucial to effectively model feature interactions to improve the prediction performance of CTR models. However, existing methods face three significant challenges. First, while most methods can automatically capture high-order feature interactions, their performance tends to diminish as the order of feature interactions increases. Second, existing methods lack the ability to provide convincing interpretations of the prediction results, especially for high-order feature interactions, which limits the trustworthiness of their predictions. Third, many methods suffer from the presence of redundant parameters, particularly in the embedding layer. This paper proposes a novel method called Gated Deep Cross Network (GDCN) and a Field-level Dimension Optimization (FDO) approach to address these challenges. As the core structure of GDCN, Gated Cross Network (GCN) captures explicit high-order feature interactions and dynamically filters important interactions with an information gate in each order. Additionally, we use the FDO approach to learn condensed dimensions for each field based on their importance. Comprehensive experiments on five datasets demonstrate the effectiveness, superiority and interpretability of GDCN. Moreover, we verify the effectiveness of FDO in learning various dimensions and reducing model parameters. The code is available on https://github.com/anonctr/GDCN. CCS CONCEPTS • Information systems → Recommender systems.},
	keywords={FeatureOperator; ProductOperator; Code},
	doi={10.1145/3583780.3615089}
}
@inproceedings{RefWorks:qu2016product-based,
	author={Yanru Qu and Han Chai and Kan Ren and Weinan Zhang and Yong Yu and Ying Wen and Jun Wang},
	year={2016},
	title={Product-Based Neural Networks for User Response Prediction},
	booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)},
	publisher={IEEE},
	pages={1149-1154},
	note={ID: 1},
	keywords={FeatureOperator; ProductOperator},
	isbn={2374-8486},
	doi={10.1109/ICDM.2016.0151}
}
@inproceedings{RefWorks:rendle2010factorization,
	author={Steffen Rendle},
	year={2010},
	title={Factorization Machines},
	booktitle={2010 IEEE International Conference on Data Mining},
	pages={995-1000},
	note={ID: 1},
	keywords={ShallowMethods},
	isbn={1550-4786},
	doi={10.1109/ICDM.2010.127}
}
@inproceedings{RefWorks:juan2016field-aware,
	author={Yuchin Juan and Yong Zhuang and Wei-Sheng Chin and Chih-Jen Lin},
	year={2016},
	title={Field-aware Factorization Machines for CTR Prediction},
	booktitle={10th ACM Conference on Recommender Systems},
	series={RecSys '16},
	publisher={Association for Computing Machinery},
	address={New York, NY, USA},
	location={Boston, Massachusetts, USA},
	pages={43–50},
	abstract={Click-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, fieldaware factorization machines (FFMs), outperforms existing models in some worldwide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an effective method for classifying large sparse data including those from CTR prediction. First, we propose efficient implementations for training FFMs. Then we comprehensively analyze FFMs and compare this approach with competing models. Experiments show that FFMs are very useful for certain classification problems. Finally, we have released a package of FFMs for public use.},
	keywords={ShallowMethods},
	isbn={9781450340359},
	url={https://doi.org/10.1145/2959100.2959134},
	doi={10.1145/2959100.2959134}
}
@inproceedings{RefWorks:richardson2007predicting,
	author={Matthew Richardson and Ewa Dominowska and Robert Ragno},
	year={2007},
	title={Predicting Clicks: Estimating the Click-Through Rate for New Ads},
	booktitle={International Conference on World Wide Web},
	series={WWW '07},
	publisher={Association for Computing Machinery},
	address={New York, NY, USA},
	location={Banff, Alberta, Canada},
	pages={521–530},
	abstract={Search engine advertising has become a significant element of the Web browsing experience. Choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. This ranking has a strong impact on the revenue the search engine receives from the ads. Further, showing the user an ad that they prefer to click on improves user satisfaction. For these reasons, it is important to be able to accurately estimate the click-through rate of ads in the system. For ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. We show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads. We also show that using our model improves the convergence and performance of an advertising system. As a result, our model increases both revenue and user satisfaction.},
	keywords={ShallowMethods},
	url={https://doi.org/10.1145/1242572.1242643},
	doi={10.1145/1242572.1242643}
}
@misc{RefWorks:he2017neural,
	author = 	 {Xiangnan He and Tat-Seng Chua},
	year = 	 {2017},
	month = 	 {-08-16},
	title = 	 {Neural Factorization Machines for Sparse Predictive Analytics},
	journal = 	 {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	abstract = 	 {Many predictive tasks of web applications need to model categorical variables, such as user IDs and demographics like genders and occupations. To apply standard machine learning techniques, these categorical predictors are always converted to a set of binary features via one-hot encoding, making the resultant feature vector highly sparse. To learn from such sparse data e ectively, it is crucial to account for the interactions between features. Factorization Machines (FMs) are a popular solution for e ciently using the second-order feature interactions. However, FM models feature interactions in a linear way, which can be insu cient for capturing the non-linear and complex inherent structure of real-world data. While deep neural networks have recently been applied to learn non-linear feature interactions in industry, such as the Wide&amp;Deep by Google and DeepCross by Microso , the deep structure meanwhile makes them di cult to train. In this paper, we propose a novel model Neural Factorization Machine (NFM) for prediction under sparse se ings. NFM seamlessly combines the linearity of FM in modelling second-order feature interactions and the non-linearity of neural network in modelling higher-order feature interactions. Conceptually, NFM is more expressive than FM since FM can be seen as a special case of NFM without hidden layers. Empirical results on two regression tasks show that with one hidden layer only, NFM signi cantly outperforms FM with a 7.3% relative improvement. Compared to the recent deep learning methods Wide&amp;Deep and DeepCross, our NFM uses a shallower structure but o ers be er performance, being much easier to train and tune in practice.},
	doi={10.1145/3077136.3080777}
}
@techreport{RefWorks:emarketer2023digital,
	author={eMarketer},
	year={2023},
	title={Digital advertising spending worldwide from 2021 to 2027 (in billion U.S. dollars)},
	institution={Statista Inc.},
	url={https://www-statista-com.iclibezp1.cc.ic.ac.uk/statistics/237974/online-advertising-spending-worldwide/}
}
@misc{RefWorks:tien2014display,
	author = 	 {Jean-Baptiste Tien and joycenv and Olivier Chapelle},
	year = 	 {2014},
	title = 	 {Display Advertising Challenge},
	keywords = 	 {Criteo},
	url = 	 {https://kaggle.com/competitions/criteo-display-ad-challenge}
}
@misc{RefWorks:aden2012kdd,
	author = 	 {Yi Wang Aden},
	year = 	 {2012},
	title = 	 {KDD Cup 2012, Track 2},
	keywords = 	 {KDD12},
	url = 	 {https://kaggle.com/competitions/kddcup2012-track2}
}
@misc{RefWorks:wang2014click-through,
	author = 	 {Steve Wang and Will Cukierski},
	year = 	 {2014},
	title = 	 {Click-Through Rate Prediction},
	keywords = 	 {Avazu},
	url = 	 {https://kaggle.com/competitions/avazu-ctr-prediction}
}
@inproceedings{RefWorks:zheng2018drn:,
	author={Guanjie Zheng and Fuzheng Zhang and Zihan Zheng and Yang Xiang and Nicholas Jing Yuan and Xing Xie and Zhenhui Li},
	year={2018},
	title={DRN: A Deep Reinforcement Learning Framework for News Recommendation},
	booktitle={2018 World Wide Web Conference},
	publisher={International World Wide Web Conferences Steering Committee},
	address={Lyon, France},
	pages={167-176},
	abstract={In this paper, we propose a novel Deep Reinforcement Learning framework for news recommendation. Online personalized news recommendation is a highly challenging problem due to the dynamic nature of news features and user preferences. Although some online recommendation models have been proposed to address the dynamic nature of news recommendation, these methods have three major issues. First, they only try to model current reward (e.g., Click Through Rate). Second, very few studies consider to use user feedback other than click / no click labels (e.g., how frequent user returns) to help improve recommendation. Third, these methods tend to keep recommending similar news to users, which may cause users to get bored. Therefore, to address the aforementioned challenges, we propose a Deep Q-Learning based recommendation framework, which can model future reward explicitly. We further consider user return pattern as a supplement to click / no click label in order to capture more user feedback information. In addition, an effective exploration strategy is incorporated to find new attractive news for users. Extensive experiments are conducted on the offline dataset and online production environment of a commercial news recommendation application and have shown the superior performance of our methods.},
	url={https://doi.org/10.1145/3178876.3185994},
	doi={10.1145/3178876.3185994}
}
@misc{RefWorks:lyumemorize,
	author = 	 {Lyu and Tang and Guo and Tang and He and Zhang and Liu},
	title = 	 {Memorize, Factorize, or be Naïve: Learning Optimal Feature Interaction Methods for CTR Prediction},
	keywords = 	 {Code}
}
@misc{RefWorks:zhang2023memonet:,
	author = 	 {Pengtao Zhang and Junlin Zhang},
	year = 	 {2023},
	month = 	 {-10-21},
	title = 	 {MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction},
	journal = 	 {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
	abstract = 	 {New findings in natural language processing (NLP) demonstrate that the strong memorization capability contributes a lot to the success of Large Language Models (LLM). This inspires us to explicitly bring an independent memory mechanism into CTR ranking model to learn and memorize cross features&#39; representations. In this paper, we propose multi-Hash Codebook NETwork (HCNet) as the memory mechanism for efficiently learning and memorizing representations of cross features in CTR tasks. HCNet uses a multihash codebook as the main memory place and the whole memory procedure consists of three phases: multi-hash addressing, memory restoring, and feature shrinking. We also propose a new CTR model named MemoNet which combines HCNet with a DNN backbone. Extensive experimental results on three public datasets and online test show that MemoNet reaches superior performance over state-of-the-art approaches. Besides, MemoNet shows scaling law of large language model in NLP, which means we can enlarge the size of the codebook in HCNet to sustainably obtain performance gains. Our work demonstrates the importance and feasibility of learning and memorizing representations of cross features, which sheds light on a new promising research direction. The source code is in https://github.com/ptzhangAlg/RecAlg. CCS CONCEPTS • Information systems → Recommender systems.},
	keywords = 	 {Code},
	doi={10.1145/3583780.3614963}
}
@misc{RefWorks:song2019autoint,
	author = 	 {Song and Shi and Xiao and Duan and Xu and Zhang and Tang},
	year = 	 {2019},
	month = 	 {-11-03},
	title = 	 {AutoInt},
	journal = 	 {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	abstract = 	 {Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on highorder combinatorial features (a.k.a. cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding lowdimensional representations of the sparse and high-dimensional raw features and their meaningful combinations. In this paper, we propose an effective and efficient method called the AutoInt to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multihead self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the lowdimensional space. With different layers of the multi-head selfattentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability. Code is available at: https://github.com/DeepGraphLearning/RecommenderSystems.},
	keywords = 	 {AttentionOperator; Code; FeatureOperator},
	doi={10.1145/3357384.3357925}
}
@misc{RefWorks:li2020interpretable,
	author = 	 {Li and Cheng and Chen and Chen and Wang},
	year = 	 {2020},
	month = 	 {-01-20},
	title = 	 {Interpretable Click-Through Rate Prediction through Hierarchical Attention},
	journal = 	 {Proceedings of the 13th International Conference on Web Search and Data Mining},
	abstract = 	 {Click-through rate (CTR) prediction is a critical task in online advertising and marketing. For this problem, existing approaches, with shallow or deep architectures, have three major drawbacks. First, they typically lack persuasive rationales to explain the outcomes of the models. Unexplainable predictions and recommendations may be difficult to validate and thus unreliable and untrustworthy. In many applications, inappropriate suggestions may even bring severe consequences. Second, existing approaches have poor efficiency in analyzing high-order feature interactions. Third, the polysemy of feature interactions in different semantic subspaces is largely ignored. In this paper, we propose InterHAt that employs a Transformer with multi-head self-attention for feature learning. On top of that, hierarchical attention layers are utilized for predicting CTR while simultaneously providing interpretable insights of the prediction results. InterHAt captures high-order feature interactions by an efficient attentional aggregation strategy with low computational complexity. Extensive experiments on four public real datasets and one synthetic dataset demonstrate the effectiveness and efficiency of InterHAt. CCS CONCEPTS • Information systems → Recommender systems.},
	keywords = 	 {AttentionOperator; FeatureOperator},
	doi={10.1145/3336191.3371785}
}
@misc{RefWorks:liu2015convolutional,
	author = 	 {Liu and Yu and Wu and Wang},
	year = 	 {2015},
	month = 	 {-10-17},
	title = 	 {A Convolutional Click Prediction Model},
	journal = 	 {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
	abstract = 	 {The explosion in online advertisement urges to better estimate the click prediction of ads. For click prediction on single ad impression, we have access to pairwise relevance among elements in an impression, but not to global interaction among key features of elements. Moreover, the existing method on sequential click prediction treats propagation unchangeable for different time intervals. In this work, we propose a novel model, Convolutional Click Prediction Model (CCPM), based on convolution neural network. CCPM can extract local-global key features from an input instance with varied elements, which can be implemented for not only single ad impression but also sequential ad impression. Experiment results on two public large-scale datasets indicate that CCPM is effective on click prediction.},
	keywords = 	 {ConvolutionalOperator; FeatureOperator},
	doi={10.1145/2806416.2806603}
}

@inproceedings{RefWorks:wang2017deep,
author = {Wang, Ruoxi and Fu, Bin and Fu, Gang and Wang, Mingliang},
title = {Deep \& Cross Network for Ad Click Predictions},
year = {2017},
isbn = {9781450351942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3124749.3124754},
doi = {10.1145/3124749.3124754},
abstract = {Feature engineering has been the key to the success of many prediction models. However, the process is nontrivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep \& Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.},
booktitle = {Proceedings of the ADKDD'17},
articleno = {12},
numpages = {7},
keywords = {Neural Networks, Feature Crossing, Deep Learning, CTR Prediction},
location = {Halifax, NS, Canada},
series = {ADKDD'17}
}

@inproceedings{RefWorks:gu2021ad,
author="Gu, Liqiong",
editor="Jensen, Christian S.
and Lim, Ee-Peng
and Yang, De-Nian
and Chang, Chia-Hui
and Xu, Jianliang
and Peng, Wen-Chih
and Huang, Jen-Wei
and Shen, Chih-Ya",
title="Ad Click-Through Rate Prediction: A Survey",
booktitle="Database Systems for Advanced Applications. DASFAA 2021 International Workshops",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="140--153",
abstract="Ad click-through rate prediction (CTR), as an essential task of charging advertisers in the field of E-commerce, provides users with appropriate advertisements according to user interests to increase users' click-through rate based on user clicks. The performance of CTR models plays a crucial role in advertising. Recently, there are many approaches to improving the performance of CTR. In this paper, we present a survey to analyze state-of-art models of CTR via types of models comprehensively. Finally, we summarize some practical challenges and then open perspective problems of CTR.",
isbn="978-3-030-73216-5"
}
@misc{RefWorks:zhang2021deep,
	author = 	 {Weinan Zhang and Jiarui Qin and Wei Guo and Ruiming Tang and Xiuqiang He},
	year = 	 {2021},
	month = 	 {21 Apr},
	title = 	 {Deep Learning for Click-Through Rate Estimation},
	abstract = 	 {Click-through rate (CTR) estimation plays as a core function module in various personalized online services, including online advertising, recommender systems, and web search etc. From 2015, the success of deep learning started to benefit CTR estimation performance and now deep CTR models have been widely applied in many industrial platforms. In this survey, we provide a comprehensive review of deep learning models for CTR estimation tasks. First, we take a review of the transfer from shallow to deep CTR models and explain why going deep is a necessary trend of development. Second, we concentrate on explicit feature interaction learning modules of deep CTR models. Then, as an important perspective on large platforms with abundant user histories, deep behavior models are discussed. Moreover, the recently emerged automated methods for deep CTR architecture design are presented. Finally, we summarize the survey and discuss the future prospects of this field.},
	keywords = 	 {Survey},
	url = 	 {https://arxiv.org/abs/2104.10584}
}

@misc{RefWorks:zhang2016deep,
	author = 	 {Weinan Zhang and Tianming Du and Jun Wang},
	year = 	 {2016},
	title = 	 {Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction},
	note = 	 {1601.02376},
	url = 	 {https://arxiv.org/abs/1601.02376}
}

@inproceedings{RefWorks:cheng2016wide,
	author={Heng-Tze Cheng and Levent Koc and Jeremiah Harmsen and Tal Shaked and Tushar Chandra and Hrishi Aradhye and Glen Anderson and Greg Corrado and Wei Chai and Mustafa Ispir and Rohan Anil and Zakaria Haque and Lichan Hong and Vihan Jain and Xiaobing Liu and Hemal Shah},
	year={2016},
	title={Wide \& Deep Learning for Recommender Systems},
	booktitle={Proceedings of the 1st Workshop on Deep Learning for Recommender Systems},
	series={DLRS 2016},
	publisher={Association for Computing Machinery},
	address={New York, NY, USA},
	location={Boston, MA, USA},
	pages={7–10},
	abstract={Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide \& Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide \& Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.},
	keywords={Wide \& Deep Learning; Recommender Systems},
	isbn={9781450347952},
	url={https://doi.org/10.1145/2988450.2988454},
	doi={10.1145/2988450.2988454}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@inproceedings{RefWorks:auer2008near-optimal,
 author = {Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
 year = {2008},
 title = {Near-optimal Regret Bounds for Reinforcement Learning},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
}

@misc{RefWorks:pike-burke2024optimism/thompson,
	author = 	 {Ciara Pike-Burke},
	year = 	 {2024},
	title = 	 {Optimism/Thompson Sampling},
	journal = 	 {Learning Agents MLDS Course},
	pages = 	 {54–55}
}

@misc{pike-burke2024LearnigAgents,
	author = 	 {Ciara Pike-Burke},
	year = 	 {2024},
	title = 	 {Learning Agents MLDS Course},
	publisher = 	 {Imperial College London}
}

@phdthesis{RefWorks:watkins1989learning,
	author={Christopher John Cornish Hellaby Watkins},
	year={1989},
	title={Learning from delayed rewards}
}
@article{RefWorks:hornik1989multilayer,
	author={Kurt Hornik and Maxwell Stinchcombe and Halbert White},
	year={1989},
	title={Multilayer feedforward networks are universal approximators},
	journal={Neural Networks},
	volume={2},
	number={5},
	pages={359–366},
	note={ID: 271125},
	abstract={This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	keywords={Feedforward networks; Universal approximation; Mapping networks; Network representation capability; Stone-Weierstrass Theorem; Squashing functions; Sigma-Pi networks; Back-propagation networks},
	isbn={0893-6080},
	url={https://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi={10.1016/0893-6080(89)90020-8}
}

@article{RefWorks:hornik1990universal,
	author={Kurt Hornik and Maxwell Stinchcombe and Halbert White},
	year={1990},
	title={Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks},
	journal={Neural Networks},
	volume={3},
	number={5},
	pages={551–560},
	keywords={Derivatives; Mathematical analysis; Mathematics},
	isbn={0893-6080},
	doi={10.1016/0893-6080(90)90005-6}
}
@article{RefWorks:cybenko1989approximation,
	author={G. Cybenko},
	year={1989},
	title={Approximation by superpositions of a sigmoidal function},
	journal={Mathematics of control, signals, and systems},
	volume={2},
	number={4},
	pages={303–314},
	keywords={Mechanics; Structural analysis (Engineering)},
	isbn={0932-4194},
	doi={10.1007/BF02551274}
}

@article{RefWorks:mnih2015human-level,
	author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
	year={2015},
	month={February 26},
	title={Human-level control through deep reinforcement learning},
	journal={Nature},
	volume={518},
	number={7540},
	pages={529–533},
	note={LR: 20220408; JID: 0410462; CIN: Nature. 2015 Feb 26;518(7540):486-7. doi: 10.1038/518486a. PMID: 25719660; 2014/07/10 00:00 [received]; 2015/01/16 00:00 [accepted]; 2015/02/27 06:00 [entrez]; 2015/02/27 06:00 [pubmed]; 2015/04/16 06:00 [medline]; AID: nature14236 [pii]; ppublish},
	keywords={Algorithms; Artificial Intelligence; Humans; Models, Psychological; Neural Networks, Computer; Reinforcement, Psychology; Reward; Video Games},
	isbn={1476-4687},
	doi={10.1038/nature14236},
	pmid={25719670}
}

@inproceedings{RefWorks:chen2019top-k,
	author={Minmin Chen and Alex Beutel and Paul Covington and Sagar Jain and Francois Belletti and Ed H. Chi},
	year={2019},
	title={Top-K Off-Policy Correction for a REINFORCE Recommender System},
	booktitle={Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
	series={WSDM '19},
	publisher={Association for Computing Machinery},
	address={New York, NY, USA},
	location={Melbourne VIC, Australia},
	pages={456–464},
	abstract={Industrial recommender systems deal with extremely large action spaces -- many millions of items to recommend. Moreover, they need to serve billions of users, who are unique at any point in time, making a complex user state space. Luckily, huge quantities of logged implicit feedback (e.g., user clicks, dwell time) are available for learning. Learning from the logged feedback is however subject to biases caused by only observing feedback on recommendations selected by the previous versions of the recommender. In this work, we present a general recipe of addressing such biases in a production top-K recommender system at Youtube, built with a policy-gradient-based algorithm, i.e. REINFORCE. The contributions of the paper are: (1) scaling REINFORCE to a production recommender system with an action space on the orders of millions; (2) applying off-policy correction to address data biases in learning from logged feedback collected from multiple behavior policies; (3) proposing a novel top-K off-policy correction to account for our policy recommending multiple items at a time; (4) showcasing the value of exploration. We demonstrate the efficacy of our approaches through a series of simulations and multiple live experiments on Youtube.},
	keywords={top-k recommendation; set recommendation; reinforce; off-policy correction; exploration; counterfactual learning},
	isbn={9781450359405},
	url={https://doi.org/10.1145/3289600.3290999},
	doi={10.1145/3289600.3290999}
}

@article{RefWorks:hancock2020survey,
	author={John T. Hancock and Taghi M. Khoshgoftaar},
	year={2020},
	title={Survey on categorical data for neural networks},
	journal={Journal of Big Data},
	volume={7},
	number={1},
	pages={28},
	note={ID: Hancock2020},
	abstract={This survey investigates current techniques for representing qualitative data for use as input to neural networks. Techniques for using qualitative data in neural networks are well known. However, researchers continue to discover new variations or entirely new methods for working with categorical data in neural networks. Our primary contribution is to cover these representation techniques in a single work. Practitioners working with big data often have a need to encode categorical values in their datasets in order to leverage machine learning algorithms. Moreover, the size of data sets we consider as big data may cause one to reject some encoding techniques as impractical, due to their running time complexity. Neural networks take vectors of real numbers as inputs. One must use a technique to map qualitative values to numerical values before using them as input to a neural network. These techniques are known as embeddings, encodings, representations, or distributed representations. Another contribution this work makes is to provide references for the source code of various techniques, where we are able to verify the authenticity of the source code. We cover recent research in several domains where researchers use categorical data in neural networks. Some of these domains are natural language processing, fraud detection, and clinical document automation. This study provides a starting point for research in determining which techniques for preparing qualitative data for use with neural networks are best. It is our intention that the reader should use these implementations as a starting point to design experiments to evaluate various techniques for working with qualitative data in neural networks. The third contribution we make in this work is a new perspective on techniques for using categorical data in neural networks. We organize techniques for using categorical data in neural networks into three categories. We find three distinct patterns in techniques that identify a technique as determined, algorithmic, or automated. The fourth contribution we make is to identify several opportunities for future research. The form of the data that one uses as an input to a neural network is crucial for using neural networks effectively. This work is a tool for researchers to find the most effective technique for working with categorical data in neural networks, in big data settings. To the best of our knowledge this is the first in-depth look at techniques for working with categorical data in neural networks.},
	isbn={2196-1115},
	url={https://doi.org/10.1186/s40537-020-00305-w},
	doi={10.1186/s40537-020-00305-w}
}

@inproceedings{RefWorks:cheng2014gradient,
	author={Chen Cheng and Fen Xia and Tong Zhang and Irwin King and Michael R. Lyu},
	year={2014},
	title={Gradient boosting factorization machines},
	booktitle={Proceedings of the 8th ACM Conference on Recommender systems},
	pages={265–272}
}

@article{RefWorks:chang2010training,
	author={Yin-Wen Chang and Cho-Jui Hsieh and Kai-Wei Chang and Michael Ringgaard and Chih-Jen Lin},
	year={2010},
	title={Training and testing low-degree polynomial data mappings via linear SVM.},
	journal={Journal of Machine Learning Research},
	volume={11},
	number={4}
}

@inproceedings{RefWorks:he2016deep,
	author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year={2016},
	title={Deep residual learning for image recognition},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770–778}
}
@article{RefWorks:krizhevsky2017imagenet,
	author={Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
	year={2017},
	title={ImageNet classification with deep convolutional neural networks},
	journal={Communications of the ACM},
	volume={60},
	number={6},
	pages={84–90}
}
@article{RefWorks:lecun1998gradient-based,
	author={Yann LeCun and Léon Bottou and Yoshua Bengio and Patrick Haffner},
	year={1998},
	title={Gradient-based learning applied to document recognition},
	journal={Proceedings of the IEEE},
	volume={86},
	number={11},
	pages={2278–2324}
}

@misc{RefWorks:webster2024week,
	author = 	 {Kevin Webster},
	year = 	 {2024},
	title = 	 {Week 2: Multilayer Perceptron},
	journal = 	 {Deep Learning MLDS Course}
}

@inproceedings{RefWorks:liu2019feature,
	author={Bin Liu and Ruiming Tang and Yingzhi Chen and Jinkai Yu and Huifeng Guo and Yuzhou Zhang},
	year={2019},
	title={Feature generation by convolutional neural network for click-through rate prediction},
	booktitle={The World Wide Web Conference},
	pages={1119–1129}
}

@inproceedings{RefWorks:rendle2020neural,
	author={Steffen Rendle and Walid Krichene and Li Zhang and John Anderson},
	year={2020},
	title={Neural Collaborative Filtering vs. Matrix Factorization Revisited},
	booktitle={Proceedings of the 14th ACM Conference on Recommender Systems},
	series={RecSys '20},
	publisher={Association for Computing Machinery},
	address={New York, NY, USA},
	location={Virtual Event, Brazil},
	pages={240–248},
	abstract={Embedding based models have been the state of the art in collaborative filtering for over a decade. Traditionally, the dot product or higher order equivalents have been used to combine two or more embeddings, e.g., most notably in matrix factorization. In recent years, it was suggested to replace the dot product with a learned similarity e.g. using a multilayer perceptron (MLP). This approach is often referred to as neural collaborative filtering (NCF). In this work, we revisit the experiments of the NCF paper that popularized learned similarities using MLPs. First, we show that with a proper hyperparameter selection, a simple dot product substantially outperforms the proposed learned similarities. Second, while a MLP can in theory approximate any function, we show that it is non-trivial to learn a dot product with an MLP. Finally, we discuss practical issues that arise when applying MLP based similarities and show that MLPs are too costly to use for item recommendation in production environments while dot products allow to apply very efficient retrieval algorithms. We conclude that MLPs should be used with care as embedding combiner and that dot products might be a better default choice.},
	keywords={Neural Collaborative Filtering; Matrix Factorization; Item Recommendation},
	isbn={9781450375832},
	url={https://doi.org/10.1145/3383313.3412488},
	doi={10.1145/3383313.3412488}
}

@article{RefWorks:qu2018product-based,
	author={Yanru Qu and Bohui Fang and Weinan Zhang and Ruiming Tang and Minzhe Niu and Huifeng Guo and Yong Yu and Xiuqiang He},
	year={2018},
	month={oct},
	title={Product-Based Neural Networks for User Response Prediction over Multi-Field Categorical Data},
	journal={ACM Trans.Inf.Syst.},
	volume={37},
	number={1},
	abstract={User response prediction is a crucial component for personalized information retrieval and filtering scenarios, such as recommender system and web search. The data in user response prediction is mostly in a multi-field categorical format and transformed into sparse representations via one-hot encoding. Due to the sparsity problems in representation and optimization, most research focuses on feature engineering and shallow modeling. Recently, deep neural networks have attracted research attention on such a problem for their high capacity and end-to-end training scheme. In this article, we study user response prediction in the scenario of click prediction. We first analyze a coupled gradient issue in latent vector-based models and propose kernel product to learn field-aware feature interactions. Then, we discuss an insensitive gradient issue in DNN-based models and propose Product-based Neural Network, which adopts a feature extractor to explore feature interactions. Generalizing the kernel product to a net-in-net architecture, we further propose Product-network in Network (PIN), which can generalize previous models. Extensive experiments on four industrial datasets and one contest dataset demonstrate that our models consistently outperform eight baselines on both area under curve and log loss. Besides, PIN makes great click-through rate improvement (relatively 34.67\%) in online A/B test.},
	keywords={recommender system; product-based neural network; Deep learning},
	isbn={1046-8188},
	url={https://doi.org/10.1145/3233770},
	doi={10.1145/3233770}
}

@inproceedings{RefWorks:shalev-shwartz2017failures,
	author={Shai Shalev-Shwartz and Ohad Shamir and Shaked Shammah},
	editor={Doina Precup and Yee Whye Teh},
	year={2017},
	month={aug},
	title={Failures of Gradient-Based Deep Learning},
	booktitle={Proceedings of the 34th International Conference on Machine Learning},
	series={Proceedings of Machine Learning Research},
	publisher={PMLR},
	volume={70},
	pages={3067–3075},
	abstract={In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradient-based algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.},
	url={https://proceedings.mlr.press/v70/shalev-shwartz17a.html}
}

@article{RefWorks:weissman2003inequalities,
	author={Tsachy Weissman and Erik Ordentlich and Gadiel Seroussi and Sergio Verdu and Marcelo J. Weinberger},
	year={2003},
	title={Inequalities for the L1 deviation of the empirical distribution},
	journal={Hewlett-Packard Labs, Tech.Rep},
	pages={125}
}

@article{RefWorks:watkins1992q-learning,
	author={Christopher JCH Watkins and Peter Dayan},
	year={1992},
	title={Q-learning},
	journal={Machine Learning},
	volume={8},
	pages={279–292}
}

@inproceedings{RefWorks:jin2020provably,
	author={Chi Jin and Zhuoran Yang and Zhaoran Wang and Michael I. Jordan},
	editor={Jacob Abernethy and Shivani Agarwal},
	year={2020},
	month={09--12 Jul},
	title={Provably efficient reinforcement learning with linear function approximation},
	booktitle={Proceedings of Thirty Third Conference on Learning Theory},
	series={Proceedings of Machine Learning Research},
	publisher={PMLR},
	volume={125},
	pages={2137–2143},
	abstract={Modern Reinforcement Learning (RL) is commonly applied to practical problems with an enormous number of states, where \emph{function approximation} must be deployed to approximate either the value function or the policy. The introduction of function approximation raises a fundamental set of challenges involving computational and statistical efficiency, especially given the need to manage the exploration/exploitation tradeoff. As a result, a core RL question remains open: how can we design provably efficient RL algorithms that incorporate function approximation? This question persists even in a basic setting with linear dynamics and linear rewards, for which only linear function approximation is needed. This paper presents the first provable RL algorithm with both polynomial runtime and polynomial sample complexity in this linear setting, without requiring a “simulator” or additional assumptions. Concretely, we prove that an optimistic modification of Least-Squares Value Iteration (LSVI)—a classical algorithm frequently studied in the linear setting—achieves $\tilde{\mathcal{O}}(\sqrt{d^3H^3T})$ regret, where $d$ is the ambient dimension of feature space, $H$ is the length of each episode, and $T$ is the total number of steps. Importantly, such regret is independent of the number of states and actions.},
	url={https://proceedings.mlr.press/v125/jin20a.html}
}

@misc{RefWorks:2024papers,
	author={{Meta AI Research}},
	year={2024},
	title={Papers with Code},
	volume={2024},
	number={9 June},
	url={https://paperswithcode.com/}
}

@misc{AWSDataWrangler,
	author={{Amazon Web Services}},
	title={Amazon Sagemaker Data Wrangler Tool},
	year={2024},
	url={https://aws.amazon.com/sagemaker/data-wrangler/}
}

@inproceedings{RefWorks:graepel2010web-scale,
	author={Thore Graepel and Joaquin Quinonero Candela and Thomas Borchert and Ralf Herbrich},
	year={2010},
	title={Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft's bing search engine},
	publisher={Omnipress}
}

@article{Kingma2014AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6980},
  url={https://api.semanticscholar.org/CorpusID:6628106}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@misc{chollet2023keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished={\url{https://github.com/keras-team/keras}},
  commit={68f9af408a1734704746f7e6fa9cfede0d6879d8}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@software{Batek_Deep_Reinforcement_Leaning_2024,
author = {Bat\v{e}k, Martin},
doi = {10.5281/zenodo.1234},
month = aug,
title = {{Deep Reinforcement Leaning for Ad Personalization}},
url = {https://github.com/martinbatek/drl-ad-personalization},
version = {1.0.0},
year = {2024}
}

@article{RefWorks:srivastava2014dropout:,
	author={Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	year={2014},
	title={Dropout: a simple way to prevent neural networks from overfitting},
	journal={The journal of machine learning research},
	volume={15},
	number={1},
	pages={1929–1958}
}

@article{RefWorks:ioffe2015batch,
	author={Sergey Ioffe},
	year={2015},
	title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
	journal={arXiv preprint arXiv:1502.03167}
}

@book{ibrahim2001bayesian,
  title={Bayesian survival analysis},
  author={Ibrahim, Joseph G and Chen, Ming-Hui and Sinha, Debajyoti and Ibrahim, JG and Chen, MH},
  volume={2},
  year={2001},
  publisher={Springer}
}

@inproceedings{jing2017neural,
  title={Neural survival recommender},
  author={Jing, How and Smola, Alexander J},
  booktitle={Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
  pages={515--524},
  year={2017}
}

@article{RefWorks:van2016deep,
	author={Hado van Hasselt and Arthur Guez and David Silver},
	year={2016},
	month={March},
	title={Deep Reinforcement Learning with Double Q-Learning},
	journal={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={30},
	number={1},
	url={https://ojs.aaai.org/index.php/AAAI/article/view/10295},
	doi={10.1609/aaai.v30i1.10295}
}

@misc{RefWorks:cxl“good”,
	author={CXL},
	title={What is a “Good” Click-Through Rate? Click-Through Rate Benchmarks},
	volume={2024},
	number={30 August},
	url={https://cxl.com/guides/click-through-rate/benchmarks/}
}