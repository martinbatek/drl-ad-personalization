\documentclass{mldsmsc}

\title{Deep Reinforcement Learning for Ad Personalization}
\author{Martin Bat\v{e}k}
\CID{00951537}
\supervisor{Mikko Pakkanen}
\date{2 September 2024}
%For today's date, use:
%\date{\today}
\logoimg{}


% THIS IS WHERE NEW COMMANDS CAN BE DEFINED
% commands below only used in the proof; otherwise can be deleted
\newcommand{\consta}{a}
\newcommand{\X}{X}
\newcommand{\EE}[1]{ \mathrm{E} [ #1 ] }
\newcommand{\inparenth}[1]{\left( #1 \right)}

\begin{document}

% Generates the Title Page
\maketitle


% Generates plagiarism declaration
\declarationname{Martin Bat\v{e}k}
\declarationdate{1 September 2024}
\declaration 


\begin{abstract}
    With the long-term
    trend of consumers increasingly turning to online channels for their daily activities, 
    sellers of a range of goods and services are increasingly
    turning to digital advertising marketplaces such as Google, Amazon and Facebook as a means to reach new customers. This demonstrates the
    commercial value of constructing an Ad marketplace that is able to autonomously adapt to
    the preferences of its users, maximize the Click-Through Rate of the advertizing content
    shown, and thereby maximize user engagement with the platforms over the longer term.

    The aim of this report is to demonstrate the value of incorporating Deep Learning and
    Reinforcement Learning techniques in online advertising. We conduct a broad and comprehensive
    Comparitive Model Analysis that compares the predictive performance of a wide range of Deep
    Learning models designed for Click Through Rate prediction, and explore the hyperparameter
    settings for the best candidate, the Deep \& Cross Network. We then use this model to propose
    a novel Deep Reinforcement Learning for Ad Personalization (DRL-AP) algorithm. By constructing
    an offline simulation of an Ad serving environment, we demonstrate that the DRL-AP algorthm
    shows promising results as framework for adaptive Click-Through Rate maximization.
\end{abstract}

\begin{acknowledgements}
    I would like to thank my supervisor, Dr Mikko Pakkanen, for providing invaluable support
    and advice throughout the duration of this project. I would also like to thank Dr Kevin
    Webster and Dr Ciara Pike-Burke for inspiring my interest in the fields of Deep Learning
    and Learning Agents, and for their patience during my various lines of questioning on the 
    subjects during the respective MLDS modules. 
    Finally, I would like to thank my wife, Juliette, for her support over the last two years.
\end{acknowledgements}

% add glossary?

% table of contents
\tableofcontents

% VERY IMPORTANT
% This command switches from Roman to Arabic numbering for main part of thesis
\mainmatter


\chapter{Introduction}

The global digital advertising market is worth approximately \$602 billion today. Due to the increasing rate of of online participation since the 
COVID-19 pandemic, this number has been rapidly increasing and is expected to reach \$871 billion by the end of 2027 \citep{RefWorks:emarketer2023digital}.
Many of the of the major Ad platforms such as Google, Facebook and Amazon operate on a cost-per-user-engagement pricing model, which usually means that 
advertisers get charged for every time a user clicks on an advertisment. This means that there is
a significant commercial incentive to design Ad-serving platforms that ensure that the content 
shown to each user is as relevent as possible, so as to maximize user engagement and platform revenues.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../figures/eMarketer - Ad Spending.png}
\caption{Global Digital Ad Spending 2021-2027. Image taken from \cite{RefWorks:emarketer2023digital}}
\label{fig:ad-spending}
\end{figure}

Predicting the Click-Through Rate (CTR) 
for a given Ad placement is a necessary first step for Ad persionalization. Consequentially, the study of CTR prediction methods have been an extremely active part of 
Machine Learning research. Methods such as Logistic Regression, Factorization Machines \citep{RefWorks:rendle2010factorization} and Field-Aware Factorization 
Machines \citep{RefWorks:juan2016field-aware} were initially considered to be best-in-class methods for CTR prediction. However, due to their relative simplicity, these methods are
often unable to learn from
higher order feature interactions, which often contain the key patterns for CTR prediction in the sparse multi-value categorical Ad Marketplace datasets \citep{RefWorks:zhang2021deep}. Deep Learning methods have been 
shown to show superior predictive ability in this domain, since their multi-layered structure alows for the learning of higher-order interactions
more easily. A number of Deep Learning models have been proposed using a
range of techniques for feature interaction modelling, including from Deep Learning extensions of Factorization Machines
such as DeepFM \citep{RefWorks:guo2017deepfm:}, to novel methods such as AutoInt \citep{RefWorks:song2019autoint}.

However, irrespective of how well these models perform in a static environments, the reality is that user preferences
and advertisment characteristics are constantly changing. Like most online reccomender systems,
Ad personalization models must be able to adapt to these changes in order to continue to provide accurate predictions 
over the longer period \citep{RefWorks:zheng2018drn:}. This problem necessitates the use
of Reinforcement Learning for Ad personalization.
Reinforcement Learning is a subdomain of Machine Learning in which the goal is for an agent to
learn an optimal policy that maximizes the expected reward in an environment where the
state-action-reward progression can be modelled as a Markov Decision Process
\citep{puterman2014markov}. Early Reinforcement Learning methods involved 
deriving the transition probabilities for the state-action pairs on the basis of interactions
with the environment. However, in cases where the state-action space is too sparse to be
reasonably enumerated, it is often more practical 
to directly estimate the expected cumalative reward for each action in each state. This method
is commonly referred to as Q-learning \citep{RefWorks:watkins1989learning}. In \citep{RefWorks:hornik1989multilayer},
\citep{RefWorks:cybenko1989approximation} and \citep{RefWorks:hornik1990universal} Deep
Neural Networks are shown to be universal function approximators
which naturally lead to the incorporation of DNN's in Q-Learning. This has lead to the
development of the Deep Q-Learning Network, which has been shown to be able to learn optimal
policies in a number of different domains, such as the Atari 2600 game environment \citep{RefWorks:mnih2015human-level}.
DRL has also be applied to online recommender systems
such as News article reccomendation \citep{RefWorks:zheng2018drn:} and video
reccomendation on Youtube \citep{RefWorks:chen2019top-k}. In both papers,
the authors show that the DRL agent is able to learn an optimal content recommendation
policy on the basis of user engagement data. This reveals that there is potential
for applying these methods to the problem of Ad personlization, thereby creating a 
truely adaptive marketing platform.

\subsubsection{Research Question and Contributions}

In this report, we aim to construct a Ad serving system that is truely adaptive and 
personalized to the changing user preferences and advertisment charateristics. In order
to achieve this goal, we will first need to find a suitable Deep Learning Model arcitecture
for CTR prediction and then incorporate this model as the Q-function approximator in
a Deep Q-Learning algorithm. The key contributions that we make in this report are as follows:

\begin{enumerate}
    \item \label{exp:comparative-model-analysis} A \textbf{Comparitive Model Analysis}, whereby the predictive performance of a range of different CTR models
    is measured using equivalent hyperparameter settings and the same benchmark CTR datasets.
    \item[] \textbf{Research Question:} \emph{Which of the CTR prediction models in scope have the highest predicted performance
    when measured using equivalent hyperparameter settings and using the same benchmark CTR prediction datasets?}
    \item \label{exp:hyperparameter-analysis} A \textbf{Hyperparameter Setting Analysis}, whereby we take the highest performing model from the previous
    experiment above, and calculate and compare that model's predictive performance for different parameter settings.
    \item[] \textbf{Research Question:} \emph{What are the ideal hyperparameter values for the given CTR prediction model?}
    \item \label{exp:rl-sim} A \textbf{Deep Reinforcement Learning for Ad Personalization} algorithm that demonstrates the
    proof-of-concept for using Deep Reinforcement Learning in the Ad domain. 
    \begin{enumerate}
        \item \label{exp:dcn-model-advantage} \textbf{Research Question:} \emph{Does the use of a Deep Learning model provide an
        advantage in terms of Reward (Click) maximization and Regret Minimization over using no model?}
        \item \label{exp:drl-ap-advantage} \textbf{Research Question:} \emph{Does the DRL-AP algorithm provide a significant
        advantage in terms of Reward (Click) maximization and Regret Minimization over using a Deep Learning model
        with no updates?}
    \end{enumerate}
\end{enumerate}

\subsubsection{Structure of the Report}

In chapter~\ref{chap:background-deepctr}, we begin by introducing the problem setting for
of Click-Through Rate prediction, and explore the unique challenges posed 
by the typically sparse multi-value categorical datasets that are common in the Ad marketplace. We then 
proceed to review the literature on Deep Learning models for CTR prediction, highlighting
the different techniques that each framework uses to capture the key feature interactions in the data. 
In chapter~\ref{chap:background-drl} we review the literature on Deep Reinforcement
Learning, specifically the DRN algorithm introduced by \cite{RefWorks:zheng2018drn:}, which can be analogously
applied to the Ad personalization context. In chapter \ref{chap:deep-ctr-model-evaluation}, I evaluate the performance of different
Deep Learning models for CTR prediction on three well-known benchmark datasets, Criteo \citep{RefWorks:tien2014display}, KDD12 \citep{RefWorks:aden2012kdd} 
and Avazu \citep{RefWorks:wang2014click-through}. Finally, in chapter~\ref{chap:deep-rl-for-ad-personalization}, we introduce the Deep Reinforcement Learning model for Ad personalization algorithm and evaluate its performance
using the KDD12 dataset.

\chapter{Background: Deep CTR Prediction}
\label{chap:background-deepctr}

\section{Problem Formulation and Ad Marketplace Data}
\label{sec:problem-formulation-data}

In their respective surveys on the use of Deep Learning methods for CTR prediction, \cite{RefWorks:gu2021ad} 
and \cite{RefWorks:zhang2021deep} outline the problem of CTR prediction as one that essentially boils down to
a binary (click/no-click) classification problem utilizing user/ad-view event level online session records. 
The goal of CTR prediction is to train a function $f$ that takes in a set of ad marketplace 
features $\mathbf{x} \in \mathbb{R}^n$, and maps these to a probability that the user 
will click on the ad in that given context. In other words, $f_{\Theta}: \mathbb{R}^n \rightarrow \mathbb{R}$ such that:

\begin{equation}
\label{eqn:ctr-classifier}
\mathbb{P}(\text{click}| \mathbf{x})
= \mathbb{P}(y = 1 | \mathbf{x})
= \sigma(f_{\Theta}(\mathbf{x}))
= \hat{y}
\end{equation}
where $y$ is the binary click label, $\hat{y}$ is the predicted likelihood that $y=1$, $\Theta$ represents the parameter vector for $f$ and
$\sigma(x)=(1 + e^{-x})^{-1}$ is the sigmoid function. 
An instance of the ad marketplace features $\mathbf{x}$ is typically
recorded at an user/ad impression event level and typically consists of

\begin{itemize}
\item \textbf{User Features:} Features that describe the user, such as User ID, demographic information,
metrics related to the user's past interactions with the platform, etc.
\item \textbf{Ad Features:} Features that describe the ad, such as Ad ID, Advertiser ID and Ad Category.
\item \textbf{Contextual Features:} Features that describe the context in which the ad is being shown, such as
the time of day, the position of the ad on the page and the site on which the ad is being shown.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../figures/kdd12_snapshot.png}
\caption{Snapshot of the KDD12 dataset \citep{RefWorks:aden2012kdd}}
\label{fig:kdd12-snapshot}
\end{figure}

A defining characteristic for this type of data is that many of the features in $\mathbf{x}$ are multi-value categories with 
a high degree of of cardinality \citep{RefWorks:he2017neural}. In order to use categorical data
in a classifier model, it is necessary to first convert the categorical values into suitable real valued vector representations.
A common mothod for doing so is to encode these categorical features $x_i$
as \emph{one-hot} vectors $\mathbf{x}_i^{OH} = \left(p_1, \ldots, p_{C_i} \right)$, where

\[
p_k =
\begin{cases}
    1 & \text{ if the value of } x_i \text{ is equal to the } k \text{-th possible category}\\
    0 & \text{ otherwise}
\end{cases}
\]
The \emph{sparse vector representation} $\tilde{\mathbf{x}}$ of feature vector $\mathbf{x}$
is then given by the concatination of one-hot vector representations of the categorical features $\{x_i\}_{i=1}^{s}$,
and any real-valued features $\{x_i\}_{s+1}^{s+d}$:

\begin{equation}
    \label{eqn:sparse-vector}
    \tilde{\mathbf{x}} = \left( \mathbf{x}_1^{OH}, \ldots, \mathbf{x}_s^{OH}, x_{s+1}, \ldots, x_{s+d}\right) \in \mathbb{R}^{\tilde{n}}
\end{equation}
where $s$ and $d$ are the number of (\textbf{s}parse) categorical and (\textbf{d}ense) numerical
features in $\mathbf{x}$, and $\tilde{n} = \sum_{i=1}^{s}C_i + d$ is the size of $\tilde{\mathbf{x}}$.

The problem posed by high cardinality is that when $C_i$ is large, most of the values
in the sparse feature vector representation in equation~\ref{eqn:sparse-vector}
will be equal to zero for an overwhelming majority of observations. This can make it extremely difficult
for a model to learn the key \emph{implicit} features and patterns present in sparse
data \citep{RefWorks:gu2021ad}. An elementary method for alleviating this issue is to project
the one-hot vector representations $\{ \mathbf{x}_i^{OH}\}_{i=1}^{s}$ to some lower dimension
$D < C_i \text{ } \forall i \in [1, \ldots, s]$. The \emph{embedded} feature vector
$\mathbf{e}_i$ for categorical feature $x_i$ is given by:

\begin{equation}
\label{eqn:cat-embedding}
\begin{split}
\mathbf{e}_i &= \mathbf{B}_i \mathbf{x}_i^{OH}\\
&= \left[\mathbf{b}_{1}^{i}, \ldots, \mathbf{b}_{m_i}^{i} \right] \mathbf{x}_i^{OH} \\
&= \begin{bmatrix}
b_{1,1}^i & \cdots & b_{1 ,C_i}^i\\
\vdots & \ddots & \\
b_{D, 1}^i & \cdots & b_{D, C_i}^i
\end{bmatrix}
\begin{bmatrix}
    0 \\
    \vdots \\
    1 \\
    0\\
    \vdots
\end{bmatrix}
\end{split}
\end{equation}
where $\mathbf{B}_i \in \mathbb{R}^{D \times C_i}$ is the embedding matrix for feature $x_i$, whose dimensions are determined
by the chosen embedding dimension $D$ and the cardinality of the feature, $C_i$. For numerical features
in $\mathbf{x}$ the cardinality is $C_i = 1$, so the embedding operation in equation~\ref{eqn:cat-embedding}
simply reduces to a scalar multiplication of feature value $x_i$ and embedding vector $\mathbf{b}_i \in \mathbb{R}^{D}$.
In Deep Learning, the embedding matrix $\mathbf{B}_i$ is commonly derived as a trainable parameter in the gradient descent
algorithm during model fitting \citep{RefWorks:hancock2020survey}. The embedded feature representation
$\dot{\mathbf{x}}$ that then gets fed into the model is 
then composed of a concatenation of all sparse feature embeddigs $\mathbf{e}_i$ dense 
numerical feature values:

\begin{equation*}
    \dot{\mathbf{x}} = \left[ \mathbf{e}_1, \dots , \mathbf{e}_n\right]
    \in \mathbb{R}^{\dot{n}}
\end{equation*}
where $\dot{n} = D\cdot n$
is the resulting dimensionality of $\dot{\mathbf{x}}$. Unfortunately, it has been shown that
even when using embeddings, constructing a model that learns the key informative patterns
in highly sparse data still proves to be an extremely difficult task \citep{RefWorks:gu2021ad,RefWorks:zhang2021deep}.
This is indeed the key challenge behind building an
accurate CTR prediction model, and is a key motivating factor as to why Deep Neural
networks have out performed the classical shallow counterparts. This transition will be examined
in more detail in sections \ref{sec:shallow-models} to \ref{sec:feature-operator-models}.

\section{Shallow CTR Models}
\label{sec:shallow-models}

\subsection{Logistic Regression}

The earliest examples of CTR classification models leveraged classical ``shallow'' 
(single layer) statistical regression methods. The most basic example of this was 
the \textbf{Logistic Regression} model, as implemented by \cite{RefWorks:richardson2007predicting} on
advertisement data from the Microsoft Search engine. The LR model
is composed by modelling the \emph{logit}
of a positive binary label as a linear combination
of all of the respective feature values:

\begin{equation}
\label{eqn:lr-model}
f_{\Theta}^{LR}(\tilde{\mathbf{x}}) = \theta_0 + \sum_{j=1}^{\tilde{n}}\theta_j \tilde{x}_j
\end{equation}
where $f_{\Theta}^{LR}: \mathbb{R}^n \rightarrow \mathbb{R}$ represents the Logistic regression
model parametized by $\Theta = (\theta_0, \ldots, \theta_{\tilde{n}})$. The simplicity and 
and the low number of parameters of the LR model mean it is relatively easy to
train and deploy in production \citep{RefWorks:zhang2021deep}.
Howevever, the formulation in equation~\ref{eqn:lr-model} reveals that the LR model does not explicitly 
account for \emph{feature interactions}. Many of the of the important patterns
for CTR prediction are likely to be expressed in terms of \emph{combinations of features}
rather then the individual feature values themselves \citep{RefWorks:zhang2021deep}. For example, a user's tendancy to
click on a given advertisment is likely to by influenced by the \emph{combination} of the category
of good or service the given advertisment is trying to sell (e.g. premium fashion retail, travel, electronics et. cetera)
and the demographic/socio-economic category that the given user falls into (e.g. university student, young professional, retiree).
These features combinations are commonly referred to in
the literature as \emph{cross-features} \citep{RefWorks:zhang2023memonet:} or more commonly
\emph{feature interactions} \citep{RefWorks:cheng2016wide,RefWorks:xiao2017attentional,RefWorks:song2019autoint}.

\subsection{Factorization Machines}

\textbf{Factorization Machines} first proposed by \cite{RefWorks:rendle2010factorization} can 
be thought of as an extension of the Logistic Regression framework in equation~\ref{eqn:lr-model}
with additional terms that explicitly account for the interactions between different features.
Its relative simplicity and computational scalability has made it a widely popular framework
for CTR modelling \citep{RefWorks:gu2021ad}. 
A 2-way (maximum feature interaction degree of 2) Factorization Machine model is formulated as:

\begin{equation}
\label{eqn:fm-2way}
f_{\Theta}^{FM^2}(\tilde{\mathbf{x}}) = \theta_0 + \sum_{j=1}^{\tilde{n}} \theta_{j} \tilde{x}_j
+ \sum_{j=1}^{\tilde{n}} \sum_{k=j+1}^{\tilde{n}} \langle \mathbf{v}_j , \mathbf{v}_k \rangle \tilde{x}_j \tilde{x}_k
\end{equation}
where $\langle \cdot , \cdot \rangle$ represents the inner product between two vectors, the final
interaction term above is parametized by $\mathbf{V} \in \mathbb{R}^{\tilde{n} \times F}$. Each
row $\mathbf{v}_j$ of $\mathbf{V}$ represents the $j$-th feature in $\tilde{\mathbf{x}}$ in terms
of $F$ latent factors. The factorization matrix $\mathbf{V}$ is typically fitted by optimizing
the binary cross-entropy loss function by means of Stochastic Gradient Descent.

By Lemma~\ref{lemma:fm-linearity} in the appendix, \cite{RefWorks:rendle2010factorization} shows that
the 2-way FM model in fact scales linearly in $\tilde{n}$ and $F$. This quality greatly simplifies the computational complexity of scaling the FM model to larger
datasets with a more sparse categorical features. Morever, the Factorization Machine framework
can be generalized to degree $R$ (i.e. up to any limit of feature interation order) as follows:

\begin{multline}
\label{eqn:fm-rway}
f_{\Theta}^{FM^R} = \theta_0 + \sum_{j=1}^{\tilde{n}} \theta_{j} \tilde{x}_{j}
+ \sum_{r=1}^{R} \sum_{j_1=1}^{\tilde{n}} \cdots \sum_{j_r = j_{r-1} + 1}^{\tilde{n}}
\left( \prod_{k=1}^{r} \tilde{x}_{j_k} \right)
\left( \sum_{f = 1}^{F_r} \prod_{k=1}^{r} v_{j_k, f}^{(r)}\right)
\end{multline}

The FM framework therefore provides an intuitive and computationally scalable method to
account for key feature interactions without the need of extensive feature engineering. Extensions
and improvements to FM have been proposed, most notably in the form of
the Field-Aware Factorization Machine (FFM) framework by \cite{RefWorks:juan2016field-aware},
which only acounts for intereactions between features of different fields 
(in otherwords, it ignores the interaction between $\tilde{x}_j$ and $\tilde{x}_k$ if
both are componets of embedding vector $mathbf{e}_i$ for some categorical feature $x_i$)
as well as Gradient Boosted Factorization Machines \citep{RefWorks:cheng2014gradient}, 
which again aims to augment the FM framework by means of the Gradient Boosting algorithm.

\section{Introducing MLPs in CTR prediction}

Despite the advantages of the FM framework, a setback of the formulation in equation~\ref{eqn:fm-rway}
is that the framework grows highly compex and overparametized for higher values of $R$.
As a consequence, only the 2-way FM framework as per equation~\ref{eqn:fm-2way} tends to be
implemented in practice, meaning that the FM model alone is practically insufficient for capturing
feature interactions of order $>>2$ \citep{RefWorks:guo2017deepfm:}. Deep Neural Networks
present a powerful alternative for addressing this shortcoming. Neural networks benefit from
being universal function approximators \citep{RefWorks:cybenko1989approximation} and from the
fact that neural network batch training is paralellizable by means of GPU accelerated computation.
This has lead to the successfull application of Deep Learning algorithms across multiple fields
such as Natural Language Processing and Image classification 
\citep{RefWorks:he2016deep,RefWorks:krizhevsky2017imagenet,RefWorks:lecun1998gradient-based}. 
These factors and successes showed that DNN's have the potential to extract informative
feature representations from highly sparse and abstract data, and as a consequence,
the application of DNN's in CTR prediction started recieving attention in the mid-2010's.

The \textbf{Multilayer Perceptron} (MLP) \label{ref:mlp} is the most elementary type of Deep Neural Network
\citep{RefWorks:webster2024week}. In general, a MLP with $L$ hidden layers is formulated as such:

\begin{align}
\label{eqn:mlp}
\mathbf{h}^{(0)} &:= \tilde{\mathbf{x}} \\
\mathbf{h}^{(l)} &= \phi_{l} \left( \mathbf{W}^{(l-1)} + \mathbf{b}^{(l-1)} \right), l = 1, \ldots, L \\
\hat{y} &= \phi_{out} \left( \mathbf{w}^{(L)} \mathbf{h}^{(L)} + b^{(L)} \right)
\end{align}

where $\mathbf{W}^{(k)} \in \mathbb{R}^{n_{l+1} \times n_l } $, $\mathbf{b}^{(k)} \in \mathbb{R}^{n_{l+1}}$,
$\mathbf{h}^{(l)} \in \mathbb{R}^{n_l}$, $n_0 = \tilde{n}$, $n_l$ is the number of hidden
units in layer $l$ and $\phi_{l}$ is the activation function for layer $l$.
When rearranged in the form of equation~\ref{eqn:ctr-classifier}, the above becomes:

\begin{equation}
\label{eqn:mlp-2}
f_{\Theta}^{MLP}(\tilde{\mathbf{x}}) = \psi_{out} \left( \psi_{L} \left( \cdots \psi_{1} \left( 
    \tilde{\mathbf{x}} \right) \cdots \right) \right)
\end{equation}
where each function $\psi_{l}$ represents the affine transformation and element-wise activation
operation for layer $l$. MLPs can be thought of as an acyclic graph, as displayed in
Figure~\ref{fig:mlp}. The data $\tilde{\mathbf{x}}$ first gets fed through the
\emph{input layer}, then gets processed by multiple \emph{hidden layers} that include
a series of affine transfromations followed by activation functions, before the final
result is produced by the \emph{output layer}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/ann_two_hidden_layers.png}
    \caption{Multilayer Perceptron with two hidden layer. Taken from \citep{RefWorks:webster2024week}}
    \label{fig:mlp}
\end{figure}

Figure~\ref{fig:mlp} demonstrates the potential that DNN's have for modelling higher order feature
interactions. By training the network by means of Stochastic Gradient Descent, it should be
possible to calculate the appropriate weight ($\mathbf{W}_l$) and bias ($\mathbf{b}_l$) parameters
in order capture the relevant high-order feature patterns in the data. As such, many CTR
modelling frameworks have been developed that use Deep Learning techniques to build upon and
improve the previously discussed classical methods by incorporating Deep Neural
Networks in the model architecture \citep{RefWorks:zhang2021deep}.

\section{Single vs Dual Tower Architectures}

Before moving on to DNN enhanced CTR models in section~\ref{sec:mlp-enhanced-models}
it is worth briefly discussing the difference between \textbf{Single-Tower}
models and \textbf{Dual-Tower} models. Single Towel models place
all layers successively in the architecture, and can generally be formulated
as in equation~\ref{eqn:mlp-2}. Since all feature inputs are passed through
the same set of successive affine transformations and activations, Single Tower
models are usually able to capture higher order feature interactions, but the signal
from the low-order interactions tend to be lost \citep{RefWorks:zhang2021deep}.
The FNN \citep{RefWorks:zhang2016deep}, FGCNN
\citep{RefWorks:liu2019feature} and PNN \citep{RefWorks:qu2016product-based} models covered in the subsections 
\ref{sec:mlp-enhanced-models} and \ref{sec:feature-operator-models}
are examples of Single Tower models.

In order to avoid diluting the signal from the lower order feature interactions, many architectures
adopt a Dual Tower architecture, as shown on the right-hand side of
Figure~\ref{fig:single-dual-models}. A separate \textbf{Feature Interaction Layer}
is placed parallely to the DNN, and the final output is composed of a weighted sum
of the feature interaction layer and DNN outputs. With this architecture, the feature interaction
layer is usually dedicated to capturing the important lower order feature interaction signals,
whereas the DNN acts as a \emph{residual network} for capturing any other meaningful signals. 
As a result, Dual Tower networks tend to benefit form better training stability and
better performance \citep{RefWorks:zhang2021deep}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/single_dual_dnn.png}
    \caption{Single vs Dual Architecture Networks. Source: \citep{RefWorks:zhang2021deep}}
    \label{fig:single-dual-models}
\end{figure}

\section{DNN Enhanced CTR models}
\label{sec:mlp-enhanced-models}

\subsection{Factorization-machine Supported Neural Networks}

One of the earliest examples of the use of Deep Neural Networks being used to enhance existing
CTR modeling methods is the \textbf{Factorization-machine Supported Neural Network} (FNN)
\citep{RefWorks:zhang2016deep}. The FNN model is a Single Tower model that works by pretraining
a 2-way Factorization Machine model as in equation~\ref{eqn:fm-2way} on the concatenated
one-hot encoded categorical feature vectors, and then using the feature interaction vectors and
weights as the embedding matrix.

Below we start with the feature vector with one-hot encoded categorical feature representations, and then \emph{prefit}
a 2-way Factorization Machine model as in section~\ref{sec:shallow-models}:
\begin{align*}
    \tilde{\mathbf{x}} &= \left[\mathbf{x}_1^{OH}, \ldots, \mathbf{x}_s^{OH},  x_{s+1}, \ldots, x_{s+d}\right]\\
    f_{\Theta}^{FM^2}(\tilde{\mathbf{x}} ) &= \theta_0 + \sum_{j=1}^{\tilde{n}} \theta_j \tilde{x}_j +
    \sum_{j=1}^{\tilde{n}} \sum_{k=j+1}^{\tilde{n}}
    \langle \mathbf{v}_j, \mathbf{v}_k \rangle \tilde{x}_j \tilde{x}_k
\end{align*}

We then use the weights and biases
from $\Theta = (\theta_0, \theta_1, \ldots , \theta_{\tilde{n}}, \mathbf{v}_1, \ldots, \mathbf{v}_{\tilde{n}})$ to
derive the feature embedding as follows:

\begin{equation*}
\tilde{\mathbf{x}} = \left[\theta_0, \mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_{s+d}\right]
\end{equation*}

where each embedding vector $\mathbf{e}_i$ is defined as the concatination of
the weight ($\theta_j$) and FM interaction vector ($\mathbf{v}_j$), where $j$ is chosen
to be the index for $\tilde{\mathbf{x}}$ that corresponds to the non-zero value in one-hot field
$i$ in $\tilde{\mathbf{x}}$:
\begin{equation*}
    \mathbf{e}_i = \left[\theta_j, \mathbf{v}_j\right], \text{ such that }start_i \leq j < end_i \text{ and } \tilde{x}_j =1 
\end{equation*}

The above can alternatively also be recovered from equation~\ref{eqn:cat-embedding} by
defining a \emph{field-wise} embedding matrix $\mathbf{B}_i$ to a $(D+1) \times C_i$ matrix with the following
values

\begin{equation*}
\mathbf{B}_i = \begin{bmatrix}
    b_{1,1}^i & \cdots & b_{1 ,C_i}^i\\
    \vdots & \ddots & \\
    b_{D+1, 1}^i & \cdots & b_{D+1, C_i}^i
    \end{bmatrix}
= \begin{bmatrix}
    \theta_{1}^{i} & \cdots & \theta_{C_i}^{i}\\
    v_{1,1}^i & \cdots & v_{1,C_i}^i \\
    \vdots & \ddots & \\
    v_{D,1}^{i
    }& \cdots & v_{D,C_i}^{i}
    \end{bmatrix}
\end{equation*}

where $\theta_{c}^i$ represents the feature weight for the $c$-th feature in field $i$ in $\tilde{\mathbf{x}}$,
and $\mathbf{v}_{c}^i = \left(v_{1,c}^i, \ldots, v_{D,c}^i\right)$ is the FM interaction vector for the
$c$-th feature in field $i$ in $\tilde{\mathbf{x}}$. Then, if we define $\tilde{\mathbf{x}}[start_i : end_i]
= x_i^{OH}$, then $\mathbf{e}_i$ can be defined as:

\begin{equation*}
    \mathbf{e}_i = \left(\mathbf{B}_i \tilde{\mathbf{x}}[start_i : end_i]^{\intercal}\right)^{\intercal}
    = \left[\theta_j, \mathbf{v}_j\right] 
\end{equation*}



\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/fnn.png}
    \caption{FNN model architecture. Source: \citep{RefWorks:zhang2016deep}}
    \label{fig:fnn}
\end{figure}
Figure~\ref{fig:fnn} portrays the structure of this model as it was presented in the original
paper by \cite{RefWorks:zhang2016deep}.
By incorporating the FM model feature interaction vectors in the embedding layer
before the DNN, the FNN model is able to leverage the FM model's strength in
interaction identification. The FNN model then leverages a MLP network
to capture the higher order interaction signals much more efficiently than would
have been feasibly possible when relying ownly on FM. Because of this, comprehensive
experiments carried out by \cite{RefWorks:zhang2016deep} confirmed that FNN
has superior CTR estimation performance than both LR and FM.

However \cite{RefWorks:guo2017deepfm:} and \cite{RefWorks:zhang2021deep} find that
due to its Single Tower architecture, lower-order feature interaction signals tend
to be lost in the network. \cite{RefWorks:guo2017deepfm:} further found that
the FM pretraining step described above represents a significant overhead in
terms of training efficiency. In the next subsections, we will see how the Wide \& Deep
and DeepFM models aim to solve for these issues.

\subsection{Wide and Deep}

The \textbf{Wide and Deep Learning} (WDL) model was developed by \cite{RefWorks:cheng2016wide}.

\begin{equation*}
    %\label{eqn:wdl}
    f_{\Theta}^{W\&D} = \theta_0 + \sum_{k=1}^{\hat{n}} \theta_k \hat{\mathbf{x}}_k
    + f_{\Phi}^{MLP}(\tilde{\mathbf{x}})
\end{equation*}

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{../figures/wdl.png}
\caption{Wide and Deep Learning Model, as illustrated in \citep{RefWorks:shen2017deepctr:}}
\label{fig:wdl}
\end{figure}
Figure~\ref{fig:wdl} reveals that the W\&D model is composed with a Dual-Tower Architecture,
with a Deep Component and a Wide Component (shown on the left and right hand sides of Figure~\ref{fig:wdl}
respectively). The Deep Component is composed of a MLP with multiple hidden layers, each
with the Rectified Linear Unit (ReLU) activation function. The Wide Component is formulated
by the first two terms in equation~\ref{eqn:wdl}, and is composed of a simple linear transformation
of the input features. The key aspect of the Wide Component is the fact that the 
linear transformation is not simply applied to the encoded features $\tilde{x}$,
but instead to these concatinated with a set of cross-product transformed features.

\begin{equation}
\label{eqn:wdl-cross-product}
\hat{\mathbf{x}} = [\tilde{\mathbf{x}}, \upsilon_1({\tilde{\mathbf{x}}}),\ldots , \upsilon_P({\tilde{\mathbf{x}}})]
\end{equation}
where $\upsilon_k({\tilde{\mathbf{x}}}) = \prod_{j=1}^{\tilde{n}}\tilde{x}_j^{c_{kj}}$
and $c_{kj} \in \{ 0, 1 \}$. Consequentially of equation~\ref{eqn:wdl-cross-product}, the Wide Component \emph{memorizes}
the key feature interations that are defined by the specific \emph{cross-product transformations}
($\upsilon_{k}(\tilde{\mathbf{x}})$) \citep{RefWorks:cheng2016wide}. Meanwhile, the Deep Component captures any residual
signals that may not have been explicitly included in the manually defined cross-product transformations
\citep{RefWorks:zhang2021deep}. In this sense, the W\&D model overcomes the
shortcomings of the FNN model, by having dedicated pathways in the archtecture
for higher and lower order feature interactions. However, the downside
of W\&D is the fact that the feature interactions in the Wide component
need to be manually incorporated by defining the cross-product transformation
functions $\{\upsilon_k(x)\}_{k=1}^{P}$. This means that there is a significant
feature engineering component that would be necessary to use this model effectively. 

\subsection{DeepFM}

The \textbf{DeepFM} model was developed by \cite{RefWorks:guo2017deepfm:} in order
to address the shortcomings of the FNN and W\&D models mentioned in this section, as 
well as those of the PNN model which will be covered in section~\ref{sec:feature-operator-models}.
Similarly to the WDL model, the DeepFM network two components arranged as a Dual Tower
architecture, as visualized in Figure~\ref{fig:deepfm}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/dfm.png}
    \caption{DeepFM network architecture. Source: \citep{RefWorks:shen2017deepctr:}}
    \label{fig:deepfm}
\end{figure}
The \emph{FM component} effectively replaces the Wide component in the WDL model. It consists
of a 2-way Factorization Machine layer that models pairwise feature interactions between the
different fields of $\tilde{\mathbf{x}}$ as inner products of the respective feature latent
vectors $\mathbf{v}_j$ \citep{RefWorks:guo2017deepfm:}. Due to the linear scalability of
the FM discussed in section~\ref{sec:shallow-models}, the FM component can effectively
capture important order-2 feature interactions automatically, without the need to manually
define cross-product functions as in the case of the W\&D model.

Meanwhile, the \emph{Deep component} fulfills a similar purpose as in the case of the WDL
model. The deep component is a MLP network that takes the feature embedding vector $\tilde{\mathbf{x}}$
as inputs, and learns the higher-order feature interactions that cannot by captured by the
2-way FM component. Like with the WDL model, the Deep component acts as a residual network
for capturing signals that we ommitted by the FM component. The DeepFM model can therefore
be formulated as in equation~\ref{eqn:deepfm}.

\begin{equation}\label{eqn:deepfm}
    f_{\Theta}^{DeepFM} = f_{\Phi}^{FM^2}(\tilde{\mathbf{x}}) + f_{\Omega}^{MLP}(\tilde{\mathbf{x}})
\end{equation}

A notable difference
in the Deep components between the WDL and DeepFM models are in the construction of the
embedding layers that preprocess the input to the MLP. In DeepFM, the latent feature vectors
($\mathbf{V}$) are trainable network weights that are derived during SGD optimization in the FM
component. For every successive step in the model training, the learned latent feature vectors
are used in the embedding layer that preprocesses that raw input features before the MLP of 
the Deep component (see Figure~\ref{fig:deepfm-embedding}). This is similar to how the feature embeddings were derived in the FNN model
\citep{RefWorks:zhang2016deep}, except for fact the FM layer is included in the overall learning
architecture of the model. This elimitates the need to pretrain the DeepFM model, thereby allowing
for the FM latent feature vectors to by learned concurrently during the overall DeepFM model
training procedure \citep{RefWorks:guo2017deepfm:}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/deepfm_embedding.png}
    \caption{The embedding layer of the Deep Component in the DeepFM model. Source: \citep{RefWorks:guo2017deepfm:}}
    \label{fig:deepfm-embedding}
\end{figure}

The key benefits of the DeepFM model are threefold. Firstly, we have already mentioned above
that the fact that the FM model is incorporated directly into the model architecture eliminates
the need to pretrain the FM latent feature vectors, thereby eliminating the computational overhead
that is neccessary for this in the case of FNN \citep{RefWorks:zhang2016deep}. Secondly,
the Dual Tower architecture allows the model to simultaneously learn both low as well as high
order feature interactions. Thirdly, the previously discussed computational efficiency of the
FM model means that DeepFM is relatively scalable in terms of the number of features and the size of
the latent embedding space, especially in comparison to the Product based Neural Network
(PNN) models \citep{RefWorks:qu2016product-based}, which will be covered in section \ref{sec:feature-operator-models}.

\section{Feature Interaction Operator Models}
\label{sec:feature-operator-models}

MLPs have been proven to be universal function approximators, meaning that any function can be
can be sufficiently approximated with a larger enough MLP \citep{RefWorks:hornik1989multilayer,RefWorks:cybenko1989approximation,RefWorks:hornik1990universal}.
However, \cite{RefWorks:shalev-shwartz2017failures} found that for complex problems where the true target function
is actually a larger set of uncorrelated solution functions, Deep Neural Networks suffer from
an \emph{insensitive gradient issue} during gradient descent optimization. \cite{RefWorks:shalev-shwartz2017failures}
show that when the target function is a set of uncorrelated functions, the variance of the gradient
with respect to the target decreases linearly with respect to the number of functions that make up the target.
The decrease in this variance has the effect of decreasing the correlation between the gratient
and the target, causing the optimization of the DNN to fail. \cite{RefWorks:qu2018product-based}
argues that since the target function for the CTR classification task typically consists of a
set of uncorralated if-then classifiers on the basis sparse categorical features, the insensitive
gradient issue is likely to be prevalent in cases where MLPs are relied upon directly
to detect the key feature interactions in sparse CTR classification data.

The above justifies the design of specific layers and architectures that explicitly detect important
feature interactions in the data. In this section, we discuss \textbf{Feature Interaction Operators},
which are deep learning layers that were developed specifically to assist the DNN in its capacity
to learn higher feature interactions \citep{RefWorks:zhang2021deep}. The three different type of Feature Interaction
Operators that are discussed in this section are Product Operators, Convolutional Operators
and Attention Operators.

\subsection{Product Operators models}

\subsubsection{Product-based Neural Network}

\textbf{Product Operator} networks are neural networks that include layers with inner or outer
product operations in order to explicitly model feature interactions \citep{RefWorks:zhang2021deep}.
The \textbf{Product-based Neural Network} (PNN) introduced the concept of product operator models
as it includes a product layer between the embedding layer and the MLP in order to model second
order feature interactions in the data. All of this is arranged as a Single Tower architecture, as shown
in Figure~\ref{fig:pnn}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/pnn.png}
    \caption{Product-based Neural Network. Source: \citep{RefWorks:qu2016product-based}.}
    \label{fig:pnn}
\end{figure}

The key defining component of the PNN is the \emph{Product Layer}. Each embedding field $\mathbf{e}_i$
in the preceding embedded layer is pair-wisely connected to each of the other fields and a ``1''
constant signal. The output of the product signal can then be broken out into two parts:

\begin{itemize}
    \item Virtue to the constant ``1'' signal, the first part is simply a vector $\mathbf{z}$ that consists of a concatenation
    of all of the field embeddings. In other words $\mathbf{z} = \dot{\mathbf{x}}$.
    \item A second-order interaction vector, $\mathbf{p} = \{p_{i,j}\}\text{ where } i,j = 1, \ldots, n$, where each element
    $p_{i,j} = g(\mathbf{e}_i, \mathbf{e}_j)$ defines a pairwise field interaction.
\end{itemize}

In their initial paper, \cite{RefWorks:qu2016product-based} proposed two variants of the PNN model, the Inner Product Based Neural Network
and the Outer Product Based Neural Network, differentiated by whether $g$ is the Inner or Outer
product operation respectively. The $\mathbf{z}$ and $\mathbf{p}$ vectors are then both
projected to $\mathbb{R}^{D^1}$ space (the input dimension of the MLP network) by means
of trainable weight matrices, $W_z$ and $W_p$. The input to the MLP network is then the
sum of the two resulting vectors and a bias vector $\mathbf{b}_1$. The formulation
for the PNN network is summarized in terms of $\dot{\mathbf{x}}$ in equation~\ref{eqn:pnn}.

\begin{equation}\label{eqn:pnn}
    f_{\Theta}^{PNN}(\dot{\mathbf{x}}) = f_{\Phi}^{MLP}(W_z \dot{\mathbf{x}} + W_p \mathbf{p} + \mathbf{b}_1)
\end{equation}

The inclusion of the product layer in the PNN model automatically incorporates second order field-wise
interactions as inputs to the MLP by means of inner or outer products, thereby partially alleviating
the previously mentioned insensitive gradient issue. \cite{RefWorks:qu2016product-based} found that
as a result of this, the PNN models outperformed LR, FM, FNN and the CCPM models in terms of Log Loss
and AUC. However, a major disadvantage of the product layer operations is the computational time complexity,
which increases quadratically with the number of fields and the embedding dimension. In order to alleviate
this, \cite{RefWorks:qu2016product-based} implement simplified versions of the inner and outer product
computations (in which some neurons are eliminated in the inner product, and the result for the outer product
is compressed for all fields at once), but even then the PNN models are still tend to be less computationally efficient
than its peers. Furthermore, since the PNN model leverages a Single Tower architecture it suffers from the
same issue as the FNN model wherein the lower order interactions are ignored.

\subsubsection{Neural Factorization Machine}

The \textbf{Neural Factorization Machine} (NFM) model introduced by \cite{RefWorks:he2017neural} works
on a similar principle to PNN, in that it also has a product layer in
its architecture. The NFM model calculates the pre-sigmoid prediction as
in equation~\ref{eqn:nfm}:

\begin{equation}
    \label{eqn:nfm}
    f_{\Theta}^{NFM}(\tilde{\mathbf{x}}) = w_0 + \sum_{j=1}^{\tilde{n}} \tilde{x}_j
    + f_{\Phi}^{NFM-Deep}(\tilde{\mathbf{x}})
\end{equation}

Recall that here, $\tilde{\mathbf{x}}$ is a sparse vector composed of concatenated one-hot
categorical feature fields, as well as the numerical features in $\mathbf{x}$.
The first two terms in equation~\ref{eqn:nfm} are simply composed of a
bias parameter $w_0$, and a linear combination of the feature values
in $\tilde{\mathbf{x}} = \left(\tilde{x}_1, \ldots, \tilde{x}_{\tilde{n}}\right)$.
The last term in equation~\ref{eqn:nfm} represents the core component of the NFM
model, and includes an architecture similar to that of the PNN model with
an embedding layer, a unique type of product layer called a \emph{Bi-Interaction Pooling Layer}
followed by a multi-layer MLP network. The architecture for $f_{\Phi}^{NFM-Deep}$
is visualized in Figure~\ref{fig:nfm}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/nfm.png}
    \caption{Neural Factorization Machines model excluding the first-order linear regression part. Source:\cite{RefWorks:he2017neural}}
    \label{fig:nfm}
\end{figure}

The \emph{Embedding Layer} in Figure~\ref{fig:nfm} is a fully connected layer that carries
out the embedding operation described in equation~\ref{eqn:cat-embedding}. The \emph{Bi-Interaction Pooling}
layer then takes the embedded feature vector $\dot{\mathbf{x}}$ produced by the embedding layer as
input and combines all of the respective feature embeddings $\{\mathbf{e}_i\}_{i=1}^{n}$ into a single feature
vector by the following operation:

\begin{equation}\label{eqn:bi-interaction-pooling}
    f^{BI}(\{\mathbf{e}_i\}_{i=1}^{n})\sum_{i=1}^{n} = \sum_{j=i+1}^{n} \mathbf{e}_i \odot \mathbf{e}_j
\end{equation}
where $\odot$ represents the element-wise product of two vectors. The benefit of the Bi-Interaction
Pooling layer is its efficiency. It does not introduce any additional model parameters and can
be calculated in linear time. Equation~\ref{eqn:bi-interaction-pooling} can be reformulated as:

\begin{equation*}
    f^{BI}(\{\mathbf{e}_i\}_{i=1}^{n})\sum_{i=1}^{n} 
    =\frac{1}{2} \left[ (\sum_{i=1}^{n} \mathbf{e}_i)^2 - \sum_{i=1}^{n} (\mathbf{e}_i)^2 \right]\
\end{equation*}

When considering the feature cardinalities $C_i$ and the embedding dimension $D$, the above calculation
can be performed in $O(n)$ time \cite{RefWorks:he2017neural}. The NFM model therefore benefits from better
training efficiency and stability in comparison to most other Deep Learning methods, while simaltaneously
being able to capture both feature interactions through the $f_{\Phi}^{NFM-Deep}$ as well as
key feature signals through the linear components of equation~\ref{eqn:nfm} \citep{RefWorks:zhang2021deep}.

\subsubsection{Deep \& Cross Network}

Neither the PNN nor the NFM models guaruntee that each order of feature interaction
will be modelled. The \textbf{Deep \& Cross Network} (DCN) devoloped by \cite{RefWorks:wang2017deep}
applies feature crossing at each layer and efficiently learns predictive cross features to a hyperparametized
bounded degree without requiring additional feature engineering. The DCN model is has a Dual-Tower architecture
comprised of a Cross Network and a Deep Network, as shown in Figure~\ref{fig:dcn}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/dcn.png}
    \caption{Deep \& Cross Network model. Source: \citep{RefWorks:wang2017deep}}
    \label{fig:dcn}
\end{figure}

The Cross Network and Deep Networks are both preceeded by an Embedding and Stacking layer, which produces
a vector containing concatinated categorical feature embeddings $\mathbf{e}_i$ and normalized numberical
feature embeddings $x_i$. The Deep Network consists of a standard Multilayer Perceptron component with
ReLu activation. The Cross Network is composed of $L$ cross layers, where the output of each
cross layer $\mathbf{x}_{l+1}$ is calculated as:

\begin{equation}\label{eqn:cross-layer}
    \mathbf{x}_{l+1} = \mathbf{x}_0 \mathbf{x}_l^{\intercal} \mathbf{w}_l + \mathbf{b}_l + \mathbf{x}_l
\end{equation}

where $\mathbf{x}_0$ is the output of the Embedding and Stacking layer and $\mathbf{w}_l$, $\mathbf{b}_l$
are trainable weight and bias parameter vectors for each cross layer. \cite{RefWorks:wang2017deep}
show linking crossing operations formulated by equation~\ref{eqn:cross-layer}, the output of of a
$L$-layer Cross Network comprises explicitly of cross-feature terms of degrees 1 to $L+1$.

\subsection{Convolutional Operator Models}

\subsubsection{Convolutional Click Prediction Model}

\textbf{Convolutional Operator Models} use the convolution operation to extract key local-global
features from the categorical field embeddings. The earliest and most well known example of a
Convolutional Operator model is the \textbf{Convolutional Click Prediction Model} (CCPM)
developed by \cite{RefWorks:liu2015convolutional}. The overall architecture of this model
was shown in the original paper as in Figure~\ref{fig:ccpm}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/ccpm}
    \caption{Convolution Click Prediction Model architecture. Source: \citep{RefWorks:liu2015convolutional}.}
    \label{fig:ccpm}
\end{figure}

The input to the CCPM model consists of a $s \times D$ dimensional matrix of stacked categorical
feature empeddigs, as shown on the left-hand side of Figure~\ref{fig:ccpm}. Per the standard
architecture discussed in \citep{RefWorks:liu2015convolutional}, this matrix is then passed through
a series of Convolutional and Pooling layers. The number
of maximum features that the intermediate pooling layers take is flexible to account for the
flexible input matrix length. The final prediction is calculated by passing the final
pooling result through a MLP, shown on the right-hand side of Figure~\ref{fig:ccpm} in green.
This makes the CCPM model a Single Tower architecture model that can be roughly summarized
as per equation~\ref{eqn:ccpm}.

\begin{equation}\label{eqn:ccpm}
    f_{\Theta}^{CCPM}(\tilde{\mathbf{x}}) = f_{\Phi}^{MLP}(f_{\Omega}^{Conv}(\tilde{\mathbf{x}}))
\end{equation}
where $f_{\Omega}^{Conv}$ represents the series of convolutions and pooling layers described above.
\cite{RefWorks:liu2015convolutional} show that the CCPM model outperforms the LR, FM and RNN models
in terms of Log Loss/Binary Cross-Entropy. However, a common criticism of the CCPM model is that
due to the equivariance property of the Convolution operations, the degree to which it is able to capture
important feature interactions in the data is highly dependant on how the features are ordered in the
input matrix \citep{RefWorks:zhang2021deep,RefWorks:qu2018product-based,RefWorks:gu2021ad}. Convolutions
by nature extract feature maps in the local neighbourhood of each variable, but fail to do so globally.

\subsubsection{Feature Generation by Convolutional Neural Network}

The \textbf{Feature Generation by Convolutional Neural Network} model proposed by \cite{RefWorks:liu2019feature}
introduces a \emph{feature recombination layer} to mitigate the feature location issue prevalent with CCPM.
The architecture for the FGCNN model is shown in Figure~\ref{fig:fgcnn}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/fgcnn.png}
    \caption{FGCNN model architecture with Feature Generation Component. Source: \citep{RefWorks:liu2019feature}}
    \label{fig:fgcnn}
\end{figure}
The main body of the FGCNN model consists Deep Classifier that is essentially an Inner Product
Neural Network, which was discussed above \citep{RefWorks:qu2018product-based}. The inputs
to the Deep Classifier consist of the input feature embeddings ($\tilde{\mathbf{x}}$), concatenated
with a set of new features that are created in the Feature Generation component of the model. This
Feature Generation component represents the primary innovation of the FGCNN model, and is visualized
at the top of Figure~\ref{fig:fgcnn}. As with CCPM, the Feature Generation component is composed of
a searies of two dimensional convolutional and max pooling layers, and takes the input feature
embedding matrix as an input. However, in order to solve for the input order dependancy issue
prevailent with CCPM, the resulting feature maps are first passed through a fully connected \emph{recombination layer}
that models non-adjacent interactions.

\subsection{Attention Operator Models}

\textbf{Attention Operator Models} aim to utilize the attention mechanism for identifying
the key feature interactions in the data. The \textbf{Autotomatic Feature Interaction Learning} (AutoInt) 
model proposed by \cite{RefWorks:song2019autoint} makes use of a multi-head self attention
network to model the important feature interactions in the data. The initial 
paper separates the model into three parts: an embedding layer, an interaction layer 
and an output layer. The embedding layer aims to project each sparse multi-value
categorical a and dense numerical feature into a lower dimensional space, as per the below:

$$
\mathbf{e_i} = \frac{1}{q} \mathbf{V_i x_i}
$$

where $\mathbf{V_i}$ is the embedding matrix for the $i$-th field, $x_i$ is a multi-hot vector, and $q$ 
is the number of non-zero values in $x_i$. The interaction layer employs the multi-head
mechanism to determine which higher order feature interaction are meaningful in the data. This not only
improves the efficiency of model traning, but it also improves the model's explainability. Lastly,
the output layer is a fully connected layer that takes in the concatinated output 
of the interaction layer, and applies the sigmoid activation function to produce the final prediction.
The architecture of the AutoInt model is shown in Figure~\ref{fig:autoint}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/autoint.png}
    \caption{The Automatic Feature Interaction Learning model architecture. Source: \citep{RefWorks:song2019autoint}}
    \label{fig:autoint}
\end{figure}


\chapter{Background: Deep Reinforcement Learning}
\label{chap:background-drl}

The second part of the background material in this report is dedicated to Deep Reinforcement Learning. We first
proceed by explaining the foundational concepts, in which we will establish definitions for
Markov Decision Processes, Reinforcement Learning and Dynamic Programming. We then move on to explain
Q-Learning, a specific class of Reinforcement Learning algorithms, as well as how Deep Learning
models are being applied in the case of Deep Q-Learning. Finally, we introduce the Deep Reinforcement
Learning News reccomendation (DRN) algorithm \citep{RefWorks:zheng2018drn:}, a Q-learning algorithm
which we have repurposed for ad recommendation.

\section{RL Basics: MDPs and Bellman Optimality Equations}

In the case of many online systems and applications where there is a series of interactions between
users and the system, it is often desirable to find the optimal set of content to display to the
users in order to maximize their engagement as time goes on. This problem can be framed as a
\textbf{Markov Decision Process}. Definition~\ref{def:mdp} was taken from \citep{pike-burke2024LearnigAgents}
have been chosen for evaluation:

\begin{definition}\label{def:mdp}
    An episodic \textbf{Markov decision process} (MDP) is defined by tuple $\mathcal{M}
    (\mathcal{S},\mathcal{A}, H,\nu, \{P_{h}\}_{h=1}^{H},\{r_{h}\}_{h=1}^{H})$ where:
    \begin{itemize}
        \item $\mathcal{S}$ is the state space of finite cardinality.
        \item $\mathcal{A}$ is the finite set of actions.
        \item $H \in \mathbb{N}$ is the horizon of the problem.
        \item $\nu$ is the initial state distribution.
        \item $\{P_{h}\}_{h=1}^{H}$ is the collection of transition functions where
        $P_h: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$
        where $\Delta(\mathcal{S})$ is the set of probability distributions over
        $\mathcal{S}$. When action $a \in \mathcal{A}$ is taken from state $s \in \mathcal{S}$
        at stage $h$, $P_h(s^\prime | s, a)$ gives the probability of transitioning to state
        $s^\prime \in \mathcal{S}$ for all $s^\prime \in \mathcal{S}$.
        \item $\{r_{h}\}_{h=1}^{H}$ is the collection of reward functions, $r_h : \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$
        where $r_h(s,a)$ gives the reward from taking action $a \in \mathcal{A}$ from state $s \in \mathcal{S}$
        at stage $h \in \{1, \ldots, H\}$.
    \end{itemize}
\end{definition}

To relate the above to the Ad marketplace context, we can adopt the language introduced in
section~\ref{sec:problem-formulation-data}. Namely, the state space $\mathcal{S}$ is comprised
of the set of all available \textbf{User} and \textbf{Contextual} features in the ad marketplace
data, the action space $\mathcal{A}$ is made up of the set of available advertisments with the
associated \textbf{advertisment} features and the reward would be the binary click label
for each instance.

The aim of \textbf{Reinforcement Learning} is to interact with the MDP process environment in such a way
that allows the agent to learn the \emph{optimal policy} - i.e. the state-stage $\rightarrow$ action mapping
that maximizes the \emph{value} over the longer term. We refine some of these terms with more definitions
from \citep{pike-burke2024LearnigAgents} below:

\begin{definition}\label{def:policy}
    A \textbf{policy} $\pi = \{\pi_h\}_{h=1}^{H}$ is a sequence of mappings $\pi_h : \mathcal{S}
    \rightarrow \mathcal{A}$ for any $s \in \mathcal{S}, a \in \mathcal{A}$.
\end{definition}

\begin{definition}\label{def:policy-value}
    the \textbf{value} of a policy $\pi$ from state $s \in \mathcal{S}$ in stage
    $h \in \{1, \ldots, H\}$ is given by:
    \begin{equation}\label{eqn:value-function}
        V_h^{\pi} = \mathbb{E} \left[ \sum_{l = h}^{H} \gamma^{l-h} r_l (s_l, a_l) \Bigm| s_h = s, a_l = \pi(s_l), s_{l+1} \sim P_l(\cdot | s_l, a_l)\right]
    \end{equation}
\end{definition}

Where $\gamma$ represents a chosen \emph{discount factor} for potential rewards received in the future. 
The \emph{optimal policy} $\pi^*$ is then the one with the highest value, i.e.:
\[
\pi_h^*(s) = \arg\max_{\pi \in \Pi} V_h^{\pi}(s)
\]
for any $s \in \mathcal{S}$ and any $h \in \{1,\ldots, H\}$. In order to optimize for the
policy that maximizes the expected value, we would need to consider the expected
value of taking a specific action $a$ at specific stage $h$ and state $s$, for some given policy $\pi$. This is given
by the \textbf{Q-function}, defined below in definition~\ref{def:q-function}.

\begin{definition}\label{def:q-function}
    The \textbf{Q-function} associated with taking action $a \in \mathcal{A}$ from state
    $s \in \mathcal{S}$ at stage $h \in \{1, \ldots, H\}$ under some given policy $\pi$ is given by
    \begin{equation}\label{eqn:q-function}
        Q_h^{\pi}(s,a) = \mathbb{E} \left[ \sum_{l = h}^{H} \gamma^{l-h} r_l (s_l, a_l) \Bigm| s_h = s, 
        a_h = a, a_l = \pi(s_l), s_{l+1} \sim P_l(\cdot | s_l, a_l)\right]
    \end{equation}
\end{definition}

The relationship between the value function $V$ and the state action value function $Q$ is
summarized in the $\textbf{Bellman equations}$. For any policy $\pi$ and for all $h, s, a$:

\begin{align*}
    V_{h}^{\pi}(s) &= Q_{h}^{\pi}(s, \pi_{h}(s))\\
    Q_{h}^{\pi}(s,a) &= r_h(s,a) + \gamma \sum_{s^\prime \in \mathcal{S}} P_h (s^\prime | s, a)V_{h+1}^{\pi}(s^\prime)\\
    V_{H+1}^{\pi}(s) &= 0
\end{align*}

The above leads to the key equations that underpin Reinforcement Learning - the \textbf{Bellman Optimality equations}.

\begin{proposition}\label{prop:bellman-optimality}
    If $V^*$ satisfies the Bellman Optimality Equations, then for all $h, a, s$:
    \begin{align*}
        V_h^*(s) &= \max_{a \in \mathcal{A}}Q_{h}^*(s,a)\\
        Q_h^*(s,a) &= r_h(s,a) + \gamma \sum_{s^\prime \in \mathcal{S}} P_h (s^\prime | s, a)V_{h+1}^*(s^\prime)\\
        V_{H+1}^*(s) = 0
    \end{align*}
    The above implies that the optimal policy $\pi^*$ is given by:
    \[
    \pi_h^*(x) = \arg\max_{a \in \mathcal{A}} Q_h^*(s,a)
    \]
\end{proposition}

In other words, the optimal policy is found by maximizing the $Q$-function at each stage and
state. The basic idea of this is implemented in the \emph{Dynamic Programming} algorithm, shown below:

\begin{algorithm}
    \caption{Dynammic Programming algorithm}\label{alg:dynammic-programming}
    \begin{algorithmic}[1]
        \Require $P_h(s^\prime|s,a)$ and $r_h(s,a)$ are known
        \State Set $V_{H+1}^*(s) = 0$ for all $s \in \mathcal{S}$
        \For{$h = H, \ldots, 1$}
            \State Calculate $Q_h^*(s,a) = r_h(s,a) + \gamma \sum_{s^\prime \in \mathcal{S}}P_h(s^\prime |s,a) V_{h+1}^*(s^\prime)$
            \State Set $\pi_h^*(s) = \arg\max_{a \in \mathcal{A}} Q_h^*(s,a)$
            \State Define $V_h^*(s) = \max_{a \in \mathcal{A}}Q_h^*(s,a) = Q_h^*(s, \pi_h^*(s))$
        \EndFor
    \end{algorithmic}
\end{algorithm}

Of course in practice, we more often than not do not know the transition probabilities and
reward function of the environment.
The aim of Reinforcement Learning
is to define an algorithm that adequately manages the \emph{exploration-exploitation trade off}.
Ideally, actions should be chosen in such a way that simultaneously allows for the collection of sufficient
datapoints for estimating the reward function and transition probabilities, while also minimizing
the long term cumulative \emph{regret} as much as possible.

\begin{definition}\label{def:regret}
Let $K$ be the total number of episodes, $\pi^*$ be the optimal policy and $\pi_t$ be
the policy chosen for episode $t$. The the cumulative regret over $K$ episodes is given by
\begin{equation}
    \mathcal{R}_T = \sum_{t=1}^K \mathbb{E}\left[ V_1^{\pi^*}(s_{1,t}) - V_1^{\pi_t}(s_{1,t})\right]
\end{equation}
\end{definition}

One possible approach for achieving this is through algorithms that explicitly model the transition
probabilities $P_h$. Two popular methods for doing so are the Upper Confidence Bound for Reinforcement Learning (UCBRL)
\citep{RefWorks:auer2008near-optimal} and the Thompson Sampling algorthm for Reinforcement
Learning \citep{RefWorks:pike-burke2024optimism/thompson}. In UCBRL, the transition probabilities
are empirically estimated on the basis of observed feedback from the environment, whereas Thompson
Sampling proceeds by maintaining a posterior categorical distribution with a Dirichelet conjugate prior,
and then sampling the transition function from the posterior.

\section{Q-Learning and Deep Q-Learning}

\subsection{Q-Learning}

A major drawback with both of the UCBRL and Thompson Sampling algorithms is that they both
require the state transtition model to be approximated and stored. This poses a serious practical
issue in the case of Ad personalization, since as we have covered in section\ref{sec:problem-formulation-data},
Ad marketplace data tends to be extremely sparse once encoded. This means that in order to fully
calculate the transition probabilities, one would need to account for a vast number to state-action-transition
tuples, which is likely to be computationally unfeasible. In this section, we explore $Q$-learning,
a subdomain of Reinforcement Learning that aims to learn the $Q$-function directly, and is therefore 
\emph{model free}.

Q-learning was pioneered by \cite{RefWorks:watkins1989learning} as a model-free, and therefore
computationally efficient method for solving RL problems that have a sparse state
and action space. The basic Q-learning algorithm works by maintaining an estimate
of the Q-function for every state-action pair, and selecting a policy that
is greedy with respect to this estimate \citep{pike-burke2024LearnigAgents}.
The Q-function estimate $\hat{Q}(s,a)$ is usually initialized with its value
set to $\hat{Q}_{h}(s,a) = H$ for all $s \in \mathcal{S}, a \in \mathcal{A}$.
In each stage, the algorithm proceeds by choosing the action that maximizes the
$\hat{Q}_{h}(s,a)$, and observing the reward received $r_h(s,a)$ and the
resulting state $s^\prime$. Before the next stage, the $\hat{Q}_{h}(s,a)$
estimate is then updated to reflect the actual results observed. The steps
for the basic Q-learning algorithm are shown in Algorithm~\ref{alg:basic-q-learning}

\begin{algorithm}
    \caption{Basic Q-Learning Algorithm. Source: \citep{pike-burke2024LearnigAgents}}\label{alg:basic-q-learning}
    \begin{algorithmic}[1]
        \State \textbf{Initialization:} $\hat{Q}_{h,0}(s,a) = H$ for all $s \in \mathcal{S}, a \in \mathcal{A}, h \in \{1, \ldots, H\}$
        \For{episode $t = 1, \ldots, K$}
            \State Observe $s_{1,t}$
            \For{Stage $h = 1, \ldots, H$}
                \State Select action $a_{h,t} = \arg\max_{a \in \mathcal{A}}\hat{Q}_{h,t}(s_{h,t},a)$
                and update $N_{h,t}(s_{h,t}, a_{h,t})$
                \State Observe $s_{h+1, t}$ and $r_{h,t}(s_{h,t}, a_{h,t})$
                \For{All $(s, a, s^\prime)$ values}
                    \If{$(s, a, s^\prime) = (s_{h,t}, a_{h,t}, s_{h+1, t})$}
                        \State Update $\hat{Q}_{h,t+1}(s,a) = (1- \alpha_{h,t})\hat{Q}_{h,t}(s,a) + \alpha_{h,t}(r_{h,t}(s, a) + \gamma \arg\max_{a^\prime \in \mathcal{A}}\hat{Q}_{h,t}(s^\prime,a^\prime))$
                    \Else
                        \State Set $\hat{Q}_{h,t+1}(s,a) = \hat{Q}_{h,t}(s,a)$
                    \EndIf
                    \EndFor
                \EndFor
            \EndFor
    \end{algorithmic}
\end{algorithm}

The advantage of the Q-learning algorithm is that the estimates for unobserved
values remains the same, there is no additional computation that needs to take place.
We only need to store $H \times |\mathcal{S}| \times |\mathcal{A}|$ unique estimates
for $\hat{Q}(s,a)$, as opposed to $H \times |\mathcal{S}| \times |\mathcal{A}| \times |\mathcal{S}|$
transition probabilities, which significantly lowers the memory requirement \citep{pike-burke2024LearnigAgents}.
Note that in Algorithm~\ref{alg:basic-q-learning} the $\alpha_{h,t}$ parameter represents an
\emph{update step size} parameter, which usually depends inversely on $N_{h,t}(s_{h,t}, a_{h,t})$.

\subsection{Deep Q-Learning}

While the basic Q-Learning algorithm significantly decreases the computational requirement
by removing the need to store the transition probability model, keeping track of Q-function values
may still be prohibitive in the case where the state and action spaces are sparse,
as is the case in the Ad personalization domain. All of the aforementioned algorithms are designed
for discrete state and action spaces with relatively small cardinalities, and the performance of these
algorithms deteriorates for increased number of state-action combinations that need to be accounted for \citep{pike-burke2024LearnigAgents}.
For sparse enviroments, it is therefore desirable to find a suitable \emph{function approximator} $f_{\Theta}: \mathbb{R}^n \rightarrow \mathbb{R}$ for the
Q-function that can estimate the value of the of a state-action pair on the basis of a set of input
action-state features $\mathbf{x} \in \mathbb{R}^n$ and a set of learned parameters $\Theta$. This
means that rather than having to store $\hat{Q}(s,a)$ values for every state-action combination,
we will only have to store the function parameters $\Theta$, and assume that $\hat{Q}(s,a) = f_{\Theta}(\mathbf{x})$,
thereby changing the memory requirement from $H \times |\mathcal{S}| \times |\mathcal{A}|$ to simply
$|\Theta|$.

\cite{RefWorks:mnih2015human-level} proposed the Deep Q-Network algorithm in which
which the Q-function is approximated using a Deep Neural Network called a \emph{Deep Q-Network}.
The full Deep Q-Network algorithm is shown in Algorithm~\ref{alg:deep-q-learning}.

\begin{algorithm}
    \caption{Deep Q-Learning with Experience Replay. Source: \citep{RefWorks:mnih2015human-level}}
    \label{alg:deep-q-learning}
    \begin{algorithmic}[1]
        \State \textbf{Initialize:} Replay memory $\mathbf{D}$ to capacity $\mathbf{N}$.
        \State \textbf{Initialize:} Q-function approximator $f_{\theta}$ with random weights $\Theta$.
        \State \textbf{Initialize:} Set target action-value function $\hat{f}_{\hat{\Theta}}$ with $\hat{\Theta} = \Theta$
        \For{Episode $t=1,\ldots, K$}
            \State Initialize state sequence $s_1$ and preprocess the sequence $\phi_1 = \phi(s_1)$
            \For{Stage $h=1,\ldots,H$}
                \State with probability $\epsilon$ select a random action $a_h$,
                otherwise select action $a_h = \arg\max_{a \in \mathcal{A}} f_{\Theta}(\phi_h, a)$
                \State Execute action $a_h$ and observe reward $r_h(s_h, a_h)$ and the next state $s_{h+1}$
                \State Proprocess the features of the next state $\phi_{h+1} = \phi(s_{h+1})$
                \State Store the transition $(\phi_{h}, a_h, r_h, \phi_{h+1})$ in $\mathbf{D}$
                \State Sample a random set of transitions $(\phi_{j}, a_j, r_j, \phi_{j+1})$
                \State \label{dqn-target} Set $y_j = \begin{cases} r_j, & \text{if } j=H\\ r_j + \gamma \max_{a} \hat{f}_{\hat{\Theta}}(\phi_{j+1},a), & \text{otherwise} \end{cases}$
                \State Perform gradient descent step on $L(\Theta) = \left(y_j - f_{\Theta}(\phi_j,a_j)\right)^2$ with respect to $\Theta$
                \State Every $C$ steps reset the target action-value function $\hat{f} = f$ by setting weights $\hat{\Theta} = \Theta$
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}
In the original paper, the Deep Q-learning Network (DQN) algorithm was initially proposed to find the optimal
playing policies for 49 of the classic Atari 2600 games, where a given state would be represented by the game's
pixel values, the set of actions were possible joystick movements and button presses and the rewards were the number
of points accumalated throughout the game. \cite{RefWorks:mnih2015human-level} found that due to the highly sequential nature of
the state-action-reward data, using newly observed data to directly update the Q-function DNN approximator
would result in significant instabilities due to autocorrelation of inputs \citep{RefWorks:mnih2015human-level}.
It is for this reason that the following modifications made in algorithm~\ref{alg:deep-q-learning} over and above
basic Q-Learning:

\begin{itemize}
    \item In order to minimize the risk posed by the sequential dependancies in the newly observed data, Deep Q-Learing
    model is trained by means of \emph{experience replay.} Every new observation is stored in memory $\mathbf{D}$. In order
    to fit the model, a sample of \emph{experience observations}  $(\phi_{j}, a_j, r_j, \phi_{j+1})$ is sampled from $\mathbf{D}$
    uniformly at random, and is then used to train the model by means of gradient descent.
    \item Using the same model for selection and calculating the gradient descent targets ($y_j$) tends to lead to further model instability as the $y_j$ values are overestimated \citep{RefWorks:mnih2015human-level}.
    To prevent this, a separate \emph{target model} $\hat{f}$ with separate weights $\hat{\Theta}$ is used to calculate the $y_j$ values.
\end{itemize}

\section{DRN: Deep Reinforcement Learning for News Recommendation}
\label{sec:drn}

DRN \citep{RefWorks:zheng2018drn:} is a Deep Q-Learning framework 
that has been adapted to do online news personalization. In this setting, the objective is for the
agent to learn a policy for choosing the ideal list of news articles to display to each user upon request, 
in order to maximize
the long term engagement of all users with the news website. In this context, the state space $\mathcal{S}$
is defined by \emph{User features} (user's past click activity) and \emph{Contexutal Features} (time, week day,
freshness of the news),  the action set $\mathcal{A}$ is defined by the features describing the
news articles available at the time of the news request (headline, provider, ranking, entity name, category etc.)
and the reward is a combination of user news article clicks, and a measure of user activeness on the site.

In order to adapt to this news personalization context, the DRN algorithm introduces a few new components to the standard Deep Q-Network (DQN) algorithm introduced
by \cite{RefWorks:mnih2015human-level}. Firstly, it defines the reward as a combination of \textbf{user clicks}
and \textbf{user activeness}, as formulated in the equation below:
\begin{equation}
    r_{total} = r_{click} + \beta r_{active}
\end{equation}

\cite{RefWorks:zheng2018drn:} defines $r_{active}$ to take real values in $[0,1]$ according to how frequently
the user has visited the news site recently before $t_i$. Specifically, $r_{active}$ is calculated using
Survival Analysis \citep{ibrahim2001bayesian,jing2017neural}. 

Secondly rather than applying the
$\epsilon$-greedy method for exploration as in Algorithm~\ref{alg:deep-q-learning}, DRN uses the \textbf{Dueling Bandit Gradient Descent} algorithm \citep{grotov2016online}.
This algorithm proceeds by generating an \emph{explore network} $\tilde{Q}(s,a) \approx \tilde{f}_{\tilde{\Theta}}$ before each timestamp $t_i$.

\begin{definition}
The \emph{explore network} $\tilde{f}_{\tilde{\Theta}}$ with weights $\tilde{\Theta}$ has the same architecture
as the base Deep Q-Network $Q \approx f_{\Theta}$ (otherwise referred to as the \emph{exploit network}), and
the weights $\tilde{\Theta}$ are calculated as
\begin{equation*}
\tilde{\Theta} = \Theta + \alpha U \Theta
\end{equation*}
where $\Theta$ represents the weights for $Q$ , $U \sim Uniform(-1,1)$ and $\alpha$ is an explore parameter
\end{definition}

For each user-query session at each timestamp $t_i$, the list of news articles $L_i$ is then chosen
from the list of candidates $I_i$ by ramdomly combining recommendations using both the exploit network
$Q$ and explore network $\tilde{Q}$. If the items recommended by $\tilde{Q}$ recieve better feedback from
the user by the end of the session, the exploit network weights $\Theta$ are updated towards $\tilde{\Theta}$.

Lastly, the DRN algorithm uses the Double DQN target calculation \citep{RefWorks:van2016deep} to calculate $y_t$ (replacing line~\ref{dqn-target} in algorithm~\ref{alg:deep-q-learning})
\begin{equation}
    \label{eqn:ddqn}
    y_j = 
    \begin{cases} 
        r_j, & \text{if } j=H\\ 
        r_j + \gamma \hat{f}_{\hat{\Theta}}(\phi_{j+1},\arg \max_{a}f_{\Theta}(\phi_{j+1},a)), & \text{otherwise} 
    \end{cases}
\end{equation}

The key distinction in the equation above is that rather than using the target network $\hat{f}_{\hat{\Theta}}$ to directly
estimate the value of future rewards $Q(\phi_{j+1},a_{j+1})$, the base Q-function approximator $f_{\Theta}$ is used to
find the optimal action at time $j+1$, after which the target approximator $\hat{f}_{\hat{\Theta}}$ is used
to evaluate the future value of this action. This method was introduced by \cite{RefWorks:van2016deep} in order
to mitigate instabilities in the DQN algorithm.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/drn.png}
    \caption{DRN Framework. Source: \citep{RefWorks:zheng2018drn:}}
    \label{fig:drn}
\end{figure}

The DRN algorithm is composed of an Offline part and an Online part, as shown in Figure~\ref{fig:drn}.
The Offline part consists fitting the initial $Q$-function approximator network $f_{\Theta}$,
using previously collected session records containing the user, contextual and news article features previously
discussed.
The Online Learning part of the DRN framework proceeds as follows, as per \citep{RefWorks:zheng2018drn:}:

\begin{enumerate}
    \item \textbf{Push:} At each timestamp $t_i$, a user $u$ browses to the news website, thereby
    submitting a request for news articles. The agent $G$ takes account of the user and context
    features, as well as the features for the set of currently available news candidates $I_i$.
    The agent then selects the top $l$ candidantes. The list of $L_i$ candidates is compiled by mixing
    items reccomended by the \emph{exploit Q-network} $Q$ and the \emph{explore network} $\tilde{Q}$.
    The list is then shown to the user.
    \item \textbf{Feedback:} The user will then provide feedback by either clicking or not clicking
    on each of the $L$ news articles. This feedback is stored in memory $D$, along with updated user
    activeness metrics for user $u$.
    \item \textbf{Minor Update:} After each timestamp $t_i$, the agent collects the user's feedback,
    and compares the relative CTR performance of news candidates recommended by $Q$ and $\hat{Q}$. 
    If $\hat{Q}$ outperformed $Q$, then the weights of the explore network $W$ are updated to those
    of the explore network $\tilde{W}$. Otherwise the weights of the exploit network remain unchanged.
    \item \textbf{Major Update:} After a pre-defined time period $T_R$, the agent will use the feedback
    data stored in $D$ to update the exploit network $Q$. The reward will be based on a compination
    of user clicks and activeness, and $Q$ will be updated using the Experience Replay technique from
    Algorithm~\ref{alg:deep-q-learning}, with the target value adaptation from equation~\ref{eqn:ddqn}.
\end{enumerate}

\chapter{Deep CTR model Evaluation}
\label{chap:deep-ctr-model-evaluation}

As explained in the preceeding introduction, chapter~\ref{chap:deep-ctr-model-evaluation} is
dedicated for evaluating a range of different click prediction models. The intention is then to
incorporate the best model candidate in a Deep Reinforcement Learning framework for Ad personalization
in chapter~\ref{chap:deep-rl-for-ad-personalization}. The contents of chapter~\ref{chap:deep-ctr-model-evaluation}
include the results of two successive experiments:

\begin{enumerate}
    \item \label{exp:comparative-model-analysis} A \textbf{Comparitive Model Analysis}, whereby the predictive performance of a range of different CTR models
    is measured using equivalent hyperparameter settings and the same benchmark CTR datasets.
    \item[] \textbf{Research Question:} \emph{Which of the CTR prediction models in scope have the highest predicted performance
    when measured using equivalent hyperparameter settings and using the same benchmark CTR prediction datasets?}
    \item \label{exp:hyperparameter-analysis} A \textbf{Hyperparameter Setting Analysis}, whereby we take the highest performing model from the previous
    experiment above, and calculate and compare that model's predictive performance for different parameter settings. To
    simplify the complexity of this task, each chosen parameter was set to to a finite set of values, and this was conducted
    sequentially for the different parameters. This will be explained in more detial in section~\ref{sec:hyperparameter-selection}.
    \item[] \textbf{Research Question:} \emph{What are the ideal hyperparameter values for the given CTR prediction model?}
\end{enumerate}


We first proceed by explaining the model selection criterea and listing the candidate models in section~\ref{sec:model-selection}.
We then describe the Experiment Setup for the model evaluations in section~\ref{sec:experiment-setup}, including
data preprocessing, evaluation metrics, optimization algorithms and hyperparameter selection. Finally, we conclude the chapter
by presenting the results for the experiments above in section~\ref{sec:model-results}, in which the \textbf{Deep and Cross Network} (DCN) architecture \citep{RefWorks:wang2017deep}
was revealed to have the highest predictive accuracy on our chosen datasets.

\section{Models and Model selection Criteria}
\label{sec:model-selection}

CTR classification with Deep Learing methods is an extremely active field of research, with many
different model architectures being explored by different authors. It is because of this
reason that it is practically impossible to evaluate all models within the scope
of this project. For the purposes of this paper, the scope of models has therefore
been narrowed down using the following model selection criteria:

\begin{itemize}
\item Competitive predition accuracy in the KDD12, Criteo and Avazu datasets as published on Papers with Code \citep{RefWorks:2024papers}.
\item The set of models should be representitive of the range of feature modelling
methods used in Deep CTR classification, as discussed in (Zhang et. al. 2021).
\item The code for the models has to be accessible and intuitive to use. The DeepCTR Python package \citep{RefWorks:shen2017deepctr:} is used
to implement the experiments in this chapter.
\end{itemize}

On the basis of the above critea, the following models from chapter~\ref{chap:background-deepctr} have been selected for the scope of this paper:

\begin{itemize}
\item Factorization Supported Neural Networks (FNN) \citep{RefWorks:zhang2016deep}
\item Product Based Neural Networks (PNN) \citep{RefWorks:qu2016product-based}
\item Neural Factorization Machines (NFM) \citep{RefWorks:he2017neural}
\item Deep and Cross Network (DCN) \citep{RefWorks:wang2017deep}
\item Wide and Deep Learning (WDL) \citep{RefWorks:cheng2016wide}
\item DeepFM (DFM) \citep{RefWorks:guo2017deepfm:}
\item Automatic Feature Interaction (AutoInt) \citep{RefWorks:song2019autoint}
\item Convolutional Click Prediction Model (CCPM) \citep{RefWorks:liu2015convolutional}
\item Feature Generation by Convolutional Neural Network (FGCNN) \citep{RefWorks:liu2019feature}
\end{itemize}

For comparison, we also evaluate the predictive performance of the shallow models, 
as a baseline:

\begin{itemize}
    \item Logistic Regression (LR) \citep{RefWorks:richardson2007predicting}
    \item 2-way Factorization Machines (FM) \citep{RefWorks:rendle2010factorization}
\end{itemize}

\section{Experiment Setting}
\label{sec:experiment-setup}

The full script for both experiments outlined above is contained in the iPython notebook
\verb|ctr_model_testing.ipynb|, which is available on the Github Repository for this project \citep{Batek_Deep_Reinforcement_Leaning_2024}.
All experiments were carried out using Tensorflow version 2.14 on AWS Sagemaker JupyterLab, utilizing a ml.g5.2x.large compute instance.
Model implementations from the DeepCTR Python package \citep{RefWorks:shen2017deepctr:} were
used for all Deep models. For full Python environment specifications, please refer to the
\verb|environment_gpu.yml| file on the aforementioned Github repository for this project.

\subsection{Datasets and Preprocessing}
\label{sec:datasets-preprocessing}

In order to evaluate and compare the predictive performance of each of the previously mentioned
models, we perform experiments using three widely known real-world benchmark datasets for
click-through rate prediction.

\subsubsection{KDD12}

The \textbf{KDD12} dataset was first released for the KDD12 cup 2012 competition
\citep{RefWorks:aden2012kdd}, with the original task being to predict the 
click through rate for a given number of impressions - i.e. the ratio of ad impressions
that would ultimately result in a click. Each line represents 
a training instance derived from the session logs for the advertizing 
marketplace. In the context of this dataset, a ``session'' refers to an 
interaction between a user and the search engine, containing the following 
components; the user, a list of adverts returned by the search engine and 
shown (impressed) to the user and zero or more adverts clicked on by the 
user. Each line in the training set includes, Click and Impression counts, 
Session features, User features and Ad features.

\subsubsection{Avazu}

The \textbf{Avazu} dataset was originally released in 2014 for a CTR prediction 
Competition on Kaggle \citep{RefWorks:wang2014click-through}. The data is 
composed of 11 days worth mobile ad marketplace data. Much like the KDD12 
dataset above, this dataset contains features ranging from user activity 
(clicks), user identification (device type, IP) to ad features.

\subsubsection{Criteo}

Finally, the \textbf{Criteo} dataset is another benchmark CTR prediction dataset 
that was originally released on Kaggle for a CTR prediction compitition \citep{RefWorks:tien2014display}. 
The original dataset is made up of 45 Million user's click activity, and 
contains the click/no-click target along with 26 categorical feature 
fields and 13 numerical feature fields. Unlike the other two datasets 
however, the semintic significance of these fields is not given - they 
are simply labelled as ``Categorical 1-26'' and ``Numerical 1-13'' 
respectively.

\subsubsection{Exploratory Data Analysis and Preprocessing}

The KDD12, Avazu and Criteo datasets mentioned above contain approximately 150, 231
and 99 million records, adding up to a 26.2 GB collective memory requirement.
As a first step in preprocessing, each dataset was shuffled and split into training (80\%) and validation (20\%)
sets. The size if the datasets necessitated the use of Amazon Web Services'
Data Wrangler Tool \citep{AWSDataWrangler} for shuffling and splitting, and this was done using a random seed of 42.
In order to restrict the training time and cost for the 11 models to a reasonable
amount (approximately half a minute on average per ephoch), model fitting was conducted using a 157440 record
training subsample and a 39360 record
subsample of each dataset.

We then proceeded to preprocess the categorical and numberical features. As previously
discussed in chapter~\ref{chap:background-deepctr}, ad platform categorical features tend to contain
a lot of unique values per feature - i.e. they exhibit high cardinality.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/dataset_cardinalities.png}
    \caption{Categorical feature cardinalities from subsets of each dataset}
    \label{fig:cardinalities}
\end{figure}

A common remidy to the above issue is to \emph{bin} the categorical feature 
values before one-hot encoding or embedding, according to some given 
threshold \citep{RefWorks:song2019autoint}. This essentially means that for a 
given threshold $t$, we retain only the values for the multi-value 
categorical features that have more than $t$ occurances in the dataset. As recommended by
\cite{RefWorks:song2019autoint}, the binning thresholds were set such that
$t = {10,5,10}$ for Criteo, KDD12 and Avazu respectively. The categorical
feature values were then label encoded as integers in descending order according
to the appearance of each value in the dataset, with integer keys starting
at $1$. In otherwords, the most frequent value mapped to $1$, the second most frequent
to $2$, and so on. The $0$ key was reserved for either infrequent or unknown categorical
feature values.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/numerical_boxplots.png}
    \caption{Boxplots of numerical features in the datasets}
    \label{fig:boxplots}
\end{figure}

Subsequently, an examinationation of the boxplots (see figure~\ref{fig:boxplots}) for the numerical features in
each of the datasets reveals the presence of high varience numerical outliers,
particularly in the case of the Criteo dataset. In order to ensure efficient
training by means of gradient descent, these values needed to be normalized to unit
variance and zero mean. Furthermore, for the Criteo Dataset the following
transformation was applied to the normalized values, as recommended by \cite{RefWorks:song2019autoint}:
\begin{equation*}
\tilde{z} = \begin{cases} z & \text{ if } z \leq 2 \\
    \log^{2}(z) & \text{ otherwise}
\end{cases}
\end{equation*}
Finally, since the datasets will be used to train a binary classifier, we examined the distributions
of target label values in each of the datasets. In Figure~\ref{fig:target-imbalance}, we see that
there is a significant target-variable imbalance for all three datasets, with positive label rates
ranging from 5\% for KDD12 to 25\% for Criteo. This is range is fairly typical of what one
can expect of average digital ad placement Click-Through rates, and KDD12 is the
most representative of the three in this regard \citep{RefWorks:cxl“good”}. We can
expect that this imbalance will have an adverse impact model Precision and AUC.
Due to the large imbalance for KDD12, we upweight all positive class labels by a
factor of $4.98$ and downweight all of the negative labels by a factor of $0.26$.
in all experiments.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/dataset_target_imbalance.png}
    \caption{Target variable distribution from 10K record random samples from each dataset}
    \label{fig:target-imbalance}
\end{figure}

\subsection{Evaluation Metrics}
\label{sec:evaluation-metrics}

Predictive performance of each model was assessed in terms of the following five metrics:

\begin{itemize}
    \item \textbf{Binary Cross Entropy Loss:} Also known as logarithmic loss. This metric
    penalizes the model when the predicted log likelihood of a positive label $\hat{y} = \sigma(f_{\Theta}(\mathbf{x}))$ deviates from the
    log likelihood that $y=1$ as suggested by the data. For $K$ feature-label observations, this Loss metric is calculated as:
    \[
    \mathcal{L}^{\text{Cross-Entropy}}(\mathbf{x},y,\Theta) = - \sum_{k=1}^{K} \left[y \log \sigma(f_{\Theta}(\mathbf{x})) + (1-y)\log \left( 1 - \sigma(f_{\Theta}(\mathbf{x}))\right) \right]
    \]
    Binary Cross Entropy is commonly used as a loss metric in multiple binary classification models
    \citep{RefWorks:zhang2021deep}, and has therefore been utilized as the primary loss function
    for model training for all experiments in this report.

    \item \textbf{AUC:} Area under ROC curve is another commonly used metric for binary classification,
    and it has been validated as a good evaluation metric for the Click Through Rate prediction task
    \citep{RefWorks:graepel2010web-scale}.

    \item \textbf{Binary Accuracy:} Binary Accuracy measures the percentage of times that the predicted label
    (with regards to the positive label likelihood prediction $\hat{y}$ and some likelihood threshold $t \in (0,1)$)
    matches the actual label $y$ for a set of $K$ observations.

    Let
    \[
    g_{t}(\hat{y}) =
    \begin{cases}
        1 & \text{ if } \hat{y} \geq t \\
        0 & \text{ otherwise }
    \end{cases}
    \]

    and let $\mathbb{I}$ be the indicator function where $\mathbb{I}_{a}(b)=1$ if $a=b$ and $\mathbb{I}_{a}(b)=0$ otherwise. Then

    \[
    M^{\text{Binary Accuracy}}(\mathbf{x},y,\Theta,t) = \frac{ \sum_{k=1}^{K} \mathbb{I}_{y_k}(g_{t}(\sigma(f_{\Theta}(\mathbf{x}_k))))}{K}
    \]
    Binary Accuracy is an intuitive metric to understand, since it simply shows how often the model is correct.
    However, this can be misleading in cases where label imbalance is prevalent, and it therefore often prodent
    to also refer to the Precision and Recall metrics. For the experiments in this report,
    we evaluate the models using Binary Accuracy with $t=0.5$.

    \item \textbf{Precision:} Precision measures the ratio of observations with positive predicted labels
    that in fact have positive labels in the data:
    \[
    M^{\text{Precision}}(\mathbf{x},y,\Theta,t) =
    \frac{ \sum_{k=1}^{K} \mathbb{I}_{y_k}(g_{t}(\hat{y}_k)) \cdot \mathbb{I}_{1}(g_{t}(\hat{y}_k))}{\sum_{k=1}^{K}\mathbb{I}_{1}(g_{t}(\hat{y}_k))}
    \]

    Where $\hat{y}_k = \sigma(f_{\Theta}(\mathbf{x}))$. In the context of Ad personalization, the Precision metric can be seen as an indicator of quality
    of the set of Ads retreived for the user, from the perspective of that user's Ad preferences. Like
    in the case of Binary Accuracy, we evaluate the models in this report using Precision with $t=0.5$
    \item \textbf{Recall:} Recall measures the proportion of the data observations with true positive
    labels that have positive predicted labels:

    \[
        M^{\text{Recall}}(\mathbf{x},y,\Theta,t) =
        \frac{ \sum_{k=1}^{K} \mathbb{I}_{y_k}(g_{t}(\hat{y}_k)) \cdot \mathbb{I}_{y_k}(1)}{\sum_{k=1}^{K}\mathbb{I}_{y_k}(1)}
    \]

    The Recall Metric is relevant in the Ad personalization context, since it in essence reveals how good
    the model is at finding Ads that the user will click on. Again, in this report we set the predicted
    probability threshold $t$ to $0.5$.
\end{itemize}

\subsection{Optimization Algorithms}
\label{sec:optimization-algorithms}

In section~\ref{sec:evaluation-metrics} we have already stated that the primary loss function used in this report
for model fitting is the Binary Crossentropy Loss function. For the all models, we used the Adam Optimizer \citep{Kingma2014AdamAM}, a widely popular optimiztion algorithm
for training Deep Neural Networks. We set the learning parameter to $\eta = 0.001$, which the
standard learning rate setting for the Keras Adam Optimizer implementation \citep{chollet2023keras}.

In order to ensure that training times (and therefore training cost)
remained within a reasonable range, all models were trained to a maximum of 15 epochs. In addition to this,
the Keras EarlyStopping Callback was used with \verb|patience| parameter set to 5 epochs. This was done to
further ensure that training time remained within a reasonable range, as well as in order to prevent
over-fitting.


\subsection{Hyperparameter Selection}
\label{sec:hyperparameter-selection}

As mentioned at the beginning of section~\ref{sec:experiment-setup}, the DeepCTR Python model developed
by \cite{RefWorks:shen2017deepctr:} was leveraged to implement all the Deep CTR models listed in
section~\ref{sec:model-selection}. The majority of the of the model classes in the DeepCTR package
use the following hyperparameter inputs, which we have standardized for experiment~\ref{exp:comparative-model-analysis}:

\begin{itemize}
    \item \textbf{DNN Hidden Units:} This parameter controls the number of hidden layers and the number of neurons
    in each hidden layer of the MLP component of the model. For experiment~\ref{exp:comparative-model-analysis},
    this parameter was set to $[200,200,200]$ for all models - i.e. 3 layers, containing 200 neurons each.

    \item \textbf{DNN Dropout Likelihood:} For every neuron in the MLP components in the model, the DNN Dropout
    parameter controls the likelihood that the neuron is removed during training. \cite{RefWorks:srivastava2014dropout:}
    found that introducing such randomness during training can greatly improve regularization. As recommended
    by \cite{RefWorks:guo2017deepfm:}, this parameter was set to $0.6$ for both experiments.

    \item \textbf{Categorical Feature Embedding Dimension:} This is represented by variable $D$
    in equation~\ref{eqn:cat-embedding}. For all experiments, we set $D=4$.

    \item \textbf{Embedding L2 Regularization:} The L2 regularization strength that was applied while fitting
    the categorical feature embedding matrices ($B_i$). This was set to $0.005$ for both experiments.

    \item \textbf{Linear L2 Regularization:} The L2 regularization strength that was applied in the
    linear component of the model, such as the Wide component of the WDL model. 
    For relevant models, this was set to $0.005$ for both experiments.

    \item \textbf{DNN L2 Regularization:} The L2 regularization strength that was applied in the
    MLP component of the model, such as the Deep component of the WDL model. 
    For relevant models, this was set to $0.005$ for both experiments.

    \item \textbf{DNN Batch Normalization:} This parameter controls whether the internal activation
    values of the MLP component are normalized for every batch during training, so as to
    promote training stability and introduce and additional regularization effect \citep{RefWorks:ioffe2015batch}. This
    parameter was set to $True$ for all models and experiments. 
\end{itemize}

In experiment~\ref{exp:comparative-model-analysis}, all models were compiled and fitted with the
standard hyperparameter settings mentioned above. In experiment~\ref{exp:hyperparameter-analysis},
we took inspiration from \citep{RefWorks:guo2017deepfm:}, and explored the predictive accuracy of the
best performing model from experiment~\ref{exp:comparative-model-analysis} for different
\emph{DNN Hidden Unit} settings, namely sequentially cumulatively the following aspects:

\begin{enumerate}
    \item \textbf{Neurons per layer:} Here we scored the model when using 100, 200, 300, 400
    and 500 neurons per layer in the MLP component. Tests were carried out using 3 hidden layers.
    \item \textbf{Number of layers:} Here we scored the model when using 2, 3, 4, 5 and 6 hidden layers in
    the MLP component. Tests were carried out using 200 neurons per layer.
    \item \textbf{DNN shape:} Here we compared the predictive performance for different
    shapes of the Deep Neural Network, namely constant, increasing, decreasing, and diamond.
\end{enumerate}

Note that experiment~\ref{exp:hyperparameter-analysis} was conducted \emph{cumulatively} in the
order above. This means that the best Neuron per layer setting was determined first and that
this setting was used when exploring the Layer count settings, and so on.
Since experiment~\ref{exp:comparative-model-analysis} revealed that the DCN model was the best performing
candidate, we additionally elected to explore the performance of this model for different cross layer parameter
values, namely $c \in \{2,3,4,5,6\}$.

\section{Deep CTR Model Results and Discussion}
\label{sec:model-results}

\subsection{Experiment 1: Comparitive Model Analysis}

The results for experiment~\ref{exp:comparative-model-analysis} for the KDD12, 
Avazu and Criteo datasets are shown
in tables \ref{tab:exp-1-kdd12}, \ref{tab:exp-1-avazu} and \ref{tab:exp-1-criteo} respectively
A visualization of these results has been included in the appendix in Figure~\ref{fig:experiment-1}. 
Only the
metrics for the epochs with the minimum achieved training loss function (Binary Crossentropy) are 
reported. The first column in the tables indicates the epochs in which the minimum loss function
value was attained. Recall that in each case, the models were fitted to a maximum of 15 epochs,
with an EarlyStopping callback and a patience value set to 5 epochs.

\begin{table}
    \csvautotabular{../logs/kdd12_model_scores_cleaned.csv}
    \caption{Experiment 1 model scores on KDD12 dataset}
    \label{tab:exp-1-kdd12}
\end{table}

\begin{table}
    \csvautotabular{../logs/avazu_model_scores_cleaned.csv}
    \caption{Experiment 1 model scores on Avazu dataset}
    \label{tab:exp-1-avazu}
\end{table}

\begin{table}
    \csvautotabular{../logs/criteo_model_scores_cleaned.csv}
    \caption{Experiment 1 model scores on Criteo dataset}
    \label{tab:exp-1-criteo}
\end{table}

We see that our models generally did not reach the levels
of predictive performance mentioned in the respective experiments conducted in the original papers.
As an example of this, DeepFM and DCN attained a minimum Binary Crossentropy of 0.45083 and 0.4419
as reported in \citep{RefWorks:guo2017deepfm:} and \citep{RefWorks:wang2017deep}, but in
our experiments these models attained minimum Log Loss values of 0.5546 and 0.5225 respectively.
This can be expected, since the fact that the intention of experiment~\ref{exp:comparative-model-analysis}
was to deliver a broader comparitive analysis of a larger number of models, we have had to use
a relatively small subsamples of the benchmark datasets. The original authors of the models were
generally not constrained in this way, and had therefore used larger samples; for Deep \& Cross Learning,
\cite{RefWorks:wang2017deep} used 41 million records of the Criteo dataset, and were therefore able
to reach much higher levels of predictive performance.

When comparing the performance of Shallow models to their Deep 
counterparts, we find that although the Logistic
Regression and Factorization Machines models are consistantly in the
bottom 5 models in terms Binary Crossentropy and AUC for the three datasets,
the difference in performance is not significantly large, and in some cases
they outperformed some of the Deep models (such as in the case of
WDL and PNN for the Avazu dataset). This marginal performance differential consistant with what has been
shown with experiments using larger dataset samples in \citep{RefWorks:guo2017deepfm:,RefWorks:wang2017deep,RefWorks:liu2019feature},
and shows that although DNN models do tend to perform better, LR and FM can
still be considered to be viable ``parameter-light'' alternatives to DNN models
in CTR prediction.

The results shown reveal that the \textbf{Deep and Cross Network} (DCN)
model outperformed the rest relatively consistantly for all three datasets. For both
Avazu and Criteo, DCN outperformed all other models on the basis of both Binary Crossentropy
and AUC, whereas for the KDD12 dataset it outperformed in terms of AUC. The DCN
model also performed relalatively well in terms of both Precision and Recall,
scoring on par with or better than the majority of the models in scope. It displayed
a significant degree of efficiency during training, with relatively stable improvements in
loss for successive epochs (this is where the closest contender, NFM, struggled - particularly with the
Criteo and Avazu datasets). It is for these reasons that the DCN model was selected as the model
for use in experiment~\ref{exp:hyperparameter-analysis} and in chapter~\ref{chap:deep-rl-for-ad-personalization}.

There are several limitations of the method that we employ in experiment~\ref{exp:comparative-model-analysis}.
We already mentioned that due to the fact that we are using relatively small subsamples of the three
datasets, we do not attain the stated Log Loss and AUC from the original papers. Secondly, as we
elected to standardize the DNN hyperparameter settings as described in section~\ref{sec:hyperparameter-selection} and to keep all hyperparameter values as per
the defaults in the DeepCTR package \citep{RefWorks:shen2017deepctr:}, it is possible that this
may have resulted in sub-optimal hyperparameter settings for the specific models. The final notable limitation
stems from the target variable imbalance mentioned in section~\ref{sec:datasets-preprocessing}. Such
target label imalance makes it difficult for the model to learn which feature combinations are most associated
with a positive label, since the number of positively labelled observations is limited. In the case of
KDD12 where the class imbalance is most severe, we partially remedy this by applying class weights during
training. Unfortunately, this method tends to result in training instability. Some of the DeepCTR
researchers such as \cite{RefWorks:liu2019feature} used negative sampling to remedy the class imbalance issue instead.

\subsection{Experiment 2: Hyperparameter Setting Analysis}

Experiment~\ref{exp:comparative-model-analysis} revealed that the DCN model consistantly outperformed
all of the models in scope for the data and standard parameter settings discussed in
section~\ref{sec:experiment-setup}. In experiment~\ref{exp:hyperparameter-analysis} we
sequentially explore each of the hyperparameter settings as discussed in the latter half
of section~\ref{sec:hyperparameter-selection}, by refitting the DCN model to each hyperparameter
setting. This hyperparameter experiment was inspired by a similar exercise conducted in section
3.3 of \citep{RefWorks:guo2017deepfm:}. Experiment~\ref{exp:hyperparameter-analysis} was
conducted using the KDD12 dataset samples only.

\subsubsection{Neurons per Layer}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/neurons.png}
    \caption{Model scores for different neuron counts}
    \label{fig:neurons}
\end{figure}

When all other factors remain the same, increasing the number of neurons per layer introduces
complexity in the model \citep{RefWorks:wang2017deep}. The level of expressiveness that this brings
initially improves the degree to which the model can learn and generalize more complex patterns in the data, 
but will eventially result in overfitting for higher neuron values. Figure~\ref{fig:neurons} reveals
that for the DCN model, the best Binary Crossentropy and Precision scores were attained with 400 neurons
per layer.

\subsubsection{Number of Layers}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/layers.png}
    \caption{Model scores for different layer counts}
    \label{fig:layers}
\end{figure}

Similarly as to the case with neurons, we find that the same model bias-variance dynamic
applies when exporing the performance for an increasing number of layers. Figure~\ref{fig:layers}
reveals that initially, the Binary
Crossentropy and Precision of the DCN model improved from 2 to 3 layers, but performance
deteriorated in terms of both of these metrics thereafter. The DCN model therefore performed
best with 3 layers.

\subsubsection{DNN Shape}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/shape.png}
    \caption{Model scores for different DNN shapes}
    \label{fig:shape}
\end{figure}

For shape exploration, we fix the total number of neurons and the number of layers
to 1200 and 3 respectively in order to remain in line with the two previous findings.
This means that in terms of DNN Neuron settings, we used $[400,400,400]$ for Constant,
$[300,400,500]$ for Increasing, $[500,400,300]$ for Decreasing and $[300,600,300]$ for Diamond.
Figure~\ref{fig:shape} revealed that Constant yielded the best results for the
DCN model.

\subsubsection{Number of Cross Layers}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/cross_layers.png}
    \caption{Model scores for different Cross Layer settings}
    \label{fig:cross-layers}
\end{figure}

Finally, we explore the DCN model performance for different Cross Layer settings, ranging
from 2 to 6 Cross Layers. Recall from section~\ref{sec:feature-operator-models} that
the Cross Network component in the DCN model explicitly captures feature interations
up to order $k+1$, where $k$ is the number of Cross Layers. In Figure~\ref{fig:cross-layers},
we see that the DCN model generally tends to perform better for higher Cross Layer parameter
values. We choose to set this parameter to 5, as informed by the Precision metric specifically.
The final model and parameter settings chosen after experiments \ref{exp:comparative-model-analysis}
and \ref{exp:hyperparameter-analysis} are shown in table~\ref{tab:deepctr-model-result}.

\begin{table}[h]
    \begin{tabular}{|l|r|}
        \hline
        \textbf{Hyperparameter} & \textbf{Value}\\
        \hline
        Model & DCN \\
        Neurons per Layer & $400$ \\
        Number of Layers & $3$ \\
        DNN Shape & Constant \\
        Number of Cross Layers & 5 \\
        \hline
    \end{tabular}
    \caption{Model and Hyperparameters chosen after experiments 1 and 2}
    \label{tab:deepctr-model-result}
\end{table}

Experiment~\ref{exp:hyperparameter-analysis} shares the same limitations stemming from
the small data sample size and target variable imbalance as mentioned for experiment~\ref{exp:comparative-model-analysis}.
In addition, due to training time limitations we have had to employ the sequential parameter search
method described above, which would not explore all of the possible parameter combinations, as in the case
of a full Grid Search. This entails that the parameters found in experiment~\ref{exp:hyperparameter-analysis}
are not guarunteed to be optimal.

\chapter{Deep Reinforcement Learning for Ad Personalization}
\label{chap:deep-rl-for-ad-personalization}

In chapter~\ref{chap:deep-ctr-model-evaluation}, we conducted experiments to ascertain the
best performing candidate CTR prediction model out of those listed in section~\ref{sec:model-selection},
and to determine the ideal hyperparameter values. As a result we found the DCN model to be the
ideal candidate. In chapter~\ref{chap:deep-rl-for-ad-personalization} we propose the Deep Reinforcement Learning
for Ad Personalization (DRL-AP) algorithm, a Reinforcement
Learning algorithm that leverages the DCN model as a Deep Q-Network function appromator, and is
otherwise largely based in DRN algorithm. The next experiment that we conduct in this report will
be a simulation of the online learning process of the DRL-AP algorithm, while aiming to answer two
research questions:
\begin{enumerate}
    \setcounter{enumi}{2}
    \item \label{exp:rl-sim} A \textbf{ad search platform simulation} in which the KDD12 dataset will be used to
    simulate the sequential events on a advertizing platform as a MDP, and DRL-AP algorithm
    is used to choose the adverts that are shown to the user.
    \begin{enumerate}
        \item \label{exp:dcn-model-advantage} \textbf{Research Question:} \emph{Does the use of the DCN model provide a significant
        advantage in terms of Reward (Click) maximization and Regret Minimization over using no model?}
        \item \label{exp:drl-ap-advantage} \textbf{Research Question:} \emph{Does the DRL-AP algorithm provide a significant
        advantage in terms of Reward (Click) maximization and Regret Minimization over using DCN model
        with no updates?}
    \end{enumerate}
\end{enumerate}

\section{DRL-AP Algorithm}

The DRL-AP algorithm is largly based on the DRN algorithm described in section~\ref{sec:drn},
and leverages the DCN model as the Q-function approximator. Since the DCN model trained in 
chapter~\ref{chap:deep-ctr-model-evaluation} is trained as a binary classifier, the target
value used during the experience replay update $y_j$ can only take values on the interval $[0,1]$.
Therefore, we modified
the target value label calculation in equation~\ref{eqn:ddqn} to the following:
\begin{equation}
    \label{eqn:drl-ap-target}
    y_j = 
    \begin{cases} 
        r_j, & \text{if } j=H\\ 
        \frac{1}{1+\gamma}\left(r_j + \gamma \hat{f}_{\hat{\Theta}}(\phi_{j+1},\arg \max_{a}f_{\Theta}(\phi_{j+1},a))\right) & \text{otherwise} 
    \end{cases}
\end{equation}
thereby normalizing the target value by a factor of $1+\gamma$ for sequences
where $j\neq H$. Furthermore, due to the fact that we have been constrained
to offline testing only for this project, we have chosen not to include the
user activeness component from DRN in DRL-AP. However, this can straightforwardly
be included in future iterations. The DRL-AP algorithm is summarized in \ref{alg:drl-ap}. In
the ad personalization context, the state space $\mathcal{S}$ is defined as by the
user and contextual features at time $t$, the action set $\mathcal{A}_t$ is defined
by the ad's available for display at time $t$ and the rewards are the user clicks.

\begin{algorithm}
    \caption{Deep Reinforcement Learning for Ad Personalization Algorithm}
    \label{alg:drl-ap}
    \begin{algorithmic}[1]
        \State \textbf{Pretrain:} DCN Q-function approximator $f_{\theta}$ with offline historical data.
        \State \textbf{Initialize:} Replay memory $\mathbf{D}$ to capacity $\mathbf{N}$.
        \State \textbf{Initialize:} Set target action-value function $\hat{f}_{\hat{\Theta}}$ with $\hat{\Theta} = \Theta$
        \For{Episode $k=1,\ldots, K$}
            \For{Session $t=1,\ldots,M$}
            \State Observe state $s_{(t-1)P+1}$ as a combination of user $u$ and context/query $c$ features
            \State Preprocess and set state values for all positions in the ad list $\{\phi_h\}_{h=(t-1)P+1}^{tP} = \phi(s_{(t-1)P+1})$
            \State Retrive the list of avalable ads on the platform $\mathcal{A}_t$
            \State \textbf{Initialize:} Ranked ad List $L_t$ of length $P$ to display to the user
            \State \textbf{Initialize:} Explore model weights $\tilde{\Theta} = \Theta + \alpha U(-1,1) \Theta$
            \State \textbf{Initialize:} Explore model $\tilde{f}_{\tilde{\Theta}}$
                \For{List position $p=1,\ldots,P$}
                    \State $h = (t-1)P + p$
                    \State with probability $\epsilon$ select ad $a_h = \arg\max_{a \in \mathcal{A}_t} \Tilde{f}_{\tilde{\Theta}}(\phi_h, a)$,
                    otherwise select ad $a_h = \arg\max_{a \in \mathcal{A}_t} f_{\Theta}(\phi_h, a)$
                    %\State Execute action $a_h$ and observe reward $r_h(s_h, a_h)$ and the next state $s_{h+1}$
                    \State Place ad $a_h$ in position $p$ of the ad list $L_t$
                    \State Remove ad $a_h$ from $\mathcal{A}_t$ for the remainder of the session
                \EndFor
            \State \textbf{PUSH:} Display list $L_t$ to the user
            \State \textbf{FEEDBACK:} Recieve clicks feedback $\{r_h\}_{h=(t-1)P+1}^{tP}$ from user
            \If{Session CTR for $\tilde{f}_{\tilde{\Theta}}$ exceeds Session CTR for $f_{\theta}$}
                \State \textbf{MINOR UPDATE} Update $\Theta$ to $\tilde{\Theta}$
            \EndIf
            \State Store the transitions $(\phi_{h}, a_h, r_h, \phi_{h+1})$ in $\mathbf{D}$
            \EndFor
            \State Sample a random set of transitions $(\phi_{j}, a_j, r_j, \phi_{j+1})$ from $\mathbf{D}$
            \State Set $y_j = \begin{cases} r_j, & \text{if } j=M \cdot P\\ \frac{1}{1+\gamma}\left(r_j + \gamma \hat{f}_{\hat{\Theta}}(\phi_{j+1},\arg \max_{a}f_{\Theta}(\phi_{j+1},a))\right) & \text{otherwise} \end{cases}$
            \State \textbf{MAJOR UPDATE} Perform gradient descent step on $\mathcal{L}(\Theta) = \left(y_j - f_{\Theta}(\phi_j,a_j)\right)^2$ with respect to $\Theta$
            \State Every $C$ steps reset the target action-value function $\hat{f} = f$ by setting weights $\hat{\Theta} = \Theta$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Experiment Setting}

To provide a proof-of-concept for the DRL-AP algorithm, we conduct a simulation of the online
learning environment using the KDD12 dataset. The pretraining step of the DRL-AP algorithm 
is implemented in the Python Notebook \verb|rl_model_pretraining.ipynb|,
whereas the simulation of the Online Part of the algorithm using the KDD12 dataset is implemented in
\verb|drl_algorithm.ipynb|. Both of these scrips are avalable at the root of the Github repository
for this report \citep{Batek_Deep_Reinforcement_Leaning_2024}.

\subsection{Simulating the Online Environment using KDD12}
\label{sec:sim-description}

The KDD12 dataset was selected to simulate the online RL environment due to the fact that out
of the three datasets introduced in section~\ref{sec:datasets-preprocessing}, it is the
only dataset with clearly discernable user, context and advertisment features. Both Criteo
and Avazu contain anonymized features, and it is therefore unclear what type of features these are.
All of the preprocessing steps discussed in section~\ref{sec:datasets-preprocessing} have 
been encoded as in the preprocessing functions, as represented by $\phi$ in algorithm~\ref{alg:drl-ap}.
The pretraining of the DCN model $f_{\Theta}$ was conducted on first 157440 records the same training
data as in chapter~\ref{chap:deep-ctr-model-evaluation}. 

Our method for the online simulation proceeds as follows. A list of unique
user ID $u$ and query ID $c$ entries from the KDD12 validation set has been compiled, ensuring that each
$u$-$c$ combination in this list had at least $P$ observations in the validation set (where $P$ is the chosen ad list length
for the environment). This list was then filtered to then include only the queries for a
selected subset of users, in order to emulate a significant change in user population in
comparison to the training set.

For each session $t$ in the simulation, we ``observe'' the
user-query combination by selecting values from our pre-prepared list, thereby
establishing the state $s_t$ for the session. The set of available adverts $\mathcal{A}_t$ is
then retrieved by querying the KDD12 validation set for observations with the
specific user-query entries that match the current state for the session. The DRL-AP
algorithm then proceeds to construct ad list $L_t$ by choosing $P$ adverts from $\mathcal{A}_t$ as described
in algorithm~\ref{alg:drl-ap}. Finally the user feedback $\{r_h\}_{h=(t-1)P+1}^{tP}$
is then simply the target labels for the specific observations used to construct $L_t$.


\subsection{Evaluation Metrics}

We demonstrate the simulated performance of the DRL-AP algorithm in terms of
\textbf{session level click through rate} and \textbf{regret}. The prior is simply
defined as the number of ad observations chosen for $L_t$ with positive click target
labels, divided by list length $P$. For the latter, we adapt definition~\ref{def:regret}
as follows to suit the context of our simulation:

\begin{equation}
    \label{eqn:sim-regret}
    \mathcal{R}_T = \sum_{t=1}^K \left[ \min(P, \sum_{r \in \mathcal{A}_t}r) - \sum_{r \in L_t}r\right]   
\end{equation}
The first term contained in the square brackets above represents the value for the
optimal policy $\pi^*$. This policy should in theory be able to maximize the number
of clicks for each session, given ad list $\mathcal{A}_t$ and list length constraint
$P$. The second term is simply equal to the value realized by the agent by constructing
and displaying list $L_t$.

\subsection{Hyperparameter Settings}

The proof-of-concept simulation was run over a time horizon of 30 episodes, each
containing 10 user-query sessions for Ad-list requests of length 10.
The hyperparameter settings for the RL simulation have been chosen arbitrarily
are stipulated in table~\ref{tab:rl-parameters}.

\begin{table}[h]
    \begin{tabular}{|l|c|r|}
        \hline
        \textbf{Hyperparameter} & \textbf{Description} & \textbf{Value}\\
        \hline
        $\mathbf{N}$ & Size of memory $\mathbf{D}$ & 1800 \\
        $K$ & Number of episodes & 30 \\
        $M$ & Sessions per episode & 10 \\
        $\alpha$ & Explore parameter & 1.0 \\
        $P$ & Ad List length & 6 \\
        $\epsilon$ & List-item level explore probability & 0.5\\
        $\gamma$ & Future value discount & 0.1\\
        \hline
    \end{tabular}
    \caption{Hyperparameter settings for RL simulation}
    \label{tab:rl-parameters}
\end{table}

\section{Deep CTR-RL Results and Discussion}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/drl-ap.png}
    \caption{Episode level Regret and CTR for DRL-AP and Random Model}
    \label{fig:drl-ap}
\end{figure}

The results for the DRL-AP algorithm are shown in blue in Figure~\ref{fig:drl-ap}. 
Additionally, we include the results for two alternative benchmark algorithms. The first
of these, represented by the orange line, is a Random Model, in which the set of 
$P$ adverts are chosen by effectively sampling from $\mathcal{A}_t$ uniformly at random without
replacement. The second baseline algorithm, represented by the green line in Figure~\ref{fig:drl-ap},
simply uses the initial pre-trained DCN model for selecting ads and does not perform the Double Dualing
Bandit Exploration, Minor Update and Major Updates stages described in section~\ref{sec:drn}.

The results shown in Figure~\ref{fig:drl-ap} are promising in the context of Research Questions
\ref{exp:dcn-model-advantage} and \ref{exp:drl-ap-advantage}. The comparison between the DCN
model and the Random indicates that the DCN model manages to accumulates a higher volume of cumulative
clicks over the 30 episode time horizon, and therefore incurs a lower level of regret (research question \ref{exp:dcn-model-advantage}). Interestingly,
the majority of the relative outperformance by the DCN model occurs before eposide 20, when the DCN
model appears to have the highest advantage over the random model in terms of average CTR rate attained.
However, this advantage appears to erode in the latter episodes, and as a result the DCN model attains
exactly the same number of clicks as Random Selection in the last 10 episode. One possible explaination
for this is that since the DCN model is not learning throughout the simulation, it is relying purely on
the trends that it derived from the KDD12 training set during pre-training. The construction of the user-query
list as discussed in section~\ref{sec:sim-description} allows for the possibility that the DCN model may have
encountered user-query combinations in the simulation that we not adequately represented in the training set.
In such a case, the model's ability to retrieve relevant Ad candidates is likely to be lower. 
On the other hand, the performance of the DRL-AP algorithm relative to the DCN model with no updates
gives a positive indication of the regret minimization advantage provided by the Minor and Major
updates of the DRL-AP algorithm (research question \ref{exp:drl-ap-advantage}). The prior has consistantly
attained a higher average CTR over the course of the simulation, and as a result has accumulated
84\% more Clicks by the end of the simulation. In the same reasoning as mentioned previously,
a possible explanation for this is that the exploration and update steps of the DRL-AP algorithm
allows its agent to \emph{learn} from its interactions with the environment in a way that allows
the Q-function approximator to adapt to any changes between the pretraining data and the current
environment.

These results demonstrate the potential that Deep Reinforcement Learning
methods have for creating a truly adaptive Ad personalization framework. Nevertheless, it is worthwhile to be mindful of a number of limitations
of experiment~\ref{exp:rl-sim}. Firstly, the KDD12 dataset is not sequential, so it is not possible
that experiment~\ref{exp:rl-sim} simulates the sequential dependencies that are characteristic of
reccomender systems \citep{RefWorks:zheng2018drn:}. Secondly, it is extremely difficult for an offline
simulation to capture the true transition dynamics and other characteristics of the online environment
\citep{pike-burke2024LearnigAgents,RefWorks:wang2024deep,RefWorks:zheng2018drn:}. This means that
the results shown in Figure~\ref{fig:drl-ap} are not guarunteed in the case when DRL-AP is productionalized
online. Lastly, the fact that we are not accounting for user engagement in the Reward function of
algorithm~\ref{alg:drl-ap} means that DRL-AP targets immediate click maximization, which could come
at the detriment to long term user engagement on the given platform \cite{RefWorks:zheng2018drn:}.

\chapter{Conclusion}
\label{chap:conclusion}


In this report, our overarching objective is to demonstrate the potential of using a combination of 
Deep Learning and Reinforcement Learning methods to construct a highly accurate truely
adaptive algorithm for Ad personalization. We explore the relative predictive performance
of a range of Deep Learning models from a pool of candidates that is representitive of the
array of state-of-the-art techniques used in CTR prediction today. We then perform a
sequential hyperparameter search to find the ideal hyperparameter settings for the Deep \& Cross
Network model. Finally, we incorporate the DCN model as the Q-function approximator in
a novel Double-Deep Q-Network Algorithm which we introduce as the Deep Reinforcement Learning
for Ad Personalization (DRL-AP) algorithms. The findings of this report offer substantial insight
and contributions in the domains of Deep Learning, Reinforcement Learning and Ad personalization.

\subsubsection{Contributions}

Firstly, our comparative CTR model analysis provides an overview of the relative
performance of the most popular deep learing models used for Click Through Rate prediction. Our
literature review revealed that this is the first comparitive model analysis that was conducted in 
the CTR prediction domain where the neuron and layer settings of the DNN components of each model
was standardized, thereby providing a more level comparison. Our analysis reveals that the Deep \& Cross Network model proposed
by \citep{RefWorks:wang2017deep} outperformed other model candidates relatively consistantly
when scored using samples from the Avazu, Criteo and KDD12 CTR prediction datasets.
Furthermore, the comparative model analysis validates the performance of Deep Learning models
over classical shallow methods such as Logistic Regression and Factorization Machines. Irrespective of
the dataset that was used, the LR and FM models we both consistantly amongst the worst performing
candidates in terms of Binary Accuracy and AUC. Our experiments also showed that although the
performance of the shallow models tends to be inferior in comparison to most DNN candidates, 
the performance differential tends to be marginal in terms of both Binary Accuracy and AUC, 
a result which has been present in many of the foundational research papers for the DNN models
explored \citep{RefWorks:guo2017deepfm:,RefWorks:liu2015convolutional,RefWorks:he2017neural}.
This indicates that the LR and FM models can still be considered reasonably viable ``parameter-light''
alternatives to DNN models in cases where a slightly lower level of predictive accuracy is acceptable.

The Deep Reinforcement Learning for Ad Personalization (DRL-AP) algorithm provides a viable
proof-of-concept for incorporating Deep Learning and Reinforcement Learning in a Digital Ad marketplace
environment. Our offline simulation demonstrates that the use of a pretrained DCN model
to choose the set of advertisments to display results in an increase in user clicks over and above
random selection. Furthermore, our simulation also demonstrates that the CTR performance of the ad
recommendation system is significantly improved when the DCN model is used as a Q-function approximator
in a Double Deep Q-Network algorithm that leverages Double Dualing Bandit method for exploration
to enhance the online learning process. The result of our simulation showed that the DRL-AP algorithm
accumulated 84\% more clicks than the use of a DNN model without Reinforcement Learning, and
115\% more clicks than Random Selection. These results provide an encouraging view of the
potential for the use of Deep Reinforcement Learning in ad personalization. 


\subsubsection{Limitations and Further Research}

The aim of the comparitive model analysis in this report was to provide a broad overview
of the relative predictive performance of the popular Deep Learning techniques that are being
applied to the CTR prediction domain. This domain represents an extremely active field of
research, and it is therefor infeasible for me to cover all relevant models in scope of this project.
While the analysis conducted in this project provide a good foundation to compare the different
categories of Deep CTR modelling techniques, further research can add to the scope of models
that have been included in this paper. In theory, the DRL-AP algorithm proposed as a part of this project
is agnostic to the choice of Deep Learning model used as the Q-function approximator, which paves
the way for the incorporation of other models in place of DCN should they be shown to have
superior predictive performance.

Furthermore, due to the number of models in scope, we have been force to work under certain data sample
size constraints in order to carry our comparitive model analysis within a reasonable amount
of training time and cost. This has already been shown to result in lower Log Loss and AUC measures
being attained after model fitting in comparison to the foundational papers.

Lastly, a significant limitation in our demonstration of the DRL-AP algorithm is the
fact that this was done by means of an offline simulation that was constructed with the
KDD12 dataset. This fact poses the following two issues. Firstly, the KDD12 dataset is not sequential, so it is not possible
that experiments experiment perfectly simulate the sequential dependencies that are characteristic of
reccomender systems. Secondly, it is extremely difficult for an offline
simulation to capture the true transition dynamics and other characteristics of the online environment
\citep{pike-burke2024LearnigAgents,RefWorks:wang2024deep,RefWorks:zheng2018drn:}.
Further research in this domain should aim to incorporate a suitable sequential dataset
containing discernable user, context and ad features for offline testing, as well as online
testing in a controlled digital ad marketplace environment.

In conclusion, throughout this report we present a Deep Reinforcement Learning
framework that incorporates the recent deep learning techniques, and show that the resulting
DRL-AP algorithm has the potential to autonomously adapt to user preferences and drive user
engagement with the online advertising platform. This project emphasizes the role of autonomous
deep learning agents in the future of Digital Advertizing.

\clearpage
 %% reset page counter and start appendix pages with A
\pagenumbering{arabic}
\renewcommand*{\thepage}{A\arabic{page}}

%% Appendix goes here
\appendix
%
\chapter{Appendix}

\section{Abbreviations and Acronyms}
\label{app:acronyms}

\begin{table}[ht]
    %\centering
    \begin{tabular}{|l|l|}
      \hline
        \textbf{Term} & \textbf{Definition} \\
      \hline
        LR& Logistic Regression  \\
        FM & Factorization Machine  \\
        FFM & Field-Aware Factorization Machine \\
        DNN & Deep Neural Network \\
        MLP & Multilayer Perceptron \\
        FNN & Factorization Machine Supported Neural Network\\
        PNN & Product-based Neural Network \\
        NFM & Neural Factorization Machines \\
        DCN & Deep \& Cross Network \\
        WDL & Wide \& Deep Learning \\
        DFM & DeepFM \\
        AutoInt & Automatic Feature Interaction \\
        CCPM & Convolutional Click Prediction Model \\ 
        FGCNN & Feature Generation by Convolutional Neural Network\\
        MDP & Markov Decision Process \\
        RL & Reinforcement Learning \\ 
        DQN & Deep Q-Network \\
        DDQN & Double Deep Q-Network \\
        DBGD & Dueling Bandit Gradient Descent \\
        DRN & Deep Reinforcement Learning for News Reccomendation\\
    \hline
    \end{tabular}
\end{table}

\pagebreak
\section{Notation}
\label{app:notation}

\begin{table}[h]
    %\centering
    \begin{tabular}{|l|l|l|}
      \hline
        \textbf{Symbol} & \textbf{Definition} & \textbf{Reference} \\
      \hline
        $\mathbf{x}$& Feature vector, before pre-processing &\\
        $n$ & the number of features in $\mathbf{x}$ &\\
        $x_i$& The $i$-th feature in $\mathbf{x}$ &\\
        $\mathbf{x_i}^{OH}$ & One-hot encoded vector representation of categorical feature $i$ &\\
        $\mathbf{e}_i$ & Embedded vector representation of categorical feature $i$ & \\
        $z_i$ & Mean and variance standardized value for feature $i$ from $\mathbf{x}$ &\\
        $\tilde{\mathbf{x}}$ & $\mathbf{x}$ after catigorical embedding and numerical standardization. &\\
        $f$ & Pre-sigmoid classification function &\\
        $\Theta$ & Parameter vector for $f$ & \\
    \hline
    \end{tabular}
\end{table}

\section{Proofs}

\begin{lemma}
    \label{lemma:fm-linearity}
    The model equation of a 2-way factorization machine (eq.~\ref{eqn:fm-2way}) can
    be computed in linear time $O(F\tilde{n})$.
    \end{lemma}
    
    \begin{proof}
    \label{prf:fm-linearity}
    
    Due to the factorization of the pairwise interactions, there is no model
    parameter that directly depends on two features $(j,k)$. This means that
    pairwise interactions can be reformulated as such
    
    \begin{align*}
        &\sum_{j=1}^{\tilde{n}} \sum_{k=j+1}^{\tilde{n}} \langle \mathbf{v}_j , \mathbf{v}_k \rangle \tilde{x}_j \tilde{x}_k \\
        &= \sum_{j=1}^{\tilde{n}} \sum_{k=j+1}^{\tilde{n}} 
        \sum_{f=1}^{F}v_{j,f} v_{k,f}\tilde{x}_j \tilde{x}_k \\
        &= \frac{1}{2} \left( \sum_{j=1}^{\tilde{n}} \sum_{k=1}^{\tilde{n}}
        \sum_{f=1}^{F} v_{j,f} v_{k,f} \tilde{x}_j \tilde{x}_k -
        \sum_{j=1}^{\tilde{n}} \sum_{f=1}^{F}v_{j,f} v_{j,f} \tilde{x}_j \tilde{x}_j \right)\\
        &=\frac{1}{2}\sum_{f=1}^{F} \left( \sum_{j=1}^{\tilde{n}}v_{j,f} \tilde{x}_j
        \sum_{k=1}^{\tilde{n}} v_{k,f} \tilde{x}_k - \sum_{j=1}^{\tilde{n}}
        v_{j,f}^2 \tilde{x}_j^2 \right)\\
        &= \frac{1}{2}\sum_{f=1}^{F} \left( \left(\sum_{j=1}^{\tilde{n}}v_{j,f} \tilde{x}_j\right)^2
        - \sum_{j=1}^{\tilde{n}} v_{j,f}^2 \tilde{x}_j^2 \right)
    \end{align*}
    
    The complexity of the final line above is $O(F\tilde{n})$, and hence the FM formulation
    as per equation~\ref{eqn:fm-2way} scales linearly in $F$ and $\tilde{n}$.
    \end{proof}

\section{Experiment Results}

\subsection{Experiment 1: Comparative Model Analysis}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../figures/model_scores.png}
    \caption{Experiment 1 Model Scores}
    \label{fig:experiment-1}
\end{figure}

\subsection{Experiment 3: Ad Search Platform Simulation}

\begin{table}[h]
    \csvautotabular{../drl_simulation/DRL-AP_episode_data.csv}
    \caption{Simulation results for the DRL-AP algorithm}
    \label{tab:drl-ap}
\end{table}

\begin{table}[h]
    \csvautotabular{../drl_simulation/Random_Model_episode_data.csv}
    \caption{Simulation results for Random Selection}
    \label{tab:random-selection}
\end{table}

\begin{table}[h]
    \csvautotabular{../drl_simulation/DCN_No_Update_episode_data.csv}
    \caption{Simulation results for the DCN model}
    \label{tab:dcn-model}
\end{table}

%%References part of appendices
% References: modify the file refs.bib
\bibliographystyle{plainnat}
\bibliography{refs}


\end{document}
